<?xml version="1.0" encoding="utf-8"?>
<search> 
  
    
    <entry>
      <title></title>
      <url>/CDH/S3/oozie_s3_jceks.html</url>
      <content type="html"><![CDATA[<p><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">hadoop credential create fs.s3a.access.key -provider jceks://hdfs/tmp/file.jceks -value BBBA74VJ69J8I4N0GW3O</div><div class="line"></div><div class="line">hadoop credential create fs.s3a.secret.key -provider jceks://hdfs/tmp/file.jceks -value eNKTsutRmHR8HHM03Mx3xT6ctaW3Fd+PX75KGatk</div></pre></td></tr></table></figure></p>
]]></content>
      
        
    </entry>
    
    <entry>
      <title></title>
      <url>/CDH/Error/is%20not%20a%20leaf%20queue.html</url>
      <content type="html"><![CDATA[<p><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">INFO  : Compiling command(queryId=hive_20171014104747_ef9adfe5-dd2b-493b-8557-7bce196ebf06): select count(*) from cdmp_dw.td_cms_content_class_d where src_file_day=&apos;20171013&apos; limit 10</div><div class="line">INFO  : Semantic Analysis Completed</div><div class="line">INFO  : Returning Hive schema: Schema(fieldSchemas:[FieldSchema(name:_c0, type:bigint, comment:null)], properties:null)</div><div class="line">INFO  : Completed compiling command(queryId=hive_20171014104747_ef9adfe5-dd2b-493b-8557-7bce196ebf06); Time taken: 0.214 seconds</div><div class="line">INFO  : Executing command(queryId=hive_20171014104747_ef9adfe5-dd2b-493b-8557-7bce196ebf06): select count(*) from cdmp_dw.td_cms_content_class_d where src_file_day=&apos;20171013&apos; limit 10</div><div class="line">INFO  : Query ID = hive_20171014104747_ef9adfe5-dd2b-493b-8557-7bce196ebf06</div><div class="line">INFO  : Total jobs = 1</div><div class="line">INFO  : Launching Job 1 out of 1</div><div class="line">INFO  : Starting task [Stage-1:MAPRED] in serial mode</div><div class="line">INFO  : Number of reduce tasks determined at compile time: 1</div><div class="line">INFO  : In order to change the average load for a reducer (in bytes):</div><div class="line">INFO  :   set hive.exec.reducers.bytes.per.reducer=&lt;number&gt;</div><div class="line">INFO  : In order to limit the maximum number of reducers:</div><div class="line">INFO  :   set hive.exec.reducers.max=&lt;number&gt;</div><div class="line">INFO  : In order to set a constant number of reducers:</div><div class="line">INFO  :   set mapreduce.job.reduces=&lt;number&gt;</div><div class="line">INFO  : number of splits:10</div><div class="line">INFO  : Submitting tokens for job: job_1507886429302_10129</div><div class="line">INFO  : Kind: HDFS_DELEGATION_TOKEN, Service: ha-hdfs:ns1, Ident: (token for hive: HDFS_DELEGATION_TOKEN owner=hive/ddp-cm.cmdmp.com@CMDMP.COM, renewer=yarn, realUser=, issueDate=1507949231769, maxDate=1508554031769, sequenceNumber=5622438, masterKeyId=763)</div><div class="line">INFO  : Cleaning up the staging area /user/hive/.staging/job_1507886429302_10129</div><div class="line">ERROR : Job Submission failed with exception &apos;java.io.IOException(org.apache.hadoop.yarn.exceptions.YarnException: Failed to submit application_1507886429302_10129 to YARN : root.users.cdmpview is not a leaf queue)&apos;</div><div class="line">java.io.IOException: org.apache.hadoop.yarn.exceptions.YarnException: Failed to submit application_1507886429302_10129 to YARN : root.users.cdmpview is not a leaf queue</div><div class="line">	at org.apache.hadoop.mapred.YARNRunner.submitJob(YARNRunner.java:306)</div><div class="line">	at org.apache.hadoop.mapreduce.JobSubmitter.submitJobInternal(JobSubmitter.java:244)</div><div class="line">	at org.apache.hadoop.mapreduce.Job$10.run(Job.java:1307)</div><div class="line">	at org.apache.hadoop.mapreduce.Job$10.run(Job.java:1304)</div><div class="line">	at java.security.AccessController.doPrivileged(Native Method)</div><div class="line">	at javax.security.auth.Subject.doAs(Subject.java:415)</div><div class="line">	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1796)</div><div class="line">	at org.apache.hadoop.mapreduce.Job.submit(Job.java:1304)</div><div class="line">	at org.apache.hadoop.mapred.JobClient$1.run(JobClient.java:578)</div><div class="line">	at org.apache.hadoop.mapred.JobClient$1.run(JobClient.java:573)</div><div class="line">	at java.security.AccessController.doPrivileged(Native Method)</div><div class="line">	at javax.security.auth.Subject.doAs(Subject.java:415)</div><div class="line">	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1796)</div><div class="line">	at org.apache.hadoop.mapred.JobClient.submitJobInternal(JobClient.java:573)</div><div class="line">	at org.apache.hadoop.mapred.JobClient.submitJob(JobClient.java:564)</div><div class="line">	at org.apache.hadoop.hive.ql.exec.mr.ExecDriver.execute(ExecDriver.java:418)</div><div class="line">	at org.apache.hadoop.hive.ql.exec.mr.MapRedTask.execute(MapRedTask.java:142)</div><div class="line">	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:214)</div><div class="line">	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:100)</div><div class="line">	at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:1976)</div><div class="line">	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1689)</div><div class="line">	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1421)</div><div class="line">	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1205)</div><div class="line">	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1200)</div><div class="line">	at org.apache.hive.service.cli.operation.SQLOperation.runQuery(SQLOperation.java:237)</div><div class="line">	at org.apache.hive.service.cli.operation.SQLOperation.access$300(SQLOperation.java:88)</div><div class="line">	at org.apache.hive.service.cli.operation.SQLOperation$3$1.run(SQLOperation.java:293)</div><div class="line">	at java.security.AccessController.doPrivileged(Native Method)</div><div class="line">	at javax.security.auth.Subject.doAs(Subject.java:415)</div><div class="line">	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1796)</div><div class="line">	at org.apache.hive.service.cli.operation.SQLOperation$3.run(SQLOperation.java:306)</div><div class="line">	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)</div><div class="line">	at java.util.concurrent.FutureTask.run(FutureTask.java:262)</div><div class="line">	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)</div><div class="line">	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)</div><div class="line">	at java.lang.Thread.run(Thread.java:745)</div><div class="line">Caused by: org.apache.hadoop.yarn.exceptions.YarnException: Failed to submit application_1507886429302_10129 to YARN : root.users.cdmpview is not a leaf queue</div><div class="line">	at org.apache.hadoop.yarn.client.api.impl.YarnClientImpl.submitApplication(YarnClientImpl.java:257)</div><div class="line">	at org.apache.hadoop.mapred.ResourceMgrDelegate.submitApplication(ResourceMgrDelegate.java:290)</div><div class="line">	at org.apache.hadoop.mapred.YARNRunner.submitJob(YARNRunner.java:290)</div><div class="line">	... 35 more</div><div class="line"></div><div class="line">ERROR : FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.mr.MapRedTask</div><div class="line">INFO  : Completed executing command(queryId=hive_20171014104747_ef9adfe5-dd2b-493b-8557-7bce196ebf06); Time taken: 1.268 seconds</div><div class="line">Query History</div></pre></td></tr></table></figure></p>
]]></content>
      
        
    </entry>
    
    <entry>
      <title><![CDATA[Linux中history历史命令使用方法详解]]></title>
      <url>/Document/Linux%E4%B8%ADhistory%E5%8E%86%E5%8F%B2%E5%91%BD%E4%BB%A4%E4%BD%BF%E7%94%A8%E6%96%B9%E6%B3%95%E8%AF%A6%E8%A7%A3.html</url>
      <content type="html"><![CDATA[<h1 id="Linux中history历史命令使用方法详解"><a href="#Linux中history历史命令使用方法详解" class="headerlink" title="Linux中history历史命令使用方法详解"></a>Linux中history历史命令使用方法详解</h1><h2 id="使用HISTTIMEFORMAT在历史中显示TIMESTAMP"><a href="#使用HISTTIMEFORMAT在历史中显示TIMESTAMP" class="headerlink" title="使用HISTTIMEFORMAT在历史中显示TIMESTAMP"></a>使用HISTTIMEFORMAT在历史中显示TIMESTAMP</h2><p>export HISTTIMEFORMAT=’%F %T ‘<br>也可以设置alias语句来查看最近的历史命令<br>alias h1=’history 10’<br>alias h2=’history 20’<br>alias h3=’history 30’</p>
<h2 id="用Ctrl-＋-R搜索历史命令"><a href="#用Ctrl-＋-R搜索历史命令" class="headerlink" title="用Ctrl ＋ R搜索历史命令"></a>用Ctrl ＋ R搜索历史命令</h2><p>按下Ctrl +R然后输入关键字。 在以下示例中，我搜索“red”，则显示以前的命令中含有”red”的命令“cat/etc/redhat-release”。<br>注：在命令行提示符下按下Ctrl＋R，终端将显示如下提示―reverse-i-search‖<br> (reverse-i-search)<code>red</code>: cat/etc/redhat-release<br>注：当看到你要的命令后按回车键，就可以重新执行这条命令了<br>cat /etc/redhat-release<br>Fedora release 9 (Sulphur)<br>而有的时候你需要在执行一条历史命令之前编辑它.比如，你可以像下面那样搜索“httpd”，终端显示历史命令“service httpd stop”，选择它把“stop”改为“start”然后执行它<br>注: 在命令提示符下按 Ctrl+R , 将会显示提示符‖reverse-i-search‖<br> (reverse-i-search)<code>httpd</code>: service httpdstop<br>注: 看到你想要的命令后按下左键或者右键,就可以在执行这条命令之前编辑它了 </p>
<h2 id="四种不同的方法快速执行之前的命令"><a href="#四种不同的方法快速执行之前的命令" class="headerlink" title="四种不同的方法快速执行之前的命令"></a>四种不同的方法快速执行之前的命令</h2><p>有时出于某些原因你需要执行之前的命令，下面的四种方法可以用来重复最后执行的命令：</p>
<ol>
<li>用向上键（up arrow ）查看上条命令，按回车执行。 </li>
<li>在命令行中输入!!并按回车。 </li>
<li>在命令行中输入!-1并按回车。 </li>
<li>按Ctrl+P显示上条命令，按回车执行。</li>
<li>如果你想再次执行第四条命令，执行！4即可</li>
<li>执行以特定字开头的历史命令,输入！和你要重新执行的命令的前几个字母。如！ps</li>
</ol>
<h2 id="使用HISTFILE改变历史文件名"><a href="#使用HISTFILE改变历史文件名" class="headerlink" title="使用HISTFILE改变历史文件名"></a>使用HISTFILE改变历史文件名</h2><p>默认位置在.bash_history文件中，把下面的一行添加到.bash_profile文件中，重新登录shell，则.commandline_warrior文件将取代.bash_history文件用来储存历史命令。你可以使用这个命令来追踪不同终端中执行的命令，届时只需要将不同终端中所执行的命令保存在不同的历史文件中即可。</p>
<p><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">#vi ~/.bash_profile </div><div class="line">HISTFILE=/root/.commandline_warrior</div></pre></td></tr></table></figure>
</p>
]]></content>
      
        <categories>
            
            <category> Document </category>
            
        </categories>
        
        
    </entry>
    
    <entry>
      <title><![CDATA[Path of CDH config]]></title>
      <url>/CDH/Path%20of%20CDH%20config.html</url>
      <content type="html"><![CDATA[<h1 id="Path-of-CDH-config"><a href="#Path-of-CDH-config" class="headerlink" title="Path of CDH config"></a>Path of CDH config</h1><p>When using Cloudera Manager, configuration is stored in a central DB, upon startup of services on a target cluster node, cloudera manager passes, through the Agent on that host, the runtime configuration that should be used, and starts the processes, pointing to that runtime location.</p>
<p>This results in the actual services configuration being stored in a non-standard location of:</p>
<p><code>/var/run/cloudera-scm-agent/process/###-[service]-[SERVICE-ROLE]</code></p>
<p>The most recient ‘instance” of a path is the current runtime config (use ls -ltr as root in the /var/run/cloudera-scm-agent/process path, the last being most current.</p>
<p>You can access the SAME information on a per service role instance basis, from the “process” tab.   For example for hdfs</p>
<p>Cloudera Manager &gt; Cluster &gt; HDFS &gt; Instances &gt; (pick for example, the NameNode from the list)&gt; Processes </p>
<p>You will see under “Configuration Files/Environment” a greater than (&gt;) that you can click to expand and show all the current configs passed to the server, the same info in the path I describe above.  This is handy as not all cluster administrators have root access to get to the indicated path.</p>
<p>The cloudera manager function of “Deploy client configuration” pushes the current configuration information, SPECIFIC TO CLIENT APPLICATIONS to the cluster hosts and defined gateway nodes, which end up in the default /etc/ locations you are used to from Hadoop and CDH documentation.  Those locations will not have the complete configuration as used by the server, just values necessary for client applications (CLI, custom apps, etc) to use the cluster.</p>
]]></content>
      
        <categories>
            
            <category> CDH </category>
            
        </categories>
        
        
    </entry>
    
    <entry>
      <title><![CDATA[nas盘用户名一致,uid不一致]]></title>
      <url>/Document/nas%E7%9B%98%E7%94%A8%E6%88%B7%E5%90%8D%E4%B8%80%E8%87%B4,uid%E4%B8%8D%E4%B8%80%E8%87%B4.html</url>
      <content type="html"><![CDATA[<h1 id="nas盘用户名一致-uid不一致"><a href="#nas盘用户名一致-uid不一致" class="headerlink" title="nas盘用户名一致,uid不一致"></a>nas盘用户名一致,uid不一致</h1><p>nas盘是已uid为准的,如果是新用户,先删除重新创建额时候指定uid</p>
]]></content>
      
        <categories>
            
            <category> Document </category>
            
        </categories>
        
        
    </entry>
    
    <entry>
      <title><![CDATA[hirecut]]></title>
      <url>/Tips/hirecut.html</url>
      <content type="html"><![CDATA[<p>In this video, MC will be showing you step by step how to do a skin fade using a straight razor.<br>-0:13 Make sure to wet the hair before using a straight razor against the scalp. This will result in less pulling and irritation.<br>-0:48 Starting in the back, MC starts removing hair with short strokes.<br>-1:25 Moves to the sides.<br>-3:16 MC starts the fade by using his Wahl clippers with an open blade.<br>-3:52 Continue to blend by using your 1/2 guard.<br>-4:02 You can remove the bulk between the top of the head and the fade by using your 1 1/2 clipper guard.<br>-4:22 Move down to your 1 guard.<br>-4:40 MC continues to remove the line left from the straight razor by using his Andis trimmers upside down to avoid leaving another line.<br>-5:07 Now he flips his Andis trimmers back to right side up. This will ensure fading the skin fade to it’s full potential.<br>-5:43 MC uses his Wahl clippers again with a closed blade. He will utilize the lever to adjust the blade accordingly.<br>-7:06 Learn a notching technique used on the top of the head. This will leave texture and allow for more movement.<br>-8:01 MC uses a hair removal technique used for the fine hairs on the face and ears with fire called; hair singeing.<br>-8:30 Learn how to properly apply pomade to the hair to style your fohawk.<br>Enjoy!!</p>
<p>2016年12月17日发布<br>In this video, MC will be showing you how to do a 90’s hairstyle for men. It includes a fade and a short, spiky, hairstyle.<br>-0:32 Using a 1 1/2 metal guard on his Oster clippers, MC removes the bulk to create his foundation.<br>-2:09 Still using his Oster clippers, MC switches to a 1 metal guard.<br>-2:49 To eliminate the line, he switches to Wahl clippers with a 1/2 plastic guard.<br>-4:09 MC moves to a 1 plastic guard on his Wahl clippers.<br>-4:30 Using Andis trimmers, he creates his shape up line.<br>-5:58 He uses scissors over knuckles to remove length from the top of his hairstyle.<br>-8:16 Learn a sculpting technique done with a straight razor to create texture and to remove bulk on the top of the head.<br>-9:28 MC uses scissors over comb to blend the sides and back of the fade into the top.<br>-10:26 Watch a quick blow dry.<br>-11:06 Using a straight razor, MC cleans up the shape up line he created with trimmers.</p>
<p>2015年6月22日发布<br>How To Cut The Top Of Your Hair: <a href="http://www.iamalpham.com/index.php/to" target="_blank" rel="external">http://www.iamalpham.com/index.php/to</a>…<br>Best Hair Styling Products: <a href="http://www.peteandpedro.com" target="_blank" rel="external">http://www.peteandpedro.com</a><br>Alpha M. Confidence Course: <a href="http://shop.aaronmarino.com/product/c" target="_blank" rel="external">http://shop.aaronmarino.com/product/c</a>…<br>My Website: <a href="http://www.iamalpham.com" target="_blank" rel="external">http://www.iamalpham.com</a><br>My Services and Products: <a href="http://www.aaronmarino.com" target="_blank" rel="external">http://www.aaronmarino.com</a><br>Alpha M. App: <a href="http://www.alphamapp.com/" target="_blank" rel="external">http://www.alphamapp.com/</a><br>My Website: <a href="http://www.iamalpham.com" target="_blank" rel="external">http://www.iamalpham.com</a><br>My Services: <a href="http://www.aaronmarino.com" target="_blank" rel="external">http://www.aaronmarino.com</a><br>Free Hairstyle E-Book: <a href="http://www.iamalpham.com/ezine" target="_blank" rel="external">http://www.iamalpham.com/ezine</a></p>
<p>Best Hair Product: <a href="http://www.peteandpedro.com" target="_blank" rel="external">http://www.peteandpedro.com</a></p>
<p>FaceBook: <a href="https://www.facebook.com/IAmAlphaM" target="_blank" rel="external">https://www.facebook.com/IAmAlphaM</a></p>
<p>In this video men’s style, grooming, fitness and lifestyle expert, Aaron Marino of <a href="http://www.iamalpham.com" target="_blank" rel="external">http://www.iamalpham.com</a> <a href="http://www.aaronmarino.com" target="_blank" rel="external">http://www.aaronmarino.com</a> and <a href="http://www.peteandpedro.com" target="_blank" rel="external">http://www.peteandpedro.com</a>, shows you how to cut your own hair at home. This is a simple how to home haircut video for men. In this video I will teach you tips and tricks to cut, blend and fade your own hair at home. Cutting your own hair at home isn’t as complicated as you may think. How to blend your back and sides are explained in this home haircut video tutorial. </p>
<p>Aaron Marino of alpha m. has been letting the top of his hair grow-out. The sides go crazy during this time, so it doesn’t look as tight as it should. He demonstrates how he trims his hair between trips to the barber.<br>Individual results may vary! It does take practice. Remember it’s hair and will grow out.<br>He’s been cutting his own hair throughout his entire life. He enjoys doing it. Once you practice, it’s not that complicated. He is able to keep things tight between visits with Stephen.</p>
<p>Supplies: sharp hair cutting scissors, comb, attachments as described, clippers, and a hand-held mirror.</p>
<p>When Alpha is blending and fading, he proceeds with dry hair. He does wet the top to keep hair out of the way and keep part tight. Alpha describes how to determine the placement of your part.</p>
<p>Alpha then starts to demonstrate how to fade with the clippers (largest attachment). Thereafter, get a hand-held mirror to see the back of your hair. He gets it as good as he can, but in a week &amp; 1/2, Stephen will clean it up professionally. At the top, he’s not touching his scalp. He’s blending better.</p>
<p>Switch to next shortest attachment. Fade up part way to blend seamlessly. In the back, go to about the center of the head and pull out. Rock the clipper out in order to fade it. The shortest attachment is last in order to blend as demonstrated. Don’t go crazy and go to far. Nice and low to complete the back. Next edge along your ears. Edging your neck is the tricky part, and you may need help so it’s doesn’t get jacked-up.</p>
<p>Time for a shower and then style the new haircut. It’s a 7 minute haircut that is passable under the circumstances. It takes a bit of time, energy, and finesse. With practice, you will be proficient enough.</p>
]]></content>
      
        <categories>
            
            <category> Tips </category>
            
        </categories>
        
        
    </entry>
    
    <entry>
      <title><![CDATA[kerberos配置文件]]></title>
      <url>/CDH/Kerberos%20MIT/kerberos%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6.html</url>
      <content type="html"><![CDATA[<p>当主KDC不可用时，从属KDC提供Kerberos票证授予服务，但不提供数据库管理。</p>
<p>最好在有限访问的安全和专用硬件上安装和运行KDC。</p>
<p><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">ddp-nn-02:/var/lib/kerberos/krb5kdc # cat kdc.conf</div><div class="line"></div><div class="line">[kdcdefaults]</div><div class="line">        kdc_ports = 750,88</div><div class="line">        kdc_tcp_ports = 750,88</div><div class="line"></div><div class="line">[realms]</div><div class="line">#       EXAMPLE.COM = &#123;</div><div class="line">#               database_name = /var/lib/kerberos/krb5kdc/principal</div><div class="line">#               admin_keytab = FILE:/var/lib/kerberos/krb5kdc/kadm5.keytab</div><div class="line">#               acl_file = /var/lib/kerberos/krb5kdc/kadm5.acl</div><div class="line">#               dict_file = /var/lib/kerberos/krb5kdc/kadm5.dict</div><div class="line">#               key_stash_file = /var/lib/kerberos/krb5kdc/.k5.EXAMPLE.COM</div><div class="line">#               kdc_ports = 750,88</div><div class="line">#               max_life = 10h 0m 0s</div><div class="line">#               max_renewable_life = 7d 0h 0m 0s</div><div class="line">#       &#125;</div><div class="line"> CMDMP.COM = &#123;</div><div class="line">  max_renewable_life = 365d 0h 0m 0s</div><div class="line">  database_name = /var/lib/kerberos/krb5kdc/principal</div><div class="line">  acl_file = /var/lib/kerberos/krb5kdc/kadm5.acl</div><div class="line">  dict_file = /var/lib/kerberos/krb5kdc/kadm5.dict</div><div class="line">  admin_keytab = /var/lib/kerberos/krb5kdc/kadm5.keytab</div><div class="line">  #default_principal_expiration = 0</div><div class="line">  supported_enctypes = aes256-cts:normal aes128-cts:normal des3-hmac-sha1:normal arcfour-hmac:normal des-hmac-sha1:normal dex-cbc-md5:normal des-cbc-crc:normal</div><div class="line">&#125;</div><div class="line"></div><div class="line">[logging]</div><div class="line">    kdc = FILE:/var/log/krb5/krb5kdc.log</div><div class="line">    admin_server = FILE:/var/log/krb5/kadmind.log</div></pre></td></tr></table></figure>
</p>
<p><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">ddp-nn-02:/var/lib/kerberos/krb5kdc # cat /etc/krb5.conf </div><div class="line">[libdefaults]</div><div class="line">#       default_realm = EXAMPLE.COM </div><div class="line">default_realm = CMDMP.COM</div><div class="line">dns_lookup_realm = false</div><div class="line">dns_lookup_kdc = false</div><div class="line">ticket_lifetime = 24h</div><div class="line">renew_lifetime = 7d</div><div class="line">forwardable = true</div><div class="line">default_tkt_enctypes = arcfour-hmac-md5</div><div class="line">default_tgs_enctypes = arcfour-hmac-md5</div><div class="line"></div><div class="line">[realms]</div><div class="line">#       EXAMPLE.COM = &#123;</div><div class="line">#                kdc = kerberos.example.com</div><div class="line">#               admin_server = kerberos.example.com</div><div class="line">#       &#125;</div><div class="line">CMDMP.COM = &#123;</div><div class="line">  kdc = ddp-nn-02.cmdmp.com</div><div class="line">  admin_server = ddp-nn-02.cmdmp.com</div><div class="line">  kdc = ddp-dn-07.cmdmp.com</div><div class="line">  admin_server = ddp-dn-07.cmdmp.com</div><div class="line">&#125;</div><div class="line"></div><div class="line">[domain_realm]</div><div class="line">.cmdmp.com = CMDMP.COM</div><div class="line">cmdmp.com = CMDMP.COM</div><div class="line"></div><div class="line">[logging]</div><div class="line">    kdc = FILE:/var/log/krb5/krb5kdc.log</div><div class="line">    admin_server = FILE:/var/log/krb5/kadmind.log</div><div class="line">    default = SYSLOG:NOTICE:DAEMON</div></pre></td></tr></table></figure>
</p>
]]></content>
      
        <categories>
            
            <category> CDH </category>
            
            <category> Kerberos MIT </category>
            
        </categories>
        
        
    </entry>
    
    <entry>
      <title><![CDATA[flume agent任务失败]]></title>
      <url>/CDH/Error/flume%20agent%E4%BB%BB%E5%8A%A1%E5%A4%B1%E8%B4%A5.html</url>
      <content type="html"><![CDATA[<h1 id="flume-agent任务失败"><a href="#flume-agent任务失败" class="headerlink" title="flume agent任务失败"></a>flume agent任务失败</h1><p>[<a href="https://community.hortonworks.com/questions/45962/dataxceiver-error-processing-write-block-operation.html" target="_blank" rel="external">https://community.hortonworks.com/questions/45962/dataxceiver-error-processing-write-block-operation.html</a>]<br>flume日志报错: </p>
<p><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">PriviledgedActionException as:hadoop@CMDMP.COM (auth:KERBEROS) cause:java.io.IOException: Couldn&apos;t setup connection for hadoop@CMDMP.COM to ddp-nn-02.cmdmp.com/10.200.60.107:8020</div><div class="line">06 Jul 2017 06:39:23,507 INFO  [hdfs-sink1-call-runner-5] (org.apache.hadoop.io.retry.RetryInvocationHandler.invoke:148)  - Exception while invoking create of class ClientNamenodeProtocolTranslatorPB over ddp-nn-02.cmdmp.com/10.200.60.107:8020 after 1 fail over attempts. Trying to fail over immediately.</div><div class="line">java.io.IOException: Failed on local exception: java.io.IOException: Couldn&apos;t setup connection for hadoop@CMDMP.COM to ddp-nn-02.cmdmp.com/10.200.60.107:8020; Host Details : local host is: &quot;ddp-dn-101.cmdmp.com/10.200.63.89&quot;; destination host is: &quot;ddp-nn-02.cmdmp.com&quot;:8020;</div><div class="line"></div><div class="line">Unable to close file because the last block des not have enough number of replicas.</div></pre></td></tr></table></figure>
</p>
<p>开头开发怀疑是kerberos的问题,后来排查发现是每个agent进程jvm内存使用超出配额8192m,但是进程也没有配上超出后自动kill的参数,所以可以运行但是会报错</p>
<p>The flume agents went in OutOfMemoryError (unable to create new native thread) and the impact on the hdfs had been the error posted above</p>
<p>同时hdfs上也会有日志写入出错的记录<br>hadoop-datanode1:50010ataXceiver error processing WRITE_BLOCK operation<br>运行mapreduce程序在reduce阶段时出现错误提示： Unable to close file because the last block does not have enough number of replicas.</p>
<p>jmap -heap pid 查看java进程内存使用情况</p>
]]></content>
      
        <categories>
            
            <category> CDH </category>
            
            <category> Error </category>
            
        </categories>
        
        
    </entry>
    
    <entry>
      <title><![CDATA[密码管理]]></title>
      <url>/CDH/Kerberos%20MIT/%E5%AF%86%E7%A0%81%E7%AE%A1%E7%90%86.html</url>
      <content type="html"><![CDATA[<h1 id="用户"><a href="#用户" class="headerlink" title="用户"></a>用户</h1><h2 id="密码管理"><a href="#密码管理" class="headerlink" title="密码管理"></a>密码管理</h2><h3 id="更改密码"><a href="#更改密码" class="headerlink" title="更改密码"></a>更改密码</h3><p>要更改您的Kerberos密码，请使用kpasswd命令。它会要求您输入您的旧密码（以防止别人在不在线时更换电脑，并更改密码），然后再提示两次。（您必须键入两次的原因是确保您已正确键入。）例如，用户david将执行以下操作：</p>
<p><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">shell% kpasswd</div><div class="line">Password for david:    &lt;- Type your old password.</div><div class="line">Enter new password:    &lt;- Type your new password.</div><div class="line">Enter it again:  &lt;- Type the new password again.</div><div class="line">Password changed.</div><div class="line">shell%</div></pre></td></tr></table></figure>
</p>
<p>更改密码后，更改将在系统中传播一段时间。根据您的系统设置，这可能是几分钟到一个小时或更长的时间。如果您需要在更改密码后立即获得新的Kerberos门票，请尝试使用新密码。如果新密码不起作用，请再次尝试使用旧密码。</p>
<h3 id="授予访问您的帐户"><a href="#授予访问您的帐户" class="headerlink" title="授予访问您的帐户"></a>授予访问您的帐户</h3><p>如果您需要授权某人登录您的帐户，您可以通过Kerberos进行登录，而不会告诉该用户您的密码。只需在您的home目录中创建一个名为.k5login的文件。此文件应包含您希望访问的每个人的Kerberos主体。每个principal必须每行分开。这是一个.k5login文件的示例：</p>
<p><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">jennifer@ATHENA.MIT.EDU</div><div class="line">david@EXAMPLE.COM</div></pre></td></tr></table></figure>
</p>
<p>该文件允许用户jennifer和david使用您的用户ID，前提是他们在各自的领域中具有Kerberos门票。如果您将通过网络登录到其他主机，则需要在每个主机上的.k5login文件中包含您自己的Kerberos主体。</p>
<p>使用.k5login文件比发出密码要安全得多，因为：</p>
<ul>
<li>您可以随时通过从.k5login文件中删除主体来随时访问。</li>
<li>虽然用户可以在一个特定的主机上完全访问您的帐户（或者如果您的.k5login文件被共享，例如，通过NFS），该用户不会继承您的网络权限。</li>
<li>Kerberos保留谁获得门票的日志，所以系统管理员可以在必要时了解谁能够在特定时间使用您的用户ID。<br>一个常见的应用是在root的主目录中设置一个.k5login文件，将该机器的root访问权限授予列出的Kerberos主体。这允许系统管理员允许用户在本地进行本地登录，或者以root用户身份远程登录，而无需发出root密码，无需任何人在网络上键入root密码。</li>
</ul>
<h2 id="票务管理"><a href="#票务管理" class="headerlink" title="票务管理"></a>票务管理</h2><p>在许多系统上，Kerberos内置在登录程序中，您可以在登录时自动获取票证。其他程序（如ssh）可将票据的副本转发到远程主机。大部分这些节目也会在退出时自动销毁您的门票。但是，麻省理工学院建议您通过它们，销毁您的Kerberos门票。一种方法是将kdestroy命令添加到.logout文件中。另外，如果您将远离机器，并且关心使用您的权限的入侵者，最安全的方式是销毁所有票证副本，或使用锁定屏幕的屏幕保护程序。</p>
<h3 id="Kerberos票证属性"><a href="#Kerberos票证属性" class="headerlink" title="Kerberos票证属性"></a>Kerberos票证属性</h3><p><a href="http://web.mit.edu/kerberos/www/krb5-latest/doc/user/tkt_mgmt.html" target="_blank" rel="external">http://web.mit.edu/kerberos/www/krb5-latest/doc/user/tkt_mgmt.html</a></p>
<h3 id="获得门票使用kinit"><a href="#获得门票使用kinit" class="headerlink" title="获得门票使用kinit"></a>获得门票使用kinit</h3><p>如果站点已将Kerberos V5与登录系统集成，登录时将自动获取Kerberos票证，否则您可能需要使用kinit程序显式获取Kerberos票证 。同样，如果您的Kerberos票证过期，请使用kinit程序获取新的。</p>
<p>要使用kinit程序，只需键入kinit，然后在提示符下键入密码。例如，Jennifer（其用户名为 jennifer）适用于Bleep，Inc.（一家域名为mit.edu和Kerberos领域ATHENA.MIT.EDU的虚构公司）。她会输入：<br><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">shell% kinit</div><div class="line">Password for jennifer@ATHENA.MIT.EDU: &lt;-- [Type jennifer&apos;s password here.]</div><div class="line">shell%</div></pre></td></tr></table></figure></p>
<p>默认情况下，kinit假定您希望在您的默认领域使用自己的用户名。假设詹妮弗的朋友大卫正在拜访，他想借一个窗口来检查他的邮件。大卫需要在自己的领域EXAMPLE.COM上获得门票。他会输入：<br><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">shell% kinit david@EXAMPLE.COM</div><div class="line">Password for david@EXAMPLE.COM: &lt;-- [Type david&apos;s password here.]</div><div class="line">shell%</div></pre></td></tr></table></figure></p>
<p>大卫然后会有门票，他可以用来登录自己的机器。请注意，他在Jennifer的机器上键入了他的密码，但是并没有通过网络。本地主机上的Kerberos对其他领域的KDC执行身份验证。</p>
<p>如果您想要将您的门票转发给另一个主机，则需要申请可转寄的ticket。您可以通过指定-f选项来执行此操作 ：<br><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">shell% kinit -f</div><div class="line">Password for jennifer@ATHENA.MIT.EDU: &lt;-- [Type your password here.]</div><div class="line">shell%</div></pre></td></tr></table></figure></p>
<p>请注意，kinit并没有告诉你它获得了可前往的门票; 您可以使用klist命令验证这一点<br>通常，您的ticket对您的系统的默认ticket生命周期有好处，这在许多系统上是十小时。您可以使用-l选项指定不同的票证生存期。将字母s添加 到秒数，m表示分钟，h表示小时，或 d表示天数。例如，要获得david@EXAMPLE.COM的可前往三个小时的优惠 券，您可以输入：<br><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">shell% kinit -f -l 3h david@EXAMPLE.COM</div><div class="line">Password for david@EXAMPLE.COM: &lt;-- [Type david&apos;s password here.]</div><div class="line">shell%</div></pre></td></tr></table></figure></p>
<p>不能混合单位; 指定3h30m的寿命将导致错误。还要注意，大多数系统指定最大票证生存期。如果您要求更长的使用寿命，则会自动将其截断到最长生命周期。</p>
<h3 id="用klist查看门票"><a href="#用klist查看门票" class="headerlink" title="用klist查看门票"></a>用klist查看门票</h3><p>The klist command shows your tickets. When you first obtain tickets, you will have only the ticket-granting ticket. The listing would look like this:<br><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">shell% klist</div><div class="line">Ticket cache: /tmp/krb5cc_ttypa</div><div class="line">Default principal: jennifer@ATHENA.MIT.EDU</div><div class="line"></div><div class="line">Valid starting     Expires            Service principal</div><div class="line">06/07/04 19:49:21  06/08/04 05:49:19  krbtgt/ATHENA.MIT.EDU@ATHENA.MIT.EDU</div><div class="line">shell%</div></pre></td></tr></table></figure></p>
<p>票据缓存是您的票证文件的位置。在上面的例子中，这个文件被命名为/ tmp / krb5cc_ttypa。默认主体是您的Kerberos主体。</p>
<p>“有效开始”和“到期”字段描述票证有效的时间段。“服务主体”描述每张票。票证授予票有第一个组件 krbtgt，第二个组成部分是领域名称。</p>
<p>现在，如果jennifer连接到机器daffodil.mit.edu，然后再次键入“klist”，她会得到以下结果：<br><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">shell% klist</div><div class="line">Ticket cache: /tmp/krb5cc_ttypa</div><div class="line">Default principal: jennifer@ATHENA.MIT.EDU</div><div class="line"></div><div class="line">Valid starting     Expires            Service principal</div><div class="line">06/07/04 19:49:21  06/08/04 05:49:19  krbtgt/ATHENA.MIT.EDU@ATHENA.MIT.EDU</div><div class="line">06/07/04 20:22:30  06/08/04 05:49:19  host/daffodil.mit.edu@ATHENA.MIT.EDU</div><div class="line">shell%</div></pre></td></tr></table></figure></p>
<p>jennifer used ssh to connect to the host daffodil.mit.edu, the ssh program presented her ticket-granting ticket to the KDC and requested a host ticket for the host daffodil.mit.edu. The KDC sent the host ticket, which ssh then presented to the host daffodil.mit.edu, and she was allowed to log in without typing her password.</p>
<p>Suppose your Kerberos tickets allow you to log into a host in another domain, such as trillium.example.com, which is also in another Kerberos realm, EXAMPLE.COM. If you ssh to this host, you will receive a ticket-granting ticket for the realm EXAMPLE.COM, plus the new host ticket for trillium.example.com. klist will now show:</p>
<p><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">shell% klist</div><div class="line">Ticket cache: /tmp/krb5cc_ttypa</div><div class="line">Default principal: jennifer@ATHENA.MIT.EDU</div><div class="line"></div><div class="line">Valid starting     Expires            Service principal</div><div class="line">06/07/04 19:49:21  06/08/04 05:49:19  krbtgt/ATHENA.MIT.EDU@ATHENA.MIT.EDU</div><div class="line">06/07/04 20:22:30  06/08/04 05:49:19  host/daffodil.mit.edu@ATHENA.MIT.EDU</div><div class="line">06/07/04 20:24:18  06/08/04 05:49:19  krbtgt/EXAMPLE.COM@ATHENA.MIT.EDU</div><div class="line">06/07/04 20:24:18  06/08/04 05:49:19  host/trillium.example.com@EXAMPLE.COM</div><div class="line">shell%</div></pre></td></tr></table></figure>
</p>
<p>Here is a sample listing. In this example, the user jennifer obtained her initial tickets (I), which are forwardable (F) and postdated (d) but not yet validated (i)</p>
<p><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">shell% klist -f</div><div class="line">Ticket cache: /tmp/krb5cc_320</div><div class="line">Default principal: jennifer@ATHENA.MIT.EDU</div><div class="line"></div><div class="line">Valid starting      Expires             Service principal</div><div class="line">31/07/05 19:06:25  31/07/05 19:16:25  krbtgt/ATHENA.MIT.EDU@ATHENA.MIT.EDU</div><div class="line">        Flags: FdiI</div><div class="line">shell%</div></pre></td></tr></table></figure>
</p>
<h3 id="用kdestroy摧毁门票"><a href="#用kdestroy摧毁门票" class="headerlink" title="用kdestroy摧毁门票"></a>用kdestroy摧毁门票</h3><p>您的Kerberos门票证明您确实是您自己，如果有人访问存储他们的计算机，门票可能被盗。如果发生这种情况，拥有它们的人可以像你一样伪装，直到它们过期。因此，当您远离计算机时，您应该销毁您的Kerberos门票。</p>
<p>销毁你的票很容易。只需键入kdestroy：<br><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">shell% kdestroy</div><div class="line">shell%</div></pre></td></tr></table></figure></p>
<h2 id="用户配置文件"><a href="#用户配置文件" class="headerlink" title="用户配置文件"></a>用户配置文件</h2><h3 id="k5login"><a href="#k5login" class="headerlink" title=".k5login"></a>.k5login</h3><h4 id="说明"><a href="#说明" class="headerlink" title="说明"></a>说明</h4><p>位于用户主目录中的.k5login文件包含Kerberos主体的列表。具有该文件中的主体的有效票证的任何人都可以使用其文件所在的主目录中的用户的UID进行主机访问。一个常见的用法是将.k5login文件放在root的主目录中，从而授予系统管理员通过Kerberos对主机进行远程root访问。</p>
<h4 id="示例"><a href="#示例" class="headerlink" title="示例"></a>示例</h4><p>Suppose the user alice had a .k5login file in her home directory containing just the following line:<br><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">bob@FOOBAR.ORG</div></pre></td></tr></table></figure></p>
<p>This would allow bob to use Kerberos network applications, such as ssh(1), to access alice‘s account, using bob‘s Kerberos tickets. In a default configuration (with k5login_authoritative set to true in krb5.conf), this .k5login file would not let alice use those network applications to access her account, since she is not listed! With no .k5login file, or with k5login_authoritative set to false, a default rule would permit the principal alice in the machine’s default realm to access the alice account.<br>让我们进一步假设爱丽丝是系统管理员。Alice和其他系统管理员将在每个主机的root的.k5login文件中拥有其主体：<br><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">alice@BLEEP.COM</div><div class="line"></div><div class="line">joeadmin/root@BLEEP.COM</div></pre></td></tr></table></figure></p>
<p>这将允许系统管理员使用其Kerberos票证登录到这些主机，而不必键入root密码。Note that because bob retains the Kerberos tickets for his own principal, bob@FOOBAR.ORG, he would not have any of the privileges that require alice‘s tickets, such as root access to any of the site’s hosts, or the ability to change alice‘s password.</p>
<h3 id="k5identity"><a href="#k5identity" class="headerlink" title=".k5identity"></a>.k5identity</h3><p>驻留在用户主目录中的.k5identity文件包含基于正在访问的服务器选择客户端主体的规则列表。这些规则用于在可能时选择缓存集合中的凭据高速缓存。</p>
<p>以＃开头的空行和行将被忽略。每行的格式如下：<br><code>principal field=value ...</code><br>如果服务器主体满足所有的字段约束，则选择principal作为客户主体。以下字段被识别：</p>
<ul>
<li>领域<br>如果服务器主体的领域是已知的，则它与值匹配，这可能是使用shell通配符的模式。对于基于主机的服务器主体，通常只有在krb5.conf中有一个<em>domain_realm</em>部分的主机名映射时，这个领域通常才会被知道。</li>
<li>服务<br>如果服务器主体是基于主机的主体，则其服务组件与值匹配，该值可能是使用shell通配符的模式。</li>
<li>host<br>如果服务器主体是基于主机的主体，则其主机名组件将转换为小写并与值匹配，该值可能是使用shell通配符的模式。<br>如果服务器主体与.k5identity文件中的多个行的约束匹配，则使用第一个匹配行中的主体。如果没有行匹配，凭证将被选择其他方式，例如领域启发式或当前主缓存。</li>
</ul>
<h4 id="示例-1"><a href="#示例-1" class="headerlink" title="示例"></a>示例</h4><p> .k5identity file selects the client principal alice@KRBTEST.COM if the server principal is within that realm, the principal alice/root@EXAMPLE.COM if the server host is within a servers subdomain, and the principal alice/mail@EXAMPLE.COM when accessing the IMAP service on mail.example.com:</p>
<p><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">alice@KRBTEST.COM       realm=KRBTEST.COM</div><div class="line">alice/root@EXAMPLE.COM  host=*.servers.example.com</div><div class="line">alice/mail@EXAMPLE.COM  host=mail.example.com service=imap</div></pre></td></tr></table></figure>
</p>
<h2 id="用户命令"><a href="#用户命令" class="headerlink" title="用户命令"></a>用户命令</h2><ul>
<li>kdestroy</li>
<li>kinit</li>
<li>klist</li>
<li>kpasswd</li>
<li>krb5-config</li>
<li>ksu</li>
<li>kswitch</li>
<li>kvno</li>
<li>sclient</li>
</ul>
]]></content>
      
        <categories>
            
            <category> CDH </category>
            
            <category> Kerberos MIT </category>
            
        </categories>
        
        
    </entry>
    
    <entry>
      <title><![CDATA[spark提交任务失败]]></title>
      <url>/CDH/Error/spark%E6%8F%90%E4%BA%A4%E4%BB%BB%E5%8A%A1%E5%A4%B1%E8%B4%A5.html</url>
      <content type="html"><![CDATA[<h1 id="spark提交任务失败"><a href="#spark提交任务失败" class="headerlink" title="spark提交任务失败"></a>spark提交任务失败</h1><p>报错信息:java.lang.NoClassDefFoundError: org/apache/hadoop/fs/FSDataInputStream<br>开发使用的环境是spark1.6<br>经过排查发现spark-shell 在这台机器上也会报相同的错误<br>发现在角色组中没有在这台机器上添加spark gateway 添加后,更新客户端配置解决该问题</p>
<p><a href="https://community.cloudera.com/t5/Cloudera-Manager-Installation/Unable-to-run-Spark-on-Cloudera-Manager-Single-User-Mode-with/td-p/45114" target="_blank" rel="external">https://community.cloudera.com/t5/Cloudera-Manager-Installation/Unable-to-run-Spark-on-Cloudera-Manager-Single-User-Mode-with/td-p/45114</a></p>
]]></content>
      
        <categories>
            
            <category> CDH </category>
            
            <category> Error </category>
            
        </categories>
        
        
    </entry>
    
    <entry>
      <title><![CDATA[JournalNode同步问题]]></title>
      <url>/CDH/Error/JournalNode%E5%90%8C%E6%AD%A5%E9%97%AE%E9%A2%98.html</url>
      <content type="html"><![CDATA[<h1 id="JournalNode-与namenode同步问题"><a href="#JournalNode-与namenode同步问题" class="headerlink" title="JournalNode 与namenode同步问题"></a>JournalNode 与namenode同步问题</h1><p><a href="https://github.com/mattshma/bigdata/issues/45" target="_blank" rel="external">https://github.com/mattshma/bigdata/issues/45</a><br><a href="http://blog.csdn.net/bdchome/article/details/52550519" target="_blank" rel="external">http://blog.csdn.net/bdchome/article/details/52550519</a><br>因为开发误操作导致集群中缺少了JournalNode和Zookeeper节点各一个,在集群中其他机器上补上缺失的角色后,zk选举正常,但是新添加的JournalNode节点和活动的namenode节点之间同步失败,<br>查询日志后发现后<br><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">java.io.FileNotFoundException: /hadoop/dfs/jn/myhadoop/current/last-promised-epoch.tmp (No such file or directory)</div></pre></td></tr></table></figure></p>
<p><code>/hadoop/dfs/jn</code> 是jn的存放目录,我的做法是先停止journalnode,然后从另外两台正常jn节点上复制jn路径下的其他文件,同时检查是否有current文件夹.如果没有新建一个后,修改路径权限<code>chown -R hdfs:hadoop /hadoop/dfs/jn/</code> 后重启jn节点 </p>
]]></content>
      
        <categories>
            
            <category> CDH </category>
            
            <category> Error </category>
            
        </categories>
        
        
    </entry>
    
    <entry>
      <title><![CDATA[SSH 登录错误]]></title>
      <url>/CDH/Error/SSH%20%E7%99%BB%E5%BD%95%E9%94%99%E8%AF%AF.html</url>
      <content type="html"><![CDATA[<h1 id="SSH-登录错误"><a href="#SSH-登录错误" class="headerlink" title="SSH 登录错误"></a>SSH 登录错误</h1><p>SSH 登录时出现如下错误：ssh_exchange_identification: read: Connection reset by peer</p>
<p>可能的原因:Linux 系统通过 /etc/hosts.allow 或 /etc/hosts.deny，启用了 TCP Wrapper  访问控制所致。<br>可以通过ssh -v查看连接时详情<code>ssh -v root@ip地址或域名</code></p>
<ol>
<li>通过带外进入系统</li>
<li>通过 cat 等指令查看 /etc/hosts.allow 或 /etc/hosts.deny中是否包含类似如下配置：<code>all:all:deny</code></li>
<li>如果需要修改相关策略配置，在继续之前建议进行文件备份。</li>
<li>使用 vi 等编辑器，按需修改 /etc/hosts.allow 和 /etc/hosts.deny中的相关配置，或者整个删除或注释（在最开头添加 # 号）整行配置。或者改为<code>sshd: ALL</code></li>
<li>重启ssh <code>service sshd restart</code></li>
<li>结果发现丢失 /var/lib/empty 把其他节点的目录复制过来后重启解决</li>
</ol>
]]></content>
      
        <categories>
            
            <category> CDH </category>
            
            <category> Error </category>
            
        </categories>
        
        
    </entry>
    
    <entry>
      <title><![CDATA[动态资源分配]]></title>
      <url>/CDH/Spark/%E5%8A%A8%E6%80%81%E8%B5%84%E6%BA%90%E5%88%86%E9%85%8D.html</url>
      <content type="html"><![CDATA[<h2 id="动态资源分配"><a href="#动态资源分配" class="headerlink" title="动态资源分配"></a>动态资源分配</h2><p>Spark提供了一种机制，可以根据工作负载动态调整应用程序占用的资源。这意味着如果您的应用程序不再使用，您的应用程序可能会将资源返回给群集，并在需要时再次请求它们。如果多个应用程序在Spark群集中共享资源，则此功能特别有用。</p>
<p>默认情况下禁用此功能，并且可在所有粗粒度集群管理器上使用此功能，例如 独立模式，YARN模式和 Mesos粗粒度模式。</p>
<h2 id="配置和设置"><a href="#配置和设置" class="headerlink" title="配置和设置"></a>配置和设置</h2><p>使用此功能有两个要求。首先，您的应用程序必须设置 spark.dynamicAllocation.enabled为true。其次，您必须 在同一集群中的每个工作节点上设置一个外部随机播放服务，并spark.shuffle.service.enabled在应用程序中设置为true。外部shuffle服务的目的是允许删除执行程序，而不删除它们写入的shuffle文件（下面将详细介绍 ）。集群管理器的设置方式各不相同：</p>
<p>在独立模式下，只需启动您的工作人员spark.shuffle.service.enabled即可true。</p>
<p>在Mesos粗粒度模式下，$SPARK_HOME/sbin/start-mesos-shuffle-service.sh在spark.shuffle.service.enabled设置为所有从节点上运行true。例如，你可以通过马拉松来这样做。</p>
<p>在YARN模式下，请按照此处的说明进行操作。</p>
<p>所有其他相关的配置是可选的，下spark.dynamicAllocation.<em>和 spark.shuffle.service.</em>命名空间</p>
<h2 id="资源分配政策"><a href="#资源分配政策" class="headerlink" title="资源分配政策"></a>资源分配政策</h2><p>在高层次上，Spark应该在不再使用执行者时放弃执行者，并在需要时获取执行者。既然没有明确的方法来预测即将被删除的执行者是否会在不久的将来执行一个任务，还是将要添加的新的执行者实际上是空闲的，那么我们需要一套启发式来确定何时删除并请求执行者。</p>
<h3 id="请求政策"><a href="#请求政策" class="headerlink" title="请求政策"></a>请求政策</h3><p>启用动态分配的Spark应用程序在等待等待调度的任务中请求附加执行程序。这种情况必然意味着现有的执行者集合不足以同时使所有已经提交但尚未完成的任务饱和。</p>
<p>Spark要求执行者轮回。当有待处理的任务持续spark.dynamicAllocation.schedulerBacklogTimeout几秒钟时，触发实际请求，spark.dynamicAllocation.sustainedSchedulerBacklogTimeout然后如果挂起的任务队列仍然存在，则每秒钟再次触发。此外，每轮要求的执行人数从上一轮呈指数级增长。例如，一个应用程序将在第一轮中添加1个执行者，然后在后续的回合中添加2,4,8个等等执行者。</p>
<p>指数增长政策的动机是双重的。首先，一个应用程序应该在开始时谨慎地请求执行者，以防只有少数额外的执行者是足够的。这回应了TCP慢启动的理由。第二，应用程序应该能够及时提高其资源使用情况，以证明实际需要许多执行者。</p>
]]></content>
      
        <categories>
            
            <category> CDH </category>
            
            <category> Spark </category>
            
        </categories>
        
        
    </entry>
    
    <entry>
      <title><![CDATA[spark-sql添加]]></title>
      <url>/CDH/Error/Spark-sql%20%E6%B7%BB%E5%8A%A0.html</url>
      <content type="html"><![CDATA[<p>CDH版本的spark阉割了spark-sql,可以重新编译cloudera/spark,或者直接在启动时添加所缺失的jar包</p>
<blockquote>
<p>操作系统:SUSE11 SP3<br>Spark版本:SPARK2-2.1.0.cloudera1-1.cdh5.7.0.p0.120904<br>CDH版本:CDH5.10.041<br>使用root用户对集群进行部署</p>
<h2 id="直接添加所缺jar包"><a href="#直接添加所缺jar包" class="headerlink" title="直接添加所缺jar包"></a>直接添加所缺jar包</h2><h3 id="下载Apache版Spark"><a href="#下载Apache版Spark" class="headerlink" title="下载Apache版Spark"></a>下载Apache版Spark</h3><p>解压后找到jars/目录下的<br><code>hive-cli-1.2.1.spark2.jar</code><br><code>spark-hive-thriftserver_2.11-2.1.0.jar</code><br>上传需要使用spark-sql的机器上</p>
<h3 id="创建spark-sql"><a href="#创建spark-sql" class="headerlink" title="创建spark-sql"></a>创建spark-sql</h3><p>在/opt/cloudera/parcels/SPARK2-2.1.0.cloudera1-1.cdh5.7.0.p0.120904/bin/下新建<strong>spark-sql</strong>文件<br><code>vi /opt/cloudera/parcels/SPARK2-2.1.0.cloudera1-1.cdh5.7.0.p0.120904/bin/spark-sql</code></p>
</blockquote>
<p><figure class="highlight bash"><table><tr><td class="code"><pre><div class="line"><span class="meta">#!/bin/bash</span></div><div class="line">  <span class="comment"># Reference: http://stackoverflow.com/questions/59895/can-a-bash-script-tell-what-directory-its-stored-in</span></div><div class="line">  SOURCE=<span class="string">"<span class="variable">$&#123;BASH_SOURCE[0]&#125;</span>"</span></div><div class="line">  BIN_DIR=<span class="string">"<span class="variable">$( dirname "$SOURCE" )</span>"</span></div><div class="line">  <span class="keyword">while</span> [ -h <span class="string">"<span class="variable">$SOURCE</span>"</span> ]</div><div class="line">  <span class="keyword">do</span></div><div class="line">    SOURCE=<span class="string">"<span class="variable">$(readlink "$SOURCE")</span>"</span></div><div class="line">    [[ <span class="variable">$SOURCE</span> != /* ]] &amp;&amp; SOURCE=<span class="string">"<span class="variable">$DIR</span>/<span class="variable">$SOURCE</span>"</span></div><div class="line">    BIN_DIR=<span class="string">"<span class="variable">$( cd -P "$( dirname "$SOURCE"  )</span>"</span> &amp;&amp; <span class="built_in">pwd</span> )<span class="string">"</span></div><div class="line">  done</div><div class="line">  BIN_DIR="$( <span class="built_in">cd</span> -P <span class="string">"<span class="variable">$( dirname "$SOURCE" )</span>"</span> &amp;&amp; <span class="built_in">pwd</span> )<span class="string">"</span></div><div class="line">  CDH_LIB_DIR=<span class="variable">$BIN_DIR</span>/../../CDH/lib</div><div class="line">  LIB_DIR=<span class="variable">$BIN_DIR</span>/../lib</div><div class="line">export HADOOP_HOME=<span class="variable">$CDH_LIB_DIR</span>/hadoop</div><div class="line"></div><div class="line"># Autodetect JAVA_HOME if not defined</div><div class="line">. <span class="variable">$CDH_LIB_DIR</span>/bigtop-utils/bigtop-detect-javahome</div><div class="line"></div><div class="line">exec <span class="variable">$LIB_DIR</span>/spark2/bin/spark-sql "<span class="variable">$@</span><span class="string">"</span></div></pre></td></tr></table></figure>
</p>
<p>在/opt/cloudera/parcels/SPARK2-2.1.0.cloudera1-1.cdh5.7.0.p0.120904/lib/spark2/bin下新建spark-sql,同时添加之前传的两个jar包<br><code>vi /opt/cloudera/parcels/SPARK2-2.1.0.cloudera1-1.cdh5.7.0.p0.120904/lib/spark2/bin/spark-sql</code><br><figure class="highlight bash"><table><tr><td class="code"><pre><div class="line"><span class="meta">#!/usr/bin/env bash</span></div><div class="line"></div><div class="line"><span class="comment">#</span></div><div class="line"><span class="comment"># Licensed to the Apache Software Foundation (ASF) under one or more</span></div><div class="line"><span class="comment"># contributor license agreements.  See the NOTICE file distributed with</span></div><div class="line"><span class="comment"># this work for additional information regarding copyright ownership.</span></div><div class="line"><span class="comment"># The ASF licenses this file to You under the Apache License, Version 2.0</span></div><div class="line"><span class="comment"># (the "License"); you may not use this file except in compliance with</span></div><div class="line"><span class="comment"># the License.  You may obtain a copy of the License at</span></div><div class="line"><span class="comment">#</span></div><div class="line"><span class="comment">#    http://www.apache.org/licenses/LICENSE-2.0</span></div><div class="line"><span class="comment">#</span></div><div class="line"><span class="comment"># Unless required by applicable law or agreed to in writing, software</span></div><div class="line"><span class="comment"># distributed under the License is distributed on an "AS IS" BASIS,</span></div><div class="line"><span class="comment"># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span></div><div class="line"><span class="comment"># See the License for the specific language governing permissions and</span></div><div class="line"><span class="comment"># limitations under the License.</span></div><div class="line"><span class="comment">#</span></div><div class="line"></div><div class="line"><span class="keyword">if</span> [ -z <span class="string">"<span class="variable">$&#123;SPARK_HOME&#125;</span>"</span> ]; <span class="keyword">then</span></div><div class="line">  <span class="built_in">source</span> <span class="string">"<span class="variable">$(dirname "$0")</span>"</span>/find-spark-home</div><div class="line"><span class="keyword">fi</span></div><div class="line"></div><div class="line"><span class="built_in">export</span> _SPARK_CMD_USAGE=<span class="string">"Usage: ./bin/spark-sql [options] [cli option]"</span></div><div class="line"><span class="comment">#exec "$&#123;SPARK_HOME&#125;"/bin/spark-submit --class org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver "$@"</span></div><div class="line"><span class="built_in">exec</span> <span class="string">"<span class="variable">$&#123;SPARK_HOME&#125;</span>"</span>/bin/spark-submit --jars /home/manage/spark-hive-thriftserver_2.11-2.1.0.jar,/home/manage/hive-cli-1.2.1.spark2.jar --class org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver <span class="string">"<span class="variable">$@</span>"</span></div></pre></td></tr></table></figure></p>
<p>同时可以设置一个别名指向spark-sql<br><code>alias spark2-sql=&quot;/opt/cloudera/parcels/SPARK2-2.1.0.cloudera1-1.cdh5.7.0.p0.120904/bin/spark-sql&quot;</code></p>
<h3 id="在log4j中添加spark-sql"><a href="#在log4j中添加spark-sql" class="headerlink" title="在log4j中添加spark-sql"></a>在log4j中添加spark-sql</h3><p>路径:/opt/cloudera/parcels/SPARK2-2.1.0.cloudera1-1.cdh5.7.0.p0.120904/lib/spark2/conf<br><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">log4j.logger.org.apache.spark.sql.SQLContext=ERROR </div><div class="line">log4j.logger.org.apache.spark.sql.catalyst.analysis.Analyzer=ERROR</div></pre></td></tr></table></figure></p>
<h3 id="spark1-6-spark-sql支持"><a href="#spark1-6-spark-sql支持" class="headerlink" title="spark1.6 spark-sql支持"></a>spark1.6 spark-sql支持</h3><p><a href="http://www.javali.org/bigdata/one-trick-on-supporting-sparksql-in-cdh5.html" target="_blank" rel="external">spark1.6 spark-sql支持</a><br>其实就是对应的jar包不同,1.6所缺的为spark-assembly-1.6.1-hadoop2.6.0.jar</p>
<h3 id="SparkHiveServer"><a href="#SparkHiveServer" class="headerlink" title="SparkHiveServer"></a>SparkHiveServer</h3><p>直到最近在调研Spark并计划将Spark取代Mapreduce来提升平台的计算效率时，发现Spark-sql能完美的兼容Hive SQL，同时还提供了ThriftServer(就是SparkHiveServer)，不止于此，由于Spark更好的使用了内存，期执行效率是MR/Hive的10倍以上。</p>
<p>其实就是在Spark集群上执行$SPARK_HOME/sbin/start-thriftserver.sh –master=spark://MASTER:7077 就默认开启了10000端口，该服务可以取代hiveserver2，如果与HiveServer2在同一台服务器上，可以先shutdown hiveserver2,再启动spark thriftserver。运行了1个礼拜，服务非常稳定，GC也正常！<br><code>cat start-thriftserver.sh</code><br><figure class="highlight bash"><table><tr><td class="code"><pre><div class="line"><span class="meta">#!/usr/bin/env bash</span></div><div class="line"></div><div class="line"><span class="comment">#</span></div><div class="line"><span class="comment"># Licensed to the Apache Software Foundation (ASF) under one or more</span></div><div class="line"><span class="comment"># contributor license agreements.  See the NOTICE file distributed with</span></div><div class="line"><span class="comment"># this work for additional information regarding copyright ownership.</span></div><div class="line"><span class="comment"># The ASF licenses this file to You under the Apache License, Version 2.0</span></div><div class="line"><span class="comment"># (the "License"); you may not use this file except in compliance with</span></div><div class="line"><span class="comment"># the License.  You may obtain a copy of the License at</span></div><div class="line"><span class="comment">#</span></div><div class="line"><span class="comment">#    http://www.apache.org/licenses/LICENSE-2.0</span></div><div class="line"><span class="comment">#</span></div><div class="line"><span class="comment"># Unless required by applicable law or agreed to in writing, software</span></div><div class="line"><span class="comment"># distributed under the License is distributed on an "AS IS" BASIS,</span></div><div class="line"><span class="comment"># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span></div><div class="line"><span class="comment"># See the License for the specific language governing permissions and</span></div><div class="line"><span class="comment"># limitations under the License.</span></div><div class="line"><span class="comment">#</span></div><div class="line"></div><div class="line"><span class="comment">#</span></div><div class="line"><span class="comment"># Shell script for starting the Spark SQL Thrift server</span></div><div class="line"></div><div class="line"><span class="comment"># Enter posix mode for bash</span></div><div class="line"><span class="built_in">set</span> -o posix</div><div class="line"></div><div class="line"><span class="keyword">if</span> [ -z <span class="string">"<span class="variable">$&#123;SPARK_HOME&#125;</span>"</span> ]; <span class="keyword">then</span></div><div class="line">  <span class="built_in">export</span> SPARK_HOME=<span class="string">"<span class="variable">$(cd "`dirname "$0"`"/..; pwd)</span>"</span></div><div class="line"><span class="keyword">fi</span></div><div class="line"></div><div class="line"><span class="comment"># <span class="doctag">NOTE:</span> This exact class name is matched downstream by SparkSubmit.</span></div><div class="line"><span class="comment"># Any changes need to be reflected there.</span></div><div class="line">CLASS=<span class="string">"org.apache.spark.sql.hive.thriftserver.HiveThriftServer2"</span></div><div class="line"></div><div class="line"><span class="keyword">function</span> usage &#123;</div><div class="line">  <span class="built_in">echo</span> <span class="string">"Usage: ./sbin/start-thriftserver [options] [thrift server options]"</span></div><div class="line">  pattern=<span class="string">"usage"</span></div><div class="line">  pattern+=<span class="string">"\|Spark assembly has been built with Hive"</span></div><div class="line">  pattern+=<span class="string">"\|NOTE: SPARK_PREPEND_CLASSES is set"</span></div><div class="line">  pattern+=<span class="string">"\|Spark Command: "</span></div><div class="line">  pattern+=<span class="string">"\|======="</span></div><div class="line">  pattern+=<span class="string">"\|--help"</span></div><div class="line"></div><div class="line">  <span class="string">"<span class="variable">$&#123;SPARK_HOME&#125;</span>"</span>/bin/spark-submit --<span class="built_in">help</span> 2&gt;&amp;1 | grep -v Usage 1&gt;&amp;2</div><div class="line">  <span class="built_in">echo</span></div><div class="line">  <span class="built_in">echo</span> <span class="string">"Thrift server options:"</span></div><div class="line">  <span class="string">"<span class="variable">$&#123;SPARK_HOME&#125;</span>"</span>/bin/spark-class <span class="variable">$CLASS</span> --<span class="built_in">help</span> 2&gt;&amp;1 | grep -v <span class="string">"<span class="variable">$pattern</span>"</span> 1&gt;&amp;2</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="keyword">if</span> [[ <span class="string">"<span class="variable">$@</span>"</span> = *--<span class="built_in">help</span> ]] || [[ <span class="string">"<span class="variable">$@</span>"</span> = *-h ]]; <span class="keyword">then</span></div><div class="line">  usage</div><div class="line">  <span class="built_in">exit</span> 0</div><div class="line"><span class="keyword">fi</span></div><div class="line"></div><div class="line"><span class="built_in">export</span> SUBMIT_USAGE_FUNCTION=usage</div><div class="line"></div><div class="line"><span class="built_in">exec</span> <span class="string">"<span class="variable">$&#123;SPARK_HOME&#125;</span>"</span>/sbin/spark-daemon.sh submit <span class="variable">$CLASS</span> 1 --name <span class="string">"Thrift JDBC/ODBC Server"</span> <span class="string">"<span class="variable">$@</span>"</span></div></pre></td></tr></table></figure></p>
<p>后来发现只要把缺少的两个jar包导入到<br>cp activation-1.1.1.jar aopalliance-1.0.jar apacheds-i18n-2.0.0-M15.jar apacheds-kerberos-codec-2.0.0-M15.jar api-asn1-api-1.0.0-M20.jar api-util-1.0.0-M20.jar base64-2.3.8.jar bcprov-jdk15on-1.51.jar commons-beanutils-1.7.0.jar commons-beanutils-core-1.8.0.jar commons-configuration-1.6.jar commons-digester-1.8.jar curator-client-2.6.0.jar curator-framework-2.6.0.jar curator-recipes-2.6.0.jar guava-14.0.1.jar guice-3.0.jar guice-servlet-3.0.jar hadoop-annotations-2.6.4.jar hadoop-auth-2.6.4.jar hadoop-client-2.6.4.jar hadoop-common-2.6.4.jar hadoop-hdfs-2.6.4.jar hadoop-mapreduce-client-app-2.6.4.jar hadoop-mapreduce-client-common-2.6.4.jar hadoop-mapreduce-client-core-2.6.4.jar hadoop-mapreduce-client-jobclient-2.6.4.jar hadoop-mapreduce-client-shuffle-2.6.4.jar hadoop-yarn-api-2.6.4.jar hadoop-yarn-client-2.6.4.jar hadoop-yarn-common-2.6.4.jar hadoop-yarn-server-common-2.6.4.jar hadoop-yarn-server-web-proxy-2.6.4.jar hive-beeline-1.2.1.spark2.jar hive-exec-1.2.1.spark2.jar hive-jdbc-1.2.1.spark2.jar htrace-core-3.0.4.jar httpclient-4.5.2.jar jackson-core-asl-1.9.13.jar jackson-mapper-asl-1.9.13.jar java-xmlbuilder-1.0.jar javax.inject-1.jar jaxb-api-2.2.2.jar jets3t-0.9.3.jar jetty-6.1.26.jar jsr305-1.3.9.jar leveldbjni-all-1.8.jar mail-1.4.7.jar mesos-1.0.0-shaded-protobuf.jar mx4j-3.0.2.jar protobuf-java-2.5.0.jar slf4j-api-1.7.16.jar slf4j-log4j12-1.7.16.jar snappy-java-1.1.2.6.jar spark-mesos_2.11-2.1.1.jar stax-api-1.0-2.jar super-csv-2.2.0.jar xercesImpl-2.9.1.jar xmlenc-0.52.jar zookeeper-3.4.6.jar /opt/cloudera/parcels/SPARK2-2.0.0.cloudera2-1.cdh5.7.0.p0.118100/lib/spark2/jars/ </p>
<p>SparkSQL accesses its metadata via the HMS directly, and does not go through a HS2, so it does not truly get covered fully by Sentry. However, in a Sentry setup the HMS is write-protected via the Sentry Authz Plugin added on it, so DDLs are still protected against, but users can still view all metadata (i.e. they can run SHOW TABLES, SHOW DATABASES, etc. and retrieve full listing [1]).</p>
<p>With Sentry HMS plugin and Sentry HDFS ACL Sync enabled, access to tables’ data by Spark programs would be limited to the same rules as your Beeline/other Hive clients would.</p>
<p><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">cp activation-1.1.1.jar aopalliance-1.0.jar apacheds-i18n-2.0.0-M15.jar apacheds-kerberos-codec-2.0.0-M15.jar api-asn1-api-1.0.0-M20.jar api-util-1.0.0-M20.jar base64-2.3.8.jar bcprov-jdk15on-1.51.jar commons-beanutils-1.7.0.jar commons-beanutils-core-1.8.0.jar commons-configuration-1.6.jar commons-digester-1.8.jar curator-client-2.6.0.jar curator-framework-2.6.0.jar curator-recipes-2.6.0.jar guava-14.0.1.jar guice-3.0.jar guice-servlet-3.0.jar hadoop-annotations-2.6.4.jar hadoop-auth-2.6.4.jar hadoop-client-2.6.4.jar hadoop-common-2.6.4.jar hadoop-hdfs-2.6.4.jar hadoop-mapreduce-client-app-2.6.4.jar hadoop-mapreduce-client-common-2.6.4.jar hadoop-mapreduce-client-core-2.6.4.jar hadoop-mapreduce-client-jobclient-2.6.4.jar hadoop-mapreduce-client-shuffle-2.6.4.jar hadoop-yarn-api-2.6.4.jar hadoop-yarn-client-2.6.4.jar hadoop-yarn-common-2.6.4.jar hadoop-yarn-server-common-2.6.4.jar hadoop-yarn-server-web-proxy-2.6.4.jar htrace-core-3.0.4.jar httpclient-4.5.2.jar jackson-core-asl-1.9.13.jar jackson-mapper-asl-1.9.13.jar java-xmlbuilder-1.0.jar javax.inject-1.jar jaxb-api-2.2.2.jar jets3t-0.9.3.jar jetty-6.1.26.jar jsr305-1.3.9.jar leveldbjni-all-1.8.jar mail-1.4.7.jar mesos-1.0.0-shaded-protobuf.jar mx4j-3.0.2.jar protobuf-java-2.5.0.jar slf4j-api-1.7.16.jar slf4j-log4j12-1.7.16.jar snappy-java-1.1.2.6.jar spark-mesos_2.11-2.1.1.jar stax-api-1.0-2.jar super-csv-2.2.0.jar xercesImpl-2.9.1.jar xmlenc-0.52.jar zookeeper-3.4.6.jar /opt/cloudera/parcels/SPARK2-2.0.0.cloudera2-1.cdh5.7.0.p0.118100/lib/spark2/jars/ </div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line">hive-beeline-1.2.1.spark2.jar hive-exec-1.2.1.spark2.jar hive-jdbc-1.2.1.spark2.jar </div><div class="line"></div><div class="line"></div><div class="line">cp /opt/spark-2.1.1-bin-hadoop2.6/jars/hive-jdbc-1.2.1.spark2.jar /opt/cloudera/parcels/SPARK2-2.0.0.cloudera2-1.cdh5.7.0.p0.118100/lib/spark2/jars/ </div><div class="line">cp /opt/spark-2.1.1-bin-hadoop2.6/jars/hive-beeline-1.2.1.spark2.jar /opt/cloudera/parcels/SPARK2-2.0.0.cloudera2-1.cdh5.7.0.p0.118100/lib/spark2/jars/ </div><div class="line"></div><div class="line">`rm /opt/cloudera/parcels/SPARK2-2.1.0.cloudera2-1.cdh5.7.0.p0.118100/lib/spark2/jars/hive-exec-1.2.1.spark2.jar`</div><div class="line"></div><div class="line"></div><div class="line">#!/bin/bash</div><div class="line">  SOURCE=&quot;$&#123;BASH_SOURCE[0]&#125;&quot;  #/opt/cloudera/parcels/SPARK2-2.1.0.cloudera1-1.cdh5.7.0.p0.120904/bin/</div><div class="line">  BIN_DIR=&quot;$( dirname &quot;$SOURCE&quot; )&quot; </div><div class="line">  while [ -h &quot;$SOURCE&quot; ]</div><div class="line">  do</div><div class="line">    SOURCE=&quot;$(readlink &quot;$SOURCE&quot;)&quot;</div><div class="line">    [[ $SOURCE != /* ]] &amp;&amp; SOURCE=&quot;$DIR/$SOURCE&quot;</div><div class="line">    BIN_DIR=&quot;$( cd -P &quot;$( dirname &quot;$SOURCE&quot;  )&quot; &amp;&amp; pwd )&quot;</div><div class="line">  done</div><div class="line">  BIN_DIR=&quot;$( cd -P &quot;$( dirname &quot;$SOURCE&quot; )&quot; &amp;&amp; pwd )&quot;</div><div class="line">  CDH_LIB_DIR=$BIN_DIR/../../CDH/lib</div><div class="line">  LIB_DIR=$BIN_DIR/../lib</div><div class="line">export HADOOP_HOME=$CDH_LIB_DIR/hadoop</div><div class="line"></div><div class="line"># Autodetect JAVA_HOME if not defined</div><div class="line">. $CDH_LIB_DIR/bigtop-utils/bigtop-detect-javahome</div><div class="line"></div><div class="line">exec $LIB_DIR/spark2/bin/spark-sql &quot;$@&quot;</div><div class="line"></div><div class="line"></div><div class="line">#!/bin/bash</div><div class="line">export HADOOP_HOME=/opt/cloudera/parcels/CDH/lib/hadoop</div><div class="line">. /opt/cloudera/parcels/CDH/lib/bigtop-utils/bigtop-detect-javahome</div><div class="line">exec $LIB_DIR/spark2/bin/spark-sql &quot;$@&quot;</div></pre></td></tr></table></figure>
</p>
<p>exec $LIB_DIR/spark2/sbin/start-thriftserver.sh –name SparkJDBC –master yarn-client  –num-executors 10 –executor-memory 2g –executor-cores 4 –driver-memory 10g –driver-cores 2  –conf spark.storage.memoryFraction=0.2 –conf spark.shuffle.memoryFraction=0.6 –hiveconf hive.server2.thrift.port=10001 –hiveconf hive.server2.logging.operation.enabled=true –hiveconf hive.server2.authentication.kerberos.principal=hive/ddp-kettle-02.cmdmp.com@CMDMP.COM –hiveconf hive.server2.authentication.kerberos.keytab /root/hiveserver.keytab “$@”</p>
<p>exec $LIB_DIR/spark2/sbin/start-thriftserver.sh –name SparkJDBC –master yarn-client  –hiveconf hive.server2.thrift.port=10001 –hiveconf hive.server2.logging.operation.enabled=true –hiveconf hive.metastore.schema.verification=false –hiveconf hive.server2.authentication.kerberos.principal=hive/ddp-kettle-02.cmdmp.com@CMDMP.COM –hiveconf hive.server2.authentication.kerberos.keytab /root/hiveserver.keytab “$@”</p>
<p>exec $LIB_DIR/spark2/sbin/start-thriftserver.sh –name SparkJDBC –master yarn-client  –hiveconf hive.server2.thrift.port=10000 –hiveconf hive.server2.logging.operation.enabled=true –hiveconf hive.metastore.schema.verification=false –hiveconf spark.sql.thriftServer.incrementalCollect=true –hiveconf hive.server2.authentication.kerberos.principal=hive/ddp-kettle-02.cmdmp.com@CMDMP.COM –hiveconf hive.server2.authentication.kerberos.keytab /root/hiveserver.keytab “$@”</p>
<p>–hiveconf hive.metastore.schema.verification=false<br>–hiveconf spark.sql.thriftServer.incrementalCollect=true<br>–hiveconf hive.support.concurrency=true</p>
<p>master ：指定spark提交模式为yarn-client</p>
<p>hive.server2.thrift.port : 指定thrift server的端口</p>
<p>hive.server2.authentication.kerberos.principal：指定启动thrift server的超级管理员principal，此处超级管理员为hive</p>
<p>hive.server2.authentication.kerberos.keytab : 超级管理员对应的keytab　</p>
]]></content>
      
        <categories>
            
            <category> CDH </category>
            
            <category> Error </category>
            
        </categories>
        
        
    </entry>
    
    <entry>
      <title><![CDATA[Sublime config]]></title>
      <url>/Document/Sublime%20config.html</url>
      <content type="html"><![CDATA[<h1 id="Sublime-个人配置信息"><a href="#Sublime-个人配置信息" class="headerlink" title="Sublime 个人配置信息"></a>Sublime 个人配置信息</h1><h2 id="Preference-sublime-setting-–User"><a href="#Preference-sublime-setting-–User" class="headerlink" title="Preference.sublime-setting –User"></a>Preference.sublime-setting –User</h2><p><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">&#123;</div><div class="line">    &quot;color_scheme&quot;: &quot;Packages/Material Theme/schemes/Material-Theme.tmTheme&quot;,</div><div class="line">    &quot;font_size&quot;: 15,</div><div class="line">    &quot;ignored_packages&quot;:</div><div class="line">    [</div><div class="line">        &quot;Vintage&quot;</div><div class="line">    ],</div><div class="line">    &quot;material_theme_bullet_tree_indicator&quot;: true,</div><div class="line">    &quot;material_theme_compact_sidebar&quot;: true,</div><div class="line">    &quot;spacegray_tabs_font_large&quot;: true,</div><div class="line">    &quot;theme&quot;: &quot;Material-Theme.sublime-theme&quot;,</div><div class="line">    &quot;update_check&quot;: false</div><div class="line">&#125;</div></pre></td></tr></table></figure>
</p>
<h2 id="Markdown-sublime-settings"><a href="#Markdown-sublime-settings" class="headerlink" title="Markdown.sublime-settings"></a>Markdown.sublime-settings</h2><p><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">   &#123;</div><div class="line">    &quot;extensions&quot;:</div><div class="line">    [</div><div class="line">        &quot;md&quot;</div><div class="line">    ],</div><div class="line"></div><div class="line">    // &quot;color_scheme&quot;: &quot;Packages/MarkdownEditing/MarkdownEditor.tmTheme&quot;,</div><div class="line">    // &quot;color_scheme&quot;: &quot;Packages/Theme - Spacegray/base16-eighties.dark.tmTheme&quot;,</div><div class="line">    &quot;color_scheme&quot;: &quot;Packages/Material Theme/schemes/Material-Theme.tmTheme&quot;,</div><div class="line">    &quot;theme&quot;: &quot;Material-Theme.sublime-theme&quot;,</div><div class="line"></div><div class="line">    &quot;line_padding_top&quot;: 4,</div><div class="line">    &quot;line_padding_bottom&quot;: 4,</div><div class="line"></div><div class="line">    &quot;tab_size&quot;: 4,</div><div class="line">    &quot;translate_tabs_to_spaces&quot;: true,</div><div class="line"></div><div class="line">    &quot;draw_centered&quot;: false,</div><div class="line">    &quot;word_wrap&quot;: true,</div><div class="line">    &quot;wrap_width&quot;: 120,</div><div class="line">    // &quot;rulers&quot;: [80, 120]</div><div class="line">&#125;</div></pre></td></tr></table></figure>
</p>
<h2 id="sublime-keymap"><a href="#sublime-keymap" class="headerlink" title="sublime.keymap"></a>sublime.keymap</h2><p><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">[</div><div class="line">    &#123; &quot;keys&quot;: [&quot;alt+m&quot;], &quot;command&quot;: &quot;markdown_preview&quot;, &quot;args&quot;: &#123;&quot;target&quot;: &quot;browser&quot;, &quot;parser&quot;:&quot;markdown&quot;&#125; &#125;</div><div class="line">]</div></pre></td></tr></table></figure>
</p>
<h2 id="总是以新窗口打开文件的解决办法"><a href="#总是以新窗口打开文件的解决办法" class="headerlink" title="总是以新窗口打开文件的解决办法"></a>总是以新窗口打开文件的解决办法</h2><p>具体设置：Preferences -&gt; Settings – User -&gt; 添加<br><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">&quot;open_files_in_new_window&quot;: false,</div></pre></td></tr></table></figure></p>
<p>重启一下sublime text 3</p>
<h2 id="Package-Control"><a href="#Package-Control" class="headerlink" title="Package Control"></a>Package Control</h2><p>import urllib.request,os,hashlib; h = ‘2915d1851351e5ee549c20394736b442’ + ‘8bc59f460fa1548d1514676163dafc88’; pf = ‘Package Control.sublime-package’; ipp = sublime.installed_packages_path(); urllib.request.install_opener( urllib.request.build_opener( urllib.request.ProxyHandler()) ); by = urllib.request.urlopen( ‘<a href="http://packagecontrol.io/" target="_blank" rel="external">http://packagecontrol.io/</a>‘ + pf.replace(‘ ‘, ‘%20’)).read(); dh = hashlib.sha256(by).hexdigest(); print(‘Error validating download (got %s instead of %s), please try manual install’ % (dh, h)) if dh != h else open(os.path.join( ipp, pf), ‘wb’ ).write(by)</p>
<hr>
<h3 id="MarkDown"><a href="#MarkDown" class="headerlink" title="MarkDown"></a>MarkDown</h3><h4 id="MarkDown-Editing："><a href="#MarkDown-Editing：" class="headerlink" title="MarkDown Editing："></a>MarkDown Editing：</h4><p>支持Markdown语法高亮；支持Github Favored Markdown语法；自带3个主题。<br><code>GFW setting:</code><br>{<br>    “extensions”:<br>    [<br>        “md”<br>    ],</p>
<pre><code>&quot;color_scheme&quot;: &quot;Packages/MarkdownEditing/MarkdownEditor.tmTheme&quot;,

&quot;line_padding_top&quot;: 4,
&quot;line_padding_bottom&quot;: 4,

&quot;tab_size&quot;: 4,
&quot;translate_tabs_to_spaces&quot;: true,

&quot;draw_centered&quot;: false,
&quot;word_wrap&quot;: true,
&quot;wrap_width&quot;: 80,
// &quot;rulers&quot;: [80, 120]

</code></pre><p>}</p>
<hr>
<h3 id="OmniMarkupPreviwer："><a href="#OmniMarkupPreviwer：" class="headerlink" title="OmniMarkupPreviwer："></a>OmniMarkupPreviwer：</h3><p>实时在浏览器中预，而MarkdownPreview是需要手动生成的和F5的。览如果双屏的话，应该具有不错的体验。快捷键如下：<br>Ctrl+Alt+O: Preview Markup in Browser.<br>Ctrl+Alt+X: Export Markup as HTML.<br>Ctrl+Alt+C: Copy Markup as HTML.</p>
<hr>
<h2 id="解决在Ubuntu14-04下Sublime-Text-3无法输入中文的问题"><a href="#解决在Ubuntu14-04下Sublime-Text-3无法输入中文的问题" class="headerlink" title="解决在Ubuntu14.04下Sublime Text 3无法输入中文的问题"></a>解决在Ubuntu14.04下Sublime Text 3无法输入中文的问题</h2><p><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">cd </div><div class="line">vim sublime_imfix.c</div><div class="line">sudo apt-get install gtk+-2.0</div><div class="line">gcc -shared -o libsublime-imfix.so sublime_imfix.c  `pkg-config --libs --cflags gtk+-2.0` -fPIC</div><div class="line">sudo mv libsublime-imfix.so /opt/sublime_text/</div><div class="line"></div><div class="line">修改文件/usr/bin/subl的内容</div><div class="line">sudo gedit /usr/bin/subl</div><div class="line">将</div><div class="line">#!/bin/sh</div><div class="line">exec /opt/sublime_text/sublime_text &quot;$@&quot;</div><div class="line">修改为</div><div class="line">#!/bin/sh</div><div class="line">LD_PRELOAD=/opt/sublime_text/libsublime-imfix.so exec /opt/sublime_text/sublime_text &quot;$@&quot;</div><div class="line">此时，在命令中执行 subl 将可以使用搜狗for linux的中文输入</div></pre></td></tr></table></figure>
</p>
]]></content>
      
        <categories>
            
            <category> Document </category>
            
        </categories>
        
        
    </entry>
    
    <entry>
      <title><![CDATA[Django]]></title>
      <url>/python/Django.html</url>
      <content type="html"><![CDATA[<h2 id="setting-py"><a href="#setting-py" class="headerlink" title="setting.py"></a>setting.py</h2><h3 id="init-py"><a href="#init-py" class="headerlink" title="__init__.py"></a><code>__init__.py</code></h3><p>Python中声明模块的文件<br>内容默认为空</p>
<h3 id="admin-py"><a href="#admin-py" class="headerlink" title="admin.py"></a>admin.py</h3><p>该应用的后台管理系统配置</p>
<h3 id="apps-py"><a href="#apps-py" class="headerlink" title="apps.py"></a>apps.py</h3><p>该应用的一些配置<br>Django-1.9 以后自动生成</p>
<h3 id="models-py"><a href="#models-py" class="headerlink" title="models.py"></a>models.py</h3><p>数据模块<br>使用ORM框架<br>类似MVC结构中的Models</p>
<h3 id="test-py"><a href="#test-py" class="headerlink" title="test.py"></a>test.py</h3><p>自动化测试模块</p>
<h3 id="views-py"><a href="#views-py" class="headerlink" title="views.py"></a>views.py</h3><p>执行响应的代码所在模块<br>代码逻辑处理的主要地点<br>项目中大部分代码均在这里编写</p>
<h2 id="创建一个页面的顺序"><a href="#创建一个页面的顺序" class="headerlink" title="创建一个页面的顺序"></a>创建一个页面的顺序</h2><h3 id="编辑blog-views"><a href="#编辑blog-views" class="headerlink" title="编辑blog.views"></a>编辑blog.views</h3><ul>
<li>每个响应对应一个函数,函数必须返回一个响应</li>
<li>函数必须存在一个参数,一般约定为request</li>
<li>每一个响应(函数)对应一个URL</li>
</ul>
<h3 id="配置URL"><a href="#配置URL" class="headerlink" title="配置URL"></a>配置URL</h3><h3 id="编辑urls-py"><a href="#编辑urls-py" class="headerlink" title="编辑urls.py"></a>编辑urls.py</h3><ul>
<li>每个URL都以URL的形式写出来</li>
<li>URL函数放在</li>
<li>urlpatterns列表中</li>
<li>URL函数三个参数:URL(正则),对应方法,名称</li>
</ul>
<hr>
<h2 id="第二种URL配置"><a href="#第二种URL配置" class="headerlink" title="第二种URL配置"></a>第二种URL配置</h2><h3 id="包含其他URL"><a href="#包含其他URL" class="headerlink" title="包含其他URL"></a>包含其他URL</h3><ul>
<li>在根urls.py中引入include</li>
<li>在APP目录下创建urls.py文件,格式与根urls.py相同</li>
<li>根urls.py中url函数第二个参数改为<code>include(&#39;blog.urls&#39;)</code></li>
</ul>
<h3 id="注意事项"><a href="#注意事项" class="headerlink" title="注意事项"></a>注意事项</h3><ul>
<li>根urls.py针对APP配置的URL名称,是该APP所有URL的总路径</li>
<li>配置URL时注意正则表达式结尾符号$和/</li>
</ul>
<hr>
<h1 id="Templates"><a href="#Templates" class="headerlink" title="Templates"></a>Templates</h1><ul>
<li>HTML文件</li>
<li>使用Django模板语言Django Template Language,DTL</li>
<li>也可以使用第三方模板</li>
</ul>
<h2 id="步骤"><a href="#步骤" class="headerlink" title="步骤"></a>步骤</h2><ul>
<li>在APP的根目录下创建名叫Templates的目录</li>
<li>在该目录下创建HTML文件</li>
<li>在views.py中返回<code>render()</code></li>
</ul>
<h2 id="DTL初步使用"><a href="#DTL初步使用" class="headerlink" title="DTL初步使用"></a>DTL初步使用</h2><ul>
<li><code>render()</code>函数中支持一个dict类型参数</li>
<li>该字典是后台传递到模板的参数,键为参数名</li>
<li>在模板中使用<code></code>来直接使用</li>
</ul>
<ul>
<li>Django查找Template 是按照INSTALED_APPS中的添加顺序查找Templates</li>
<li>不同APP下Template目录中的同名.html文件会造成冲突</li>
<li>解决: 在APP的templates目录下创建以APP名为名称的目录</li>
<li>将html文件放入新创建的目录下</li>
</ul>
<hr>
<h2 id="models"><a href="#models" class="headerlink" title="models"></a>models</h2><ul>
<li>一个Model对应数据库的一张数据表</li>
<li>Django中的Models以类的形式表现</li>
<li>它包含了一些基本字段以及数据的一些行为</li>
</ul>
<h3 id="ORM"><a href="#ORM" class="headerlink" title="ORM"></a>ORM</h3><ul>
<li>对象关系映射(Object Relation Mapping)</li>
<li>实现了对象和数据库之间的映射</li>
<li>隐藏了数据访问的细节,不需要编写SQL语句</li>
</ul>
<h3 id="编写Models"><a href="#编写Models" class="headerlink" title="编写Models"></a>编写Models</h3><p>步骤:</p>
<ul>
<li>在应用根目录下创建models.py,并引入models模块</li>
<li>创建类,继承models.Model,该类即是一张数据表</li>
<li>在类中创建字段<ul>
<li>字段即类里面的属性(变量)</li>
<li><code>attr = models.CharField(max_length=64)</code></li>
</ul>
</li>
<li>生成数据表<ul>
<li>命令中进入manage.py同级目录</li>
<li>执行python manage.py makemigrations app名(可选) #默认给该项目下所有的应用都生成</li>
<li>在执行 python manage.py migrate</li>
<li>查看: <ul>
<li>django会自动在 app/migrations/目录下生成移植文件</li>
<li>执行<code>python manage.py sqlmigrate 应用名 文件id 查询SQL语句</code></li>
<li>默认sqlite3的数据库在项目根目录下db.sqlite3</li>
</ul>
</li>
</ul>
</li>
<li>页面呈现数据<ul>
<li>后台步骤<ul>
<li>views.py中import models</li>
<li><code>article = models.Article.objects.get(pk=1)</code></li>
<li><code>render(request,page,{&#39;article&#39; :article})</code></li>
</ul>
</li>
<li>前端步骤<p><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">* 模块可直接使用对象以及对象的&quot;.&quot;操作</div><div class="line">* `&#123;&#123; article.title &#125;&#125;`</div></pre></td></tr></table></figure>
</p>
</li>
</ul>
</li>
</ul>
<hr>
<h2 id="Admin"><a href="#Admin" class="headerlink" title="Admin"></a>Admin</h2><ul>
<li>Admin是Django自带的一个功能强大的自动化数据管理界面</li>
<li>被授权的用户可直接在Admin中管理数据库</li>
<li>Django提供了许多针对Admin的定制功能</li>
<li>创建用户<ul>
<li><code>python manage.py createsuperuser</code> 创建超级用户</li>
<li><code>localhost:8000/admin/</code> Admin入口</li>
<li>修改settings.py中 <code>LANGUAGE_CODE = &#39;zh-hans&#39;</code></li>
</ul>
</li>
<li>配置应用<ul>
<li>在应用下admin.py中引入自身的models模块(或里面的模型类)</li>
<li>编辑admin.py: <code>admin.site.register(models.Article)</code></li>
</ul>
</li>
<li>修改数据<ul>
<li>点击Article超链接进入Article列表页面</li>
<li>点击任意一天数据,进入编辑页面修改</li>
<li>编辑页面下方一排按钮可执行相应操作</li>
<li>修改数据默认显示名称<p><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">* 在Article类下添加一个方法</div><div class="line">* 根据Python版本选择`__str__(self)`或`__unicode__(self)`</div><div class="line">* `return self.title`</div></pre></td></tr></table></figure>
</p>
</li>
</ul>
</li>
</ul>
<hr>
<h2 id="博客页面"><a href="#博客页面" class="headerlink" title="博客页面"></a>博客页面</h2><ul>
<li>页面概要<ul>
<li>博客主页面<ul>
<li>文章标题列表,超链接<ul>
<li>取出数据库中所有文章对象</li>
<li>将文章对象们打包成列表,传递到前端</li>
<li>前端页面吧文章以标题超链接的形式逐个列出</li>
</ul>
</li>
<li>发表博客按钮(超链接)</li>
</ul>
</li>
<li>博客页面内容页面<ul>
<li>标题</li>
<li>文章内容</li>
<li>修改文章按钮(超链接)</li>
<li>URL传递参数<ul>
<li>参数写在响应函数中request后,可以有默认值</li>
<li>URL正则表达式:<code>r&#39; ^/article/(?&lt;articel_id&gt;[0-9]+)/$&#39;</code></li>
<li>URL正则中的组名必须喝参数名一致</li>
</ul>
</li>
<li>超链接目标地址<p><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">- href后面是目标地址</div><div class="line">- template中可以用 `&#123;% url &apos;app_name:url_name&apos; param %&#125;`</div><div class="line">- 其中app_name和url_name都在url中配置</div><div class="line">- url函数的名称参数</div><div class="line">    + 根urls,写在include()的第二个参数位置,`namespace= &apos;blog&apos;`</div><div class="line">    + 应用下则写在url()的第三个参数位置,`name= &apos;article&apos;`</div><div class="line">    + 主要取决于是否使用include引用了另一个url配置文件</div></pre></td></tr></table></figure>
</p>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<pre><code>+ 博客撰写页面
    + 页面内容
        * 标题编辑栏
        * 文章内容编辑区域
        * 提交按钮
    * 编辑响应函数
        - 使用`request.POST[ &apos;参数名&apos; ]`获取表单数据
        - `models.Article.objects.create(title,content)`创建对象


</code></pre><hr>
<ul>
<li>模板for循环</li>
<li><code></code></li>
</ul>
<hr>
<h2 id="两个编辑页面"><a href="#两个编辑页面" class="headerlink" title="两个编辑页面"></a>两个编辑页面</h2><p>思路<br>    新文章为空,修改文章有内容<br>    修改文章页面有文章对象<br>    文章的ID 文章对象的ID </p>
<h2 id="博客撰写页面"><a href="#博客撰写页面" class="headerlink" title="博客撰写页面"></a>博客撰写页面</h2><p>修改数据</p>
<ul>
<li><code>article.title = title</code></li>
<li><code>article.save()</code></li>
</ul>
<hr>
<h3 id="Templates过滤器"><a href="#Templates过滤器" class="headerlink" title="Templates过滤器"></a>Templates过滤器</h3><ul>
<li>什么是过滤器?<ul>
<li>写在模板中,属于Django模板语言</li>
<li>可以修改模板中的变量,从而显示不同内容</li>
</ul>
</li>
<li>怎么使用过滤器<p><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">+ &#123;&#123;value | filter &#125;&#125;</div><div class="line">+ 例子: &#123;&#123; list_nums | length &#125;&#125;</div><div class="line">+ 过滤器可叠加: &#123;&#123;value|filter1|filter2|...&#125;&#125;</div><div class="line">+ 如果出现不存在的变量是不会报错的,只会给出一个空值</div></pre></td></tr></table></figure>
</p>
</li>
</ul>
<h2 id="Django-shell"><a href="#Django-shell" class="headerlink" title="Django shell"></a>Django shell</h2><ul>
<li>什么是Django Shell?<ul>
<li>是一个Python的交互式命令行程序</li>
<li>它自动引入了我们的项目环境</li>
<li>我们可以使用它与我们的项目进行交互</li>
</ul>
</li>
<li>如何使用Django Shell?<ul>
<li><code>python manage.py shell</code></li>
<li><code>from blog.models import Article</code></li>
<li><code>Article.objects.all()</code></li>
</ul>
</li>
<li>有什么用<ul>
<li>可以使用Django shell来进行一些调试工作</li>
<li>测试未知的方法</li>
</ul>
</li>
</ul>
<h2 id="admin"><a href="#admin" class="headerlink" title="admin"></a>admin</h2><ul>
<li>创建admin配置类<ul>
<li><code>class ArticleAdmin(admin.ModelAdmin)</code></li>
<li>注册:<code>admin.site.register(Article,ArticleAdmin)</code></li>
<li>显示其他字段<ul>
<li><code>list_display = (&#39;title&#39;,&#39;content&#39;)</code></li>
<li>list_display同时支持tuple和list</li>
</ul>
</li>
<li>过滤器<ul>
<li><code>list_filter = (&#39;pub_time&#39;, )</code></li>
</ul>
</li>
</ul>
</li>
</ul>
]]></content>
      
        <categories>
            
            <category> python </category>
            
        </categories>
        
        
    </entry>
    
    <entry>
      <title><![CDATA[hbase Region调优]]></title>
      <url>/CDH/HBase/hbase%20Region%E8%B0%83%E4%BC%98.html</url>
      <content type="html"><![CDATA[<h1 id="hbase-Region调优"><a href="#hbase-Region调优" class="headerlink" title="hbase Region调优"></a>hbase Region调优</h1><p>在hbase shell 中输入:<br><code>balance_switch true</code></p>
<p>在滚动重启的时候 会有如下警告:<br>    The Load Balancer is not enabled which will eventually cause performance degradation in HBase as Regions will not be distributed across all RegionServers. The balancer is only expected to be disabled during rolling upgrade scenarios.<br>说明重启过程中改变这个参数 之前由于重启失败导致 region balance这个默认参数被改为false后,没能自动恢复</p>
<p>但是设置为true后依然没有平衡,最终重启master(leader)后,新的master开始触发平衡,说明之前的master不正常工作了</p>
]]></content>
      
        <categories>
            
            <category> CDH </category>
            
            <category> HBase </category>
            
        </categories>
        
        
    </entry>
    
    <entry>
      <title><![CDATA[Yarn磁盘检查]]></title>
      <url>/CDH/Error/Yarn%E7%A3%81%E7%9B%98%E6%A3%80%E6%9F%A5.html</url>
      <content type="html"><![CDATA[<h2 id="yarn-nodemanage-参数临时修改到100"><a href="#yarn-nodemanage-参数临时修改到100" class="headerlink" title="yarn nodemanage 参数临时修改到100"></a>yarn nodemanage 参数临时修改到100</h2><p>保证mapreduce shuffle阶段不会因为磁盘检测问题导致写入失败影响作业进度,cm默认配置为95%</p>
<p>报错如下:<br>    Directory /mnt/sdd1/yarn/nm error, used space above threshold of 95.0%, removing from list of valid directories</p>
<p><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">Failed to setup application log directory for application_1494574323287_3723</div><div class="line">org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.security.token.SecretManager$InvalidToken): token (token for optaim: HDFS_DELEGATION_TOKEN owner=optaim@CMDMP.COM, renewer=yarn, realUser=, issueDate=1494607575657, maxDate=1495212375657, sequenceNumber=2291196, masterKeyId=599) can&apos;t be found in cache</div><div class="line"></div><div class="line"></div><div class="line">Error: org.apache.hadoop.mapreduce.task.reduce.Shuffle$ShuffleError: error in shuffle in fetcher#8 at org.apache.hadoop.mapreduce.task.reduce.Shuffle.run(Shuffle.java:134) at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:376) at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:164) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:415) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1796) at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:158) Caused by: java.lang.OutOfMemoryError: Java heap space at org.apache.hadoop.io.BoundedByteArrayOutputStream.&lt;init&gt;(BoundedByteArrayOutputStream.java:56) at org.apache.hadoop.io.BoundedByteArrayOutputStream.&lt;init&gt;(BoundedByteArrayOutputStream.java:46) at org.apache.hadoop.mapreduce.task.reduce.InMemoryMapOutput.&lt;init&gt;(InMemoryMapOutput.java:63) at org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl.unconditionalReserve(MergeManagerImpl.java:309) at org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl.reserve(MergeManagerImpl.java:299) at org.apache.hadoop.mapreduce.task.reduce.Fetcher.copyMapOutput(Fetcher.java:539) at org.apache.hadoop.mapreduce.task.reduce.Fetcher.copyFromHost(Fetcher.java:348) at org.apache.hadoop.mapreduce.task.reduce.Fetcher.run(Fetcher.java:198)</div></pre></td></tr></table></figure></p>
]]></content>
      
        <categories>
            
            <category> CDH </category>
            
            <category> Error </category>
            
        </categories>
        
        
    </entry>
    
    <entry>
      <title><![CDATA[hiveserver2 haproxy]]></title>
      <url>/CDH/Hive/hiveserver2%20haproxy.html</url>
      <content type="html"><![CDATA[<h1 id="配置HiveServer2负载平衡代理"><a href="#配置HiveServer2负载平衡代理" class="headerlink" title="配置HiveServer2负载平衡代理"></a>配置HiveServer2负载平衡代理</h1><blockquote>
<p>操作系统:SUSE11 SP3<br>CM版本:CM5.10.041<br>CDH版本:CDH5.10.041<br>使用root用户对集群进行部署<br><a href="https://www.cloudera.com/documentation/enterprise/5-10-x/topics/admin_ha_hiveserver2.html" target="_blank" rel="external">cloudera对应文档</a></p>
</blockquote>
<ol>
<li>下载负载平衡代理软件到需要的主机上。<a href="http://rpm.pbone.net/index.php3/stat/4/idpl/33562430/dir/opensuse/com/haproxy-1.5.14-91.1.x86_64.rpm.html" target="_blank" rel="external">haproxy-1.5.14-91.1.x86_64.rpm</a></li>
<li><p>安装,配置开机启动,修改配置文件后启动haproxy,并检查对应端口是否开启  </p>
<ol>
<li>Set the port for the load balancer to listen on and relay HiveServer2 requests back and forth.</li>
<li>Set the port and hostname for each HiveServer2 host—that is, the hosts from which the load balancer chooses when relaying each query. </li>
</ol>
<p><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">rpm -ihv haproxy-1.5.14-91.1.x86_64.rpm </div><div class="line">chkconfig haproxy on</div><div class="line">chkconfig --list haproxy </div><div class="line">cp /etc/haproxy/haproxy.cfg /etc/haproxy/bak.cfg</div><div class="line">vi /etc/haproxy/haproxy.cfg </div><div class="line">/etc/init.d/haproxy start</div><div class="line"></div><div class="line">ss -an |grep 10003</div></pre></td></tr></table></figure>
</p>
<p>配置文件:<a href="https://issues.cloudera.org/browse/HUE-4990" target="_blank" rel="external">因为balance模式选择导致HUE请求超时</a>  </p>
<p><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">global</div><div class="line">    # To have these messages end up in /var/log/haproxy.log you will</div><div class="line">    # need to:</div><div class="line">    #</div><div class="line">    # 1) configure syslog to accept network log events.  This is done</div><div class="line">    #    by adding the &apos;-r&apos; option to the SYSLOGD_OPTIONS in</div><div class="line">    #    /etc/sysconfig/syslog</div><div class="line">    #</div><div class="line">    # 2) configure local2 events to go to the /var/log/haproxy.log</div><div class="line">    #   file. A line like the following can be added to</div><div class="line">    #   /etc/sysconfig/syslog</div><div class="line">    #</div><div class="line">    #    local2.*                       /var/log/haproxy.log</div><div class="line">    #</div><div class="line">    pidfile     /var/run/haproxy.pid</div><div class="line">    nbproc 1</div><div class="line">    maxconn     4000</div><div class="line">    daemon</div><div class="line"></div><div class="line">    # turn on stats unix socket</div><div class="line">    #stats socket /var/lib/haproxy/stats</div><div class="line"></div><div class="line">#---------------------------------------------------------------------</div><div class="line"># common defaults that all the &apos;listen&apos; and &apos;backend&apos; sections will</div><div class="line"># use if not designated in their block</div><div class="line">#</div><div class="line"># You might need to adjust timing values to prevent timeouts.</div><div class="line">#---------------------------------------------------------------------</div><div class="line">defaults</div><div class="line">    mode                    tcp</div><div class="line">    log                     127.0.0.1 local0 err</div><div class="line">    option                  redispatch</div><div class="line">    retries                 3</div><div class="line">    maxconn                 3000</div><div class="line">    contimeout 1800000</div><div class="line">    timeout client 1800000</div><div class="line">    timeout server 1800000</div><div class="line"></div><div class="line">#</div><div class="line"># This sets up the admin page for HA Proxy at port 25002.</div><div class="line">#</div><div class="line">listen stats</div><div class="line">    bind 10.200.65.71:25002</div><div class="line">    balance</div><div class="line">    mode http</div><div class="line">    stats enable</div><div class="line">    stats auth username:password</div><div class="line"></div><div class="line"># This is the setup for Impala. Impala client connect to load_balancer_host:25003.</div><div class="line"># HAProxy will balance connections among the list of servers listed below.</div><div class="line"># The list of Impalad is listening at port 21000 for beeswax (impala-shell) or original ODBC driver.</div><div class="line"># For JDBC or ODBC version 2.x driver, use port 21050 instead of 21000.</div><div class="line">listen hive</div><div class="line">    bind 10.200.65.71:10003</div><div class="line">    mode tcp</div><div class="line">    balance source</div><div class="line">    maxconn 1024</div><div class="line">    server ddp-cm ddp-cm.cmdmp.com:10000 check inter 1800000 rise 1 fall 2</div><div class="line">    server ddp-dn-12 ddp-dn-12.cmdmp.com:10000 check inter 1800000 rise 1 fall 2</div></pre></td></tr></table></figure>
</p>
</li>
<li><p>如果开启hiveserver UDF 配置,需要将之前hiveserver2对应路径下的jar包,放在与之一致的路径里,之后如果更新jar包需要同时更新所有的hiveserver2节点下对应的目录以及haproxy节点下对应的目录</p>
<p><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">mkdir -p /opt/local/hive/lib/</div><div class="line">mkdir -p /opt/local/hive/reloadable/</div><div class="line">scp root@10.200.65.50:/opt/local/hive/lib/* /opt/local/hive/lib/</div><div class="line">scp root@10.200.65.50:/opt/local/hive/reloadable/* /opt/local/hive/reloadable/</div><div class="line"></div><div class="line">chown hadoop:hadoop -R /opt/local/hive/lib/</div><div class="line">chown hadoop:hadoop -R /opt/local/hive/reloadable/</div></pre></td></tr></table></figure>
</p>
</li>
<li><p>Run the load-balancing proxy server and point it at the configuration file.</p>
</li>
<li><p>In Cloudera Manager, configure HiveServer2 Load Balancer for the proxy server.</p>
<ol>
<li>Go to the Hive service.</li>
<li>Click the Configuration tab.</li>
<li>Select Scope &gt; HiveServer2.</li>
<li>Select Category &gt; Main. </li>
<li>Locate the HiveServer2 Load Balancer property or search for it by typing its name in the Search box.</li>
<li>Enter values for <code>hostname:port number</code>.</li>
<li>Click Save Changes to commit the changes.</li>
<li>Restart the Hive service.<p><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">Note:When you set the HiveServer2 Load Balancer property, Cloudera Manager regenerates the keytabs for HiveServer2 roles. The principal in these keytabs contains the load balancer hostname. If there is a Hue service that depends on this Hive service, it also uses the load balancer to communicate with Hive.</div></pre></td></tr></table></figure>
</p>
</li>
</ol>
</li>
<li><p>Point all scripts, jobs, or application configurations to the new proxy server instead of any specific HiveServer2 instance.一旦CM中hive配置配上balance后,原先所有的hiveserver2都将无法继续使用</p>
</li>
</ol>
<p>附录:<a href="http://blog.cloudera.com/blog/2013/08/how-to-achieve-higher-availability-for-hue/" target="_blank" rel="external">Higher Availability</a>    </p>
]]></content>
      
        <categories>
            
            <category> CDH </category>
            
            <category> Hive </category>
            
        </categories>
        
        
    </entry>
    
    <entry>
      <title><![CDATA[mysql命令]]></title>
      <url>/MySQL/mysql%E5%91%BD%E4%BB%A4.html</url>
      <content type="html"><![CDATA[<p>在mysql的命令提示符下，执行下面一句话,查看mysql服务器的所有全局配置信息：<br><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">show global variables;</div><div class="line">show slave status\G</div><div class="line">show global variables like &quot;%datadir%&quot;;</div></pre></td></tr></table></figure></p>
<p>前言：之前因为了修改服务器hostname，导致从库同步失败，一直解决不掉，没办法，只能重新设置同步。<br>先停止从库同步，并清空同步信息<br><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">stop slave；</div><div class="line">reset slave all;</div></pre></td></tr></table></figure></p>
<p>全量备份主库数据</p>
<p><code>mysqldump -u root -p --all-databases --routines &gt; senqiang.sql</code><br>全量还原数据库到从库</p>
<p><code>mysql -uroot -p &lt; senqiang.sql</code><br>查看主库最新的binlog日志偏移量，从库重新授权同步</p>
<p><code>change master to master_host=&#39;192.168.0.2&#39;,master_user=&#39;syncdb&#39;,master_password=&#39;passwd&#39;,master_log_file=&#39;mysql-bin.000005&#39;,master_log_pos=8188;</code><br>开启从库同步<br><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">start slave;</div><div class="line">show slave status\G;</div></pre></td></tr></table></figure></p>
<p>注：备份主库前最好停止主库的写入，记录最新日志偏移量，否则在你还原数据，重新授权同步时会有数据偏差</p>
<p>Error: Error while compiling statement: FAILED: InvalidConfigurationException hive.server2.authentication can’t be none in non-testing mode (state=42000,code=40000</p>
]]></content>
      
        <categories>
            
            <category> MySQL </category>
            
        </categories>
        
        
    </entry>
    
    <entry>
      <title><![CDATA[Cloudera部署CentOS]]></title>
      <url>/CDH/Cloudera%E9%83%A8%E7%BD%B2CentOS.html</url>
      <content type="html"><![CDATA[<blockquote>
<p>操作系统:CentOS 6.5<br>CM版本:CM5.10.041<br>CDH版本:CDH5.10.041<br>使用version用户对集群进行部署</p>
</blockquote>
<h2 id="服务器配置"><a href="#服务器配置" class="headerlink" title="服务器配置"></a>服务器配置</h2><ul>
<li>关闭防火墙</li>
<li>配置时钟同步服务</li>
<li>检查数据盘是否挂载</li>
</ul>
<p>安装的机器ip:<br>192.168.86.119,192.168.86.120,</p>
<ol>
<li>安装操作系统，建议对操作系统盘做RAID1</li>
<li>如果不能连接互联网，先创建OS的repository，以便yum或zypper可以访问OS镜像</li>
<li>为了使集群中各个节点之间能互相通信，需要静态或动态配置节点的IP地址。如果使用动态配置，请安装DHCP和DNS服务器，具体请参见对应软件的安装文档，此不赘述；如果使用静态IP地址，请正确配置各节点的IP，并在/etc/hosts配置所有节点的静态DNS解析。<br>以cm节点为例：<code>/etc/sysconfig/network-scripts/ifcfg-eth0</code></li>
</ol>
<p><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">DEVICE=&quot;bond0&quot;</div><div class="line">BOOTPROTO=&quot;static&quot;</div><div class="line">IPV6INIT=&quot;no&quot;</div><div class="line">MTU=&quot;1500&quot;</div><div class="line">NM_CONTROLLED=&quot;no&quot;</div><div class="line">ONBOOT=&quot;yes&quot;</div><div class="line">TYPE=&quot;Ethernet&quot;</div><div class="line">IPADDR=192.168.60.25</div><div class="line">NETMASK=255.255.255.0</div><div class="line">GATEWAY=192.168.60.1</div></pre></td></tr></table></figure>
</p>
<p>/etc/hosts样例<br><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">10.200.65.50    ddp-cm.cmdmp.com    ddp-cm</div><div class="line">10.200.65.51    ddp-nn-01.cmdmp.com ddp-nn-01</div><div class="line">10.200.65.52    ddp-nn-02.cmdmp.com ddp-nn-02</div><div class="line">10.200.65.53    ddp-dn-01.cmdmp.com ddp-dn-01</div><div class="line">10.200.65.54    ddp-dn-02.cmdmp.com ddp-dn-02</div><div class="line">10.200.65.55    ddp-dn-03.cmdmp.com ddp-dn-03</div><div class="line">10.200.65.56    ddp-dn-04.cmdmp.com ddp-dn-04</div><div class="line">10.200.65.57    ddp-dn-05.cmdmp.com ddp-dn-05</div><div class="line">10.200.65.58    ddp-dn-06.cmdmp.com ddp-dn-06</div><div class="line">10.200.65.59    ddp-dn-07.cmdmp.com ddp-dn-07</div><div class="line">10.200.65.60    ddp-dn-08.cmdmp.com ddp-dn-08</div><div class="line">10.200.65.61    ddp-dn-09.cmdmp.com ddp-dn-09</div><div class="line">10.200.65.62    ddp-dn-10.cmdmp.com ddp-dn-10</div><div class="line">10.200.65.63    ddp-dn-11.cmdmp.com ddp-dn-11</div><div class="line">10.200.65.64    ddp-dn-12.cmdmp.com ddp-dn-12</div><div class="line">10.200.65.71    ddp-kettle-02.cmdmp.com ddp-kettle-02</div><div class="line">10.200.63.132   ddp-yonghong-test1.cmdmp.com    ddp-yonghong-test1.cmdmp.com</div><div class="line">10.200.86.119   ddp-sunflower-test1.cmdmp.com   ddp-sunflower-test1.cmdmp.com</div><div class="line">10.200.86.120   ddp-sunflower-test2.cmdmp.com   ddp-sunflower-test2.cmdmp.com</div></pre></td></tr></table></figure></p>
<p><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">service network restart</div><div class="line">ping 10.200.86.1</div><div class="line">cd vmware-tools-distrib/</div><div class="line">./vmware-install.pl </div><div class="line">reboot</div><div class="line">groupadd version</div><div class="line">useradd -m -g version version</div><div class="line">echo 2wsx@WSX|passwd --stdin version</div><div class="line">yes|cp -p /etc/sudoers /etc/sudoers_bak</div><div class="line">chmod +x /etc/sudoers</div><div class="line">sed -i &quot;/NOPASSWD: ALL/c %version ALL=(ALL) NOPASSWD: ALL&quot; /etc/sudoers</div><div class="line">chmod -x /etc/sudoers</div><div class="line">yum install daemontools</div><div class="line">route</div><div class="line">vim /etc/sysconfig/network-scripts/ifcfg-eth0</div><div class="line">service network restart</div><div class="line">vim /etc/ntp.conf </div><div class="line">/etc/init.d/ntpd start</div></pre></td></tr></table></figure>
</p>
<ol>
<li>关闭并禁用防火墙<br><code>/etc/init.d/iptables stop</code><br><code>/etc/init.d/iptables status</code><br><code>/sbin/chkconfig --level 2345 iptables off</code></li>
<li>关闭SELinux<br><code>echo &quot;SELINUX=disabled&quot; &gt; /etc/sysconfig/selinux ;</code></li>
<li>重启网络服务，并初始化网络<br><code>/etc/init.d/network restart</code></li>
<li>修改transparent_hugepage参数，这一参数默认值可能会导致CDH性能下降<br>在/etc/init.d/after.local中增加一行：<br><code>if test -f /sys/kernel/mm/transparent_hugepage/enabled; then echo never &gt; /sys/kernel/mm/transparent_hugepage/enabled fi</code></li>
<li>禁用交换内存<br>vi /etc/sysconfig/selinux<br>vi /sys/kernel/mm/redhat_transparent_hugepage/defrag<br>echo never &gt; /sys/kernel/mm/redhat_transparent_hugepage/defrag<br>vi /sys/kernel/mm/redhat_transparent_hugepage/defrag<br>sudo sysctl vm.swappiness=0<br>vim /etc/sysctl.conf<br>reboot </li>
<li>如果机器配置有双网卡，可以做双网卡绑定；</li>
</ol>
<hr>
<p><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">hostname ddp-sunflower-test1.cmdmp.com</div><div class="line">hostname </div><div class="line"></div><div class="line">vi /etc/hosts</div><div class="line">vi /etc/krb5.conf </div><div class="line">scp root@10.200.65.71:/home/sunflower/sunflower.keytab .</div><div class="line">ll</div><div class="line">mkdir rpm</div><div class="line">cd rpm/</div><div class="line">scp root@10.200.65.50:/srv/www/htdocs/cloudera/centos_cm/* .</div><div class="line">ll</div><div class="line">yum install ntp</div><div class="line">vi /etc/yum.repos.d/myrepo.repo</div><div class="line">yum install oracle-j2sdk1.7</div><div class="line">yum updata</div><div class="line">yum --help</div><div class="line">yum clean</div><div class="line">yum upgrade</div><div class="line">yum repolist</div><div class="line">vi /etc/yum.repos.d/CentOS-Base.repo </div><div class="line">yum upgrade</div><div class="line">yum install oracle-j2sdk1.7</div><div class="line">vi /etc/yum.repos.d/CentOS-Base.repo </div><div class="line">yum install oracle-j2sdk1.7</div><div class="line">yum clean</div><div class="line">yum clean all</div><div class="line">yum install oracle-j2sdk1.7</div><div class="line">rpm -ivh oracle-j2sdk1.7-1.7.0+update67-1.x86_64.rpm </div><div class="line">rpm -ivh cloudera-manager-daemons-5.10.0-1.cm5100.p0.85.el6.x86_64.rpm </div><div class="line">find /etc/yum.repos.d/ -name &quot;*.repo&quot; -exec mv &#123;&#125; &#123;&#125;.bak \; </div><div class="line">rpm -ivh --force --nodeps http://10.200.58.39/pub/centos6-1.0-1.el6.x86_64.rpm </div><div class="line">yum clean all </div><div class="line">yum makecache </div><div class="line">mv /etc/yum.repos.d/myrepo.repo.bak /etc/yum.repos.d/myrepo.repo</div><div class="line">yum install oracle-j2sdk1.7</div><div class="line">history</div></pre></td></tr></table></figure></p>
]]></content>
      
        <categories>
            
            <category> CDH </category>
            
        </categories>
        
        
    </entry>
    
    <entry>
      <title><![CDATA[hive zk 锁分区]]></title>
      <url>/CDH/Error/hive%20zk%20%E9%94%81%E5%88%86%E5%8C%BA.html</url>
      <content type="html"><![CDATA[<p>错误日志:<br><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">Got user-level KeeperException when processing sessionid:0x15bfba733be2c2b type:create cxid:0x8621e1c zxid:0x170a4b8e0d txntype:-1 reqpath:n/a Error Path:/hive_zookeeper_namespace_hive/rptdata/fact_kesheng_sdk_new_device_hourly/src_file_day=20170414/src_file_hour=07 Error:KeeperErrorCode = NodeExists for /hive_zookeeper_namespace_hive/rptdata/fact_kesheng_sdk_new_device_hourly/src_file_day=20170414/src_file_hour=07</div></pre></td></tr></table></figure></p>
<p><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">2013-11-04 23:52:40,485 ERROR ZooKeeperHiveLockManager (ZooKeeperHiveLockManager.java:unlockPrimitive(447)) - Failed to release ZooKeeper lock:</div><div class="line">org.apache.zookeeper.KeeperException$NoNodeException: KeeperErrorCode = NoNode for /hive_zookeeper_namespace/&lt;hiveDBName&gt;/&lt;Table&gt;/&lt;PARTITION&gt;/LOCK-SHARED-0000000000</div></pre></td></tr></table></figure>
</p>
<p>在ZK中可以看到对应的分区锁住了<br><code>/hive_zookeeper_namespace_hive/&lt;hiveDBName&gt;/&lt;Table&gt;/LOCK-SHARED-</code></p>
<p>在beeline中查看被锁的表和分区<br><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">show locks &lt;hiveDBName&gt;.&lt;Table&gt;</div><div class="line"></div><div class="line">show locks &lt;hiveDBName&gt;.&lt;Table&gt; partition (src_file_day=&apos;20170218&apos; , src_file_our=&apos;17&apos;); </div><div class="line">show locks app.mgba_client_event_detail partition(src_file_day=&apos;20171114&apos;);</div><div class="line">unlock table</div></pre></td></tr></table></figure></p>
<p>可以看到<br><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">| &lt;hiveDBName&gt;@ugc_90103_bossmonthorderlog_test@&lt;PARTITION&gt;/&lt;PARTITION&gt; | SHARED     |</div></pre></td></tr></table></figure></p>
<p>依旧在beeline中:<br><code>set hive.support.concurrency=false;</code><br>然后把分区删了,重建</p>
<p>之后又被锁了一次,详细过程如下:<br><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">show locks quancheng.ugc_secondline_order_v;</div><div class="line"></div><div class="line">+-----------------------------------+---------+--+</div><div class="line">|             tab_name              |  mode   |</div><div class="line">+-----------------------------------+---------+--+</div><div class="line">| quancheng@ugc_secondline_order_v  | SHARED  |</div><div class="line">| quancheng@ugc_secondline_order_v  | SHARED  |</div><div class="line">| quancheng@ugc_secondline_order_v  | SHARED  |</div><div class="line">+-----------------------------------+---------+--+</div><div class="line">set hive.support.concurrency=false;</div><div class="line">0: jdbc:hive2://10.200.60.106:10000/default&gt; show locks quancheng.ugc_secondline_order_v;</div><div class="line">INFO  : Compiling command(queryId=hive_20170629105454_e0e96d5e-9602-40e5-b100-250ebf6e5bf9): show locks quancheng.ugc_secondline_order_v</div><div class="line">INFO  : Semantic Analysis Completed</div><div class="line">INFO  : Returning Hive schema: Schema(fieldSchemas:[FieldSchema(name:tab_name, type:string, comment:from deserializer), FieldSchema(name:mode, type:string, comment:from deserializer)], properties:null)</div><div class="line">INFO  : Completed compiling command(queryId=hive_20170629105454_e0e96d5e-9602-40e5-b100-250ebf6e5bf9); Time taken: 0.07 seconds</div><div class="line">INFO  : Concurrency mode is disabled, not creating a lock manager</div><div class="line">INFO  : Executing command(queryId=hive_20170629105454_e0e96d5e-9602-40e5-b100-250ebf6e5bf9): show locks quancheng.ugc_secondline_order_v</div><div class="line">INFO  : Starting task [Stage-0:DDL] in serial mode</div><div class="line">INFO  : Completed executing command(queryId=hive_20170629105454_e0e96d5e-9602-40e5-b100-250ebf6e5bf9); Time taken: 0.034 seconds</div><div class="line">INFO  : OK</div><div class="line">+-----------------------------------+---------+--+</div><div class="line">|             tab_name              |  mode   |</div><div class="line">+-----------------------------------+---------+--+</div><div class="line">| quancheng@ugc_secondline_order_v  | SHARED  |</div><div class="line">| quancheng@ugc_secondline_order_v  | SHARED  |</div><div class="line">| quancheng@ugc_secondline_order_v  | SHARED  |</div><div class="line">+-----------------------------------+---------+--+</div><div class="line">3 rows selected (0.144 seconds)</div><div class="line">0: jdbc:hive2://10.200.60.106:10000/default&gt; drop quancheng.ugc_secondline_order_v;</div><div class="line">Error: Error while compiling statement: FAILED: ParseException line 1:5 cannot recognize input near &apos;drop&apos; &apos;quancheng&apos; &apos;.&apos; in ddl statement (state=42000,code=40000)</div><div class="line">0: jdbc:hive2://10.200.60.106:10000/default&gt; drop view quancheng.ugc_secondline_order_v;</div><div class="line">INFO  : Compiling command(queryId=hive_20170629105656_e957e27e-ee27-45ba-853e-ac39660ddf7e): drop view quancheng.ugc_secondline_order_v</div><div class="line">INFO  : Semantic Analysis Completed</div><div class="line">INFO  : Returning Hive schema: Schema(fieldSchemas:null, properties:null)</div><div class="line">INFO  : Completed compiling command(queryId=hive_20170629105656_e957e27e-ee27-45ba-853e-ac39660ddf7e); Time taken: 0.168 seconds</div><div class="line">INFO  : Concurrency mode is disabled, not creating a lock manager</div><div class="line">INFO  : Executing command(queryId=hive_20170629105656_e957e27e-ee27-45ba-853e-ac39660ddf7e): drop view quancheng.ugc_secondline_order_v</div><div class="line">INFO  : Starting task [Stage-0:DDL] in serial mode</div><div class="line">INFO  : Completed executing command(queryId=hive_20170629105656_e957e27e-ee27-45ba-853e-ac39660ddf7e); Time taken: 0.061 seconds</div><div class="line">INFO  : OK</div><div class="line">No rows affected (0.238 seconds)</div><div class="line">0: jdbc:hive2://10.200.60.106:10000/default&gt; show tables;</div><div class="line">INFO  : Compiling command(queryId=hive_20170629105656_63fcacb8-ffae-4e00-b810-25317cec8058): show tables</div><div class="line">INFO  : Semantic Analysis Completed</div><div class="line">INFO  : Returning Hive schema: Schema(fieldSchemas:[FieldSchema(name:tab_name, type:string, comment:from deserializer)], properties:null)</div><div class="line">INFO  : Completed compiling command(queryId=hive_20170629105656_63fcacb8-ffae-4e00-b810-25317cec8058); Time taken: 0.068 seconds</div><div class="line">INFO  : Concurrency mode is disabled, not creating a lock manager</div><div class="line">INFO  : Executing command(queryId=hive_20170629105656_63fcacb8-ffae-4e00-b810-25317cec8058): show tables</div><div class="line">INFO  : Starting task [Stage-0:DDL] in serial mode</div><div class="line">INFO  : Completed executing command(queryId=hive_20170629105656_63fcacb8-ffae-4e00-b810-25317cec8058); Time taken: 0.1 seconds</div><div class="line">INFO  : OK</div><div class="line">+-----------+--+</div><div class="line">| tab_name  |</div><div class="line">+-----------+--+</div><div class="line">+-----------+--+</div></pre></td></tr></table></figure></p>
<p><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">&gt;show locks ods.migulive_10108_func_use_log_ex;</div><div class="line"></div><div class="line">+-------------------------------------+---------+--+</div><div class="line">|              tab_name               |  mode   |</div><div class="line">+-------------------------------------+---------+--+</div><div class="line">| ods@migulive_10108_func_use_log_ex  | SHARED  |</div><div class="line">| ods@migulive_10108_func_use_log_ex  | SHARED  |</div><div class="line">| ods@migulive_10108_func_use_log_ex  | SHARED  |</div><div class="line">+-------------------------------------+---------+--+</div><div class="line">3 rows selected (0.27 seconds)</div><div class="line">&gt;show locks ods.migulive_10108_func_use_log_ex partition(src_file_day=&apos;20171114&apos;, src_file_hour=&apos;00&apos;);</div><div class="line">+----------------------------------------------------+---------+--+</div><div class="line">|                      tab_name                      |  mode   |</div><div class="line">+----------------------------------------------------+---------+--+</div><div class="line">| ods@migulive_10108_func_use_log_ex@src_file_day=20171114/src_file_hour=00 | SHARED  |</div><div class="line">| ods@migulive_10108_func_use_log_ex@src_file_day=20171114/src_file_hour=00 | SHARED  |</div><div class="line">| ods@migulive_10108_func_use_log_ex@src_file_day=20171114/src_file_hour=00 | SHARED  |</div><div class="line">+----------------------------------------------------+---------+--+</div></pre></td></tr></table></figure>
</p>
<p>解锁分区<br><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">&gt; unlock table ods.migulive_10108_func_use_log_ex partition(src_file_day=&apos;20171114&apos;, src_file_hour=&apos;00&apos;);</div><div class="line">INFO  : Compiling command(queryId=hive_20171115221717_540d811a-473a-443a-91c3-e5aa545a2bc1): unlock table ods.migulive_10108_func_use_log_ex partition(src_file_day=&apos;20171114&apos;, src_file_hour=&apos;00&apos;)</div><div class="line">INFO  : Semantic Analysis Completed</div><div class="line">INFO  : Returning Hive schema: Schema(fieldSchemas:null, properties:null)</div><div class="line">INFO  : Completed compiling command(queryId=hive_20171115221717_540d811a-473a-443a-91c3-e5aa545a2bc1); Time taken: 0.083 seconds</div><div class="line">INFO  : Executing command(queryId=hive_20171115221717_540d811a-473a-443a-91c3-e5aa545a2bc1): unlock table ods.migulive_10108_func_use_log_ex partition(src_file_day=&apos;20171114&apos;, src_file_hour=&apos;00&apos;)</div><div class="line">INFO  : Starting task [Stage-0:DDL] in serial mode</div><div class="line">INFO  : Completed executing command(queryId=hive_20171115221717_540d811a-473a-443a-91c3-e5aa545a2bc1); Time taken: 0.025 seconds</div><div class="line">INFO  : OK</div><div class="line">No rows affected (0.117 seconds)</div><div class="line">&gt; unlock table ods.migulive_10108_func_use_log_ex partition(src_file_day=&apos;20171114&apos;, src_file_hour=&apos;01&apos;);</div><div class="line">INFO  : Compiling command(queryId=hive_20171115221717_a3869646-831b-4d37-9344-3c8a598307c2): unlock table ods.migulive_10108_func_use_log_ex partition(src_file_day=&apos;20171114&apos;, src_file_hour=&apos;01&apos;)</div><div class="line">INFO  : Semantic Analysis Completed</div><div class="line">INFO  : Returning Hive schema: Schema(fieldSchemas:null, properties:null)</div><div class="line">INFO  : Completed compiling command(queryId=hive_20171115221717_a3869646-831b-4d37-9344-3c8a598307c2); Time taken: 0.084 seconds</div><div class="line">INFO  : Executing command(queryId=hive_20171115221717_a3869646-831b-4d37-9344-3c8a598307c2): unlock table ods.migulive_10108_func_use_log_ex partition(src_file_day=&apos;20171114&apos;, src_file_hour=&apos;01&apos;)</div><div class="line">INFO  : Starting task [Stage-0:DDL] in serial mode</div><div class="line">INFO  : Completed executing command(queryId=hive_20171115221717_a3869646-831b-4d37-9344-3c8a598307c2); Time taken: 0.025 seconds</div><div class="line">INFO  : OK</div><div class="line">No rows affected (0.117 seconds)</div></pre></td></tr></table></figure></p>
]]></content>
      
        <categories>
            
            <category> CDH </category>
            
            <category> Error </category>
            
        </categories>
        
        
    </entry>
    
    <entry>
      <title><![CDATA[Hive集成Sentry(无Kerboros认证)]]></title>
      <url>/CDH/Hive/Hive%E9%85%8D%E7%BD%AESentry(%E6%97%A0Kerboros%E8%AE%A4%E8%AF%81).html</url>
      <content type="html"><![CDATA[<h1 id="Hive集成Sentry-无Kerboros认证"><a href="#Hive集成Sentry-无Kerboros认证" class="headerlink" title="Hive集成Sentry(无Kerboros认证)"></a>Hive集成Sentry(无Kerboros认证)</h1><ol>
<li>在集群中添加Sentry服务<ul>
<li>sentry-site.xml 的 Sentry 客户端高级配置代码段（安全阀）<ul>
<li><code>&lt;property&gt;&lt;name&gt;sentry.hive.testing.mode&lt;/name&gt;&lt;value&gt;true&lt;/value&gt;&lt;/property&gt;</code></li>
</ul>
</li>
</ul>
</li>
<li>在hive服务中修改配置<ul>
<li>开启sentry服务</li>
<li>关闭impersonation选项</li>
<li>sentry-site.xml 的 Hive 服务高级配置代码段（安全阀）<ul>
<li><code>&lt;property&gt;&lt;name&gt;sentry.hive.testing.mode&lt;/name&gt;&lt;value&gt;true&lt;/value&gt;&lt;description&gt;强制开启sentry模式&lt;/description&gt;&lt;/property&gt;</code></li>
</ul>
</li>
</ul>
<ul>
<li>hive-site.xml 的 Hive Metastore Server 高级配置代码段（安全阀）<ul>
<li><code>&lt;property&gt;&lt;name&gt;hive.insert.into.multilevel.dirs&lt;/name&gt;&lt;value&gt;true&lt;/value&gt;&lt;description&gt;hive MoveTask报错&lt;/description&gt;&lt;/property&gt;&lt;property&gt;&lt;name&gt;hive.reloadable.aux.jars.path&lt;/name&gt;&lt;value&gt;/opt/local/hive/reloadable&lt;/value&gt;&lt;final&gt;true&lt;/final&gt;&lt;description&gt;reload jar&lt;/description&gt;&lt;/property&gt;&lt;property&gt;&lt;name&gt;hive.metastore.pre.event.listeners&lt;/name&gt;&lt;value&gt;org.apache.sentry.binding.metastore.MetastoreAuthzBinding&lt;/value&gt;&lt;final&gt;true&lt;/final&gt;&lt;description&gt;强制开启sentry&lt;/description&gt;&lt;/property&gt;&lt;property&gt;&lt;name&gt;hive.metastore.event.listeners&lt;/name&gt;&lt;value&gt;org.apache.sentry.binding.metastore.SentryMetastorePostEventListener&lt;/value&gt;&lt;final&gt;true&lt;/final&gt;&lt;description&gt;强制开启sentry&lt;/description&gt;&lt;/property&gt;</code></li>
</ul>
</li>
</ul>
<ul>
<li>hive-site.xml 的 HiveServer2 高级配置代码段（安全阀）<ul>
<li><code>&lt;property&gt;&lt;name&gt;hive.server2.enable.impersonation&lt;/name&gt;&lt;value&gt;true&lt;/value&gt;&lt;/property&gt;&lt;property&gt;&lt;name&gt;hive.security.authorization.task.factory&lt;/name&gt;&lt;value&gt;org.apache.sentry.binding.hive.SentryHiveAuthorizationTaskFactoryImpl&lt;/value&gt;&lt;/property&gt;&lt;property&gt;&lt;name&gt;hive.server2.session.hook&lt;/name&gt;&lt;value&gt;org.apache.sentry.binding.hive.HiveAuthzBindingSessionHook&lt;/value&gt;&lt;/property&gt;</code></li>
</ul>
</li>
</ul>
</li>
<li>在HDFS中可以选择将Hive的权限同步到HDFS上，但是该选项不支持两个Metastore的配置方式，另外启用HDFS ACL提供HDFS上更多的授权<ul>
<li>启用访问控制列表</li>
<li>启动Sentry同步</li>
</ul>
</li>
</ol>
<p>beeline 连接:<br><code>beeline -u &quot;jdbc:hive2://10.200.65.40:10000/&quot; -n hive -p hive -d org.apache.hive.jdbc.HiveDriver</code><br>创建admin role;<br><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">create role admin;</div><div class="line">grant all on server server1 to role admin;</div><div class="line">grant role admin to group hive;</div></pre></td></tr></table></figure></p>
]]></content>
      
        <categories>
            
            <category> CDH </category>
            
            <category> Hive </category>
            
        </categories>
        
        
    </entry>
    
    <entry>
      <title><![CDATA[reload_UDF]]></title>
      <url>/CDH/reload_UDF.html</url>
      <content type="html"><![CDATA[<p>步骤就是<br>先在beeline drop对应函数<br>然后本地(每个hiveserver 和hivemetastore机器上) 替换对应jar包<br>然后 beeline 里面 打reload;<br>最后重新create</p>
]]></content>
      
        <categories>
            
            <category> CDH </category>
            
        </categories>
        
        
    </entry>
    
    <entry>
      <title><![CDATA[Cloudera部署SUSE]]></title>
      <url>/CDH/Cloudera%E9%83%A8%E7%BD%B2SUSE.html</url>
      <content type="html"><![CDATA[<blockquote>
<p>操作系统:SUSE11 SP3<br>CM版本:CM5.11.0<br>CDH版本:CDH5.10.1<br>使用root用户对集群进行部署</p>
</blockquote>
<h2 id="服务器配置"><a href="#服务器配置" class="headerlink" title="服务器配置"></a>服务器配置</h2><ul>
<li>关闭防火墙</li>
<li>配置时钟同步服务</li>
<li>检查数据盘是否挂载</li>
</ul>
<p>安装的机器ip:<br>192.168.65.40,192.168.65.41,192.168.65.42,192.168.65.43,192.168.65.44,192.168.65.45,192.168.65.46,192.168.65.47,192.168.65.48,192.168.65.49,</p>
<ol>
<li>安装操作系统，建议对操作系统盘做RAID1</li>
<li>如果不能连接互联网，先创建OS的repository，以便yum或zypper可以访问OS镜像</li>
<li><p>为了使集群中各个节点之间能互相通信，需要静态或动态配置节点的IP地址。如果使用动态配置，请安装DHCP和DNS服务器，具体请参见对应软件的安装文档，此不赘述；如果使用静态IP地址，请正确配置各节点的IP，并在/etc/hosts配置所有节点的静态DNS解析。<br>以cm节点为例：<code>/etc/sysconfig/network-scripts/ifcfg-eth0</code></p>
<p><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">DEVICE=&quot;bond0&quot;</div><div class="line">BOOTPROTO=&quot;static&quot;</div><div class="line">IPV6INIT=&quot;no&quot;</div><div class="line">MTU=&quot;1500&quot;</div><div class="line">NM_CONTROLLED=&quot;no&quot;</div><div class="line">ONBOOT=&quot;yes&quot;</div><div class="line">TYPE=&quot;Ethernet&quot;</div><div class="line">IPADDR=192.168.60.25</div><div class="line">NETMASK=255.255.255.0</div><div class="line">GATEWAY=192.168.60.1</div></pre></td></tr></table></figure>
</p>
<p>/etc/hosts样例</p>
<p><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">192.168.60.24  ddp-cm</div><div class="line">192.168.60.25  ddp-hnn-01</div><div class="line">192.168.60.26  ddp-dn-001</div><div class="line">192.168.60.27  ddp-dn-002</div></pre></td></tr></table></figure>
</p>
</li>
<li><p>如果机器配置有双网卡，可以做双网卡绑定；</p>
</li>
<li>关闭并禁用防火墙<br><code>/sbin/rcSuSEfirewall2 stop</code><br><code>chkconfig SuSEfirewall2_setup off</code></li>
<li>关闭SELinux<br><code>service boot.apparmor stop</code><br><code>chkconfig boot.apparmor off</code></li>
<li>重启网络服务，并初始化网络<br><code>/etc/init.d/network restart</code></li>
<li>修改transparent_hugepage参数，这一参数默认值可能会导致CDH性能下降<br>在/etc/init.d/after.local中增加一行：<br><code>if test -f /sys/kernel/mm/transparent_hugepage/enabled; then echo never &gt; /sys/kernel/mm/transparent_hugepage/enabled fi</code></li>
<li><p>禁用交换内存</p>
<p><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">$&gt; vim  /etc/sysctl.conf</div><div class="line">增加一行：vm.swappiness=0</div><div class="line">echo 0 &gt;/proc/sys/vm/swappiness</div></pre></td></tr></table></figure>
</p>
</li>
<li><p>修改/etc/security/limits.conf或者在/etc/security/limits.d下增加相应的配置文件，可以设置一些硬限制和软限制；Cloudera Manager节点会为所有节点自动做这些修改。</p>
</li>
<li>在需要作为Repo库的节点上安装必要的软件，包含HTTP服务和Repo创建工具</li>
<li><p>SUSE11 SP3 安装会有缺失的依赖需要手动安装rpm包<br>记得先查看是否有失效的zypper源,删除后添加内网配置的zypper源</p>
<p><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">zypper ar http://192.168.58.55/suse/11/x86_64/ main</div><div class="line">zypper ref</div><div class="line">zypper in postgresql91</div></pre></td></tr></table></figure>
</p>
<p>之后即可安装所缺的依赖包 python-psycopg<br><code>sudo rpm -ivh python-psycopg2-2.6.2-3.2.x86_64.rpm</code></p>
</li>
</ol>
<h2 id="MySQL建表语句"><a href="#MySQL建表语句" class="headerlink" title="MySQL建表语句:"></a>MySQL建表语句:</h2><p><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">CREATE USER &apos;hive&apos;@&apos;%&apos; IDENTIFIED BY &apos;xxxxxxxx&apos;;</div><div class="line">create database metastore_r default character set utf8;</div><div class="line">GRANT ALL PRIVILEGES ON metastore_r. * TO &apos;hive&apos;@&apos;%&apos;;</div><div class="line"></div><div class="line">create user &apos;amon&apos;@&apos;%&apos; identified by &apos;xxxxxxxx&apos;</div><div class="line">create database amon_r default character set utf8;</div><div class="line">grant all privileges on amon_r.* to &apos;amon&apos;@&apos;%&apos;;</div><div class="line"></div><div class="line">create user &apos;rman&apos;@&apos;%&apos; identified by &apos;xxxxxxxx&apos;;</div><div class="line">create database rman_r default character set utf8;</div><div class="line">grant all privileges on rman_r.* to &apos;rman&apos;@&apos;%&apos;;</div><div class="line"></div><div class="line">create user &apos;sentry&apos;@&apos;%&apos; identified by &apos;xxxxxxxx&apos;;</div><div class="line">create database sentry_r default character set utf8;</div><div class="line">grant all privileges on sentry_r.* to &apos;sentry&apos;@&apos;%&apos;;</div><div class="line"></div><div class="line">create user &apos;nav&apos;@&apos;%&apos; identified by &apos;xxxxxxxx&apos;;</div><div class="line">create database nav_r default character set utf8;</div><div class="line">grant all privileges on nav_r.* to &apos;nav&apos;@&apos;%&apos;;</div><div class="line"></div><div class="line">create user &apos;navms&apos;@&apos;%&apos; identified by &apos;xxxxxxxx&apos;;</div><div class="line">create database navms_r default character set utf8;</div><div class="line">grant all privileges on navms_r.* to &apos;navms&apos;@&apos;%&apos;;</div><div class="line"></div><div class="line">create user &apos;cm&apos;@&apos;%&apos; identified by &apos;xxxxxxxx&apos;;</div><div class="line">create database cm_r default character set utf8;</div><div class="line">grant all privileges on cm_r.* to &apos;cm&apos;@&apos;%&apos;;</div><div class="line"></div><div class="line">CREATE USER &apos;hue&apos;@&apos;%&apos; IDENTIFIED BY &apos;xxxxxxxx&apos;; </div><div class="line">create database hue_r default character set utf8;</div><div class="line">GRANT ALL PRIVILEGES ON hue_r.* TO &apos;hue&apos;@&apos;%&apos;; </div><div class="line"></div><div class="line">CREATE USER &apos;oozie&apos;@&apos;%&apos; IDENTIFIED BY &apos;xxxxxxxx&apos;; </div><div class="line">create database oozie_r default character set utf8;</div><div class="line">GRANT ALL PRIVILEGES ON oozie_r.* TO &apos;oozie&apos;@&apos;%&apos;; </div><div class="line">FLUSH PRIVILEGES;</div></pre></td></tr></table></figure>
</p>
<h3 id="Mysql-修改密码"><a href="#Mysql-修改密码" class="headerlink" title="Mysql 修改密码"></a>Mysql 修改密码</h3><p>use mysql:<br>5.7之前版本<br>update user set password=passworD(“xxxxxxxx”) where user=’hive’;<br>update user set password=password(“xxxxxxxx”) where user=’hue’;<br>update user set password=passworD(“xxxxxxxx”) where user=’oozie’;<br>5.7之后版本<br>update user set authentication_string = password(‘xxxxxxxx’) where user = ‘hue’;</p>
<h2 id="CDH软件下载与配置（Cloudera管理器节点）"><a href="#CDH软件下载与配置（Cloudera管理器节点）" class="headerlink" title="CDH软件下载与配置（Cloudera管理器节点）"></a>CDH软件下载与配置（Cloudera管理器节点）</h2><ol>
<li><a href="http://archive-primary.cloudera.com/cm5/sles/11/x86_64/cm/5/RPMS/x86_64/" target="_blank" rel="external">下载Cloudera管理器需要的rpm包</a></li>
<li>下载Parcel包（包含了CDH中的Hadoop组件）<br><a href="http://archive-primary.cloudera.com/cdh5/parcels/latest" target="_blank" rel="external">选择合适版本的parcel包,包括manifest.json文件</a>    </li>
<li>下载后将下载的RPM包放置在一个文件夹后,<code>createrepo .</code></li>
<li>执行完后，在cm目录下生成目录repodata</li>
<li>将文件移动到特定的目录，确保可以通过HTTP协议进行访问<br>Apache2服务安装<code>zypper install apache2</code></li>
</ol>
<p>http服务侦听配置:修改/etc/apache2/listen.conf 配置HTTP服务器侦听IP和端口，如：<br><code>Listen 192.168.65.50:80</code><br>http共享目录配置:修改/etc/apache2/default-server.conf 配置一个共享目录，如:<br><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">DocumentRoot &quot;/srv/www/htdocs&quot;</div><div class="line">&lt;Directory &quot;/srv/www/htdocs&quot;&gt;</div><div class="line">    Options Indexes FollowSymLinks</div><div class="line">    AllowOverride None</div><div class="line">    Order allow,deny</div><div class="line">    Allow from all</div><div class="line">&lt;/Directory&gt;</div></pre></td></tr></table></figure></p>
<h3 id="启动apache2服务："><a href="#启动apache2服务：" class="headerlink" title="启动apache2服务："></a>启动apache2服务：</h3><p><code>/etc/init.d/apache2 start</code><br>将下载的CM和Parcels介质文件存放到/srv/www/htdocs/下的自定义目录下<br>把cm的rpm包移到/var/www/htdocs/cm5.11<br>把parcels包移到/srv/www/htdocs/parcels/<br>能通过浏览器通过HTTP URL访问介质目录内容即可，如：<br>http://&lt;IP&gt;/cm5.11/<br>http://&lt;IP&gt;/parcels/<br>新建/etc/zypp/repos.d/myrepo.repo<br><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">[myrepo]</div><div class="line">name=repo</div><div class="line">baseurl=http://192.168.65.40/cm5.11/</div><div class="line">enabled=true</div><div class="line">gpgcheck=false</div></pre></td></tr></table></figure></p>
<h2 id="安装Cloudera管理器"><a href="#安装Cloudera管理器" class="headerlink" title="安装Cloudera管理器"></a>安装Cloudera管理器</h2><ol>
<li>安装JDK <code>zypper install oracle-j2sdk1.7</code></li>
<li>安装Cloudera管理器服务器 <code>zypper install cloudera-manager-daemons cloudera-manager-server</code></li>
<li><p>在需要访问MySQL的节点上安装mysql-connector-java</p>
<p><figure class="highlight bash"><table><tr><td class="code"><pre><div class="line">cp mysql-connector-java-5.1.34.jar /usr/share/java/</div><div class="line">ln –s mysql-connector-java-5.1.34.jar mysql-connector-java.jar</div></pre></td></tr></table></figure>
</p>
</li>
<li><p>为Cloudera管理器配置外部数据库</p>
<p><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">/usr/share/cmf/schema/scm_prepare_database.sh -h &lt;MYSQL_HOST&gt; &lt;DB_TYPE&gt; &lt;DATABASE&gt; &lt;USERNAME&gt; &lt;PASSWORD&gt;</div><div class="line">-h &lt;MYSQL_HOST&gt;可以不指定，默认是localhost</div><div class="line">DB_TYPE可以是mysql, oracle</div><div class="line">DATABASE即为之前为Cloudera Manager配置的数据库</div><div class="line">USERNAME/PASSWORD 即为可以访问这个数据库的用户</div><div class="line"></div><div class="line">/usr/share/cmf/schema/scm_prepare_database.sh -h 192.168.60.173 mysql &apos;cm_r&apos; cm &apos;xxxxxxxx&apos;</div></pre></td></tr></table></figure>
</p>
</li>
<li><p>启动Cloudera管理器服务器 <code>service cloudera-scm-server start</code> 启动后就可以访问Cloudera管理器页面了,Cloudera管理器的监听端口为7180</p>
</li>
</ol>
<h2 id="安装CDH集群"><a href="#安装CDH集群" class="headerlink" title="安装CDH集群"></a>安装CDH集群</h2><ol>
<li>输入账户密码admin/admin，点击“登录” </li>
<li>选择要安装的集群版本，点击“继续</li>
<li>了解CDH支持的Hadoop组件信息，点击“继续”</li>
<li>查找并选择需要安装CDH的机器，点击“继续”</li>
<li>点击“使用Parcels（建议）”右侧的“更多选项”按钮，在弹出框中设置CDH Parcel包的URL，点击“确定”</li>
<li>选择5.10.1-1.cdh5.10.1.p0.10，在“自定义存储库”填写依赖包的URL，点击“继续”</li>
<li>选择需要的JDK，点击“继续”</li>
<li>输入集群机器的登录密码，点击“继续”</li>
<li>集群依赖包安装，安装完后点击“继续”</li>
<li>Parcel包安装，安装完后点击“继续”</li>
<li>检查主机正确性，如果检查出现任何潜在问题，你可以到集群中进行修复，修复后点击“重新运行”重新检查。解决所有问题后，点击“完成”</li>
<li>选择要安装的服务套装，点击“继续”</li>
<li>选择具体的角色分配</li>
<li>设置数据库，设置完毕后点击“测试连接”，测试全部通过后点击“继续”</li>
<li>配置集群组件的相关参数，点击“继续”</li>
<li>启动集群，完成后点击“继续”</li>
<li>点击“完成”</li>
<li>访问集群</li>
<li>搞定…</li>
</ol>
<hr>
<h2 id="彻底删除CM和CDH步骤-以便重来"><a href="#彻底删除CM和CDH步骤-以便重来" class="headerlink" title="彻底删除CM和CDH步骤,以便重来"></a>彻底删除CM和CDH步骤,以便重来</h2><p><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">service cloudera-scm-agent stop</div><div class="line">umount /var/run/cloudera-scm-agent/process</div><div class="line">rm -rf /usr/share/cmf /var/lib/cloudera* /var/cache/zypp/packages/cloudera-* /var/cache/zypp/raw/cloudera-* /var/cache/zypp/solv/cloudera-* /var/log/cloudera-scm-* /var/run/cloudera-scm-* /etc/cloudera-scm-server*</div><div class="line"></div><div class="line">rpm -qa |grep cloudera</div><div class="line">rpm -e cloudera-manager-agent-5.11.1-1.cm5111.p0.9.sles11 </div><div class="line">rpm -e cloudera-manager-server-5.11.1-1.cm5111.p0.9.sles11</div><div class="line">rpm -e cloudera-manager-daemons-5.11.1-1.cm5111.p0.9.sles11</div><div class="line"></div><div class="line">rm -rf /var/lib/hadoop-* /var/lib/impala /var/lib/solr /var/lib/zookeeper /var/lib/hue /var/lib/oozie  /var/lib/pgsql  /var/lib/sqoop2  /data/dfs/  /data/impala/ /data/yarn/  /dfs/ /impala/ /yarn/  /var/run/hadoop-*/ /var/run/hdfs-*/ /usr/bin/hadoop* /usr/bin/zookeeper* /usr/bin/hbase* /usr/bin/hive* /usr/bin/hdfs /usr/bin/mapred /usr/bin/yarn /usr/bin/sqoop* /usr/bin/oozie /etc/hadoop* /etc/zookeeper* /etc/hive* /etc/hue /etc/impala /etc/sqoop* /etc/oozie /etc/hbase* /etc/hcatalog </div><div class="line">rm -rf /opt/cloudera/parcel-cache /opt/cloudera/parcels</div><div class="line"></div><div class="line">ps aux|grep  supervisord |grep -v grep | awk &apos;&#123;print &quot;kill -9 &quot;$2&#125;&apos;|sh</div><div class="line"></div><div class="line">umount /dev/sdb1 /dev/sdc1 /dev/sdd1 /dev/sde1 /dev/sdf1 /dev/sdg1 /dev/sdh1 /dev/sdi1  /dev/sdj1  /dev/sdk1  /dev/sdl1  /dev/sdm1 </div><div class="line">查看磁盘占用:fuser -m -v /mnt/test</div></pre></td></tr></table></figure>
</p>
<p>sh formatdisk.sh<br>sh mountdisk.sh </p>
<p>删除创建的数据库<br><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">drop database amon_r;      </div><div class="line">drop database hue_r;      </div><div class="line">drop database metastore_r;                </div><div class="line">drop database nav_r;      </div><div class="line">drop database navms_r;</div><div class="line">drop database oozie_r;</div><div class="line">drop database rman_r;</div><div class="line">drop database sentry_r;</div></pre></td></tr></table></figure></p>
<hr>
<h2 id="Running-a-MapReduce-Job"><a href="#Running-a-MapReduce-Job" class="headerlink" title="Running a MapReduce Job"></a>Running a MapReduce Job</h2><ol>
<li>Log into a cluster host.</li>
<li><p>Run the Hadoop PiEstimator example:</p>
<p><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">sudo -u hdfs hadoop jar \</div><div class="line">/opt/cloudera/parcels/CDH/lib/hadoop-mapreduce/hadoop-mapreduce-examples.jar \</div><div class="line">pi 10 100</div></pre></td></tr></table></figure>
</p>
</li>
<li><p>View the result of running the job by selecting the following from the top navigation bar in the Cloudera Manager Admin Console: Clusters &gt; Cluster Name &gt; YARN Applications. </p>
</li>
</ol>
]]></content>
      
        <categories>
            
            <category> CDH </category>
            
        </categories>
        
        
    </entry>
    
    <entry>
      <title><![CDATA[hive create table]]></title>
      <url>/CDH/Hive/hive%20create%20table.html</url>
      <content type="html"><![CDATA[<h2 id="创建表的语句："><a href="#创建表的语句：" class="headerlink" title="创建表的语句："></a>创建表的语句：</h2><p><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">Create [EXTERNAL] TABLE [IF NOT EXISTS] table_name </div><div class="line">[(col_name data_type [COMMENT col_comment], ...)] </div><div class="line">[COMMENT table_comment] </div><div class="line">[PARTITIONED BY (col_name data_type [COMMENT col_comment], ...)] </div><div class="line">[CLUSTERED BY (col_name, col_name, ...) [SORTED BY (col_name [ASC|DESC],...)]INTO num_buckets BUCKETS]</div><div class="line">[ROW FORMAT row_format] </div><div class="line">[STORED AS file_format] </div><div class="line">[LOCATION hdfs_path]</div></pre></td></tr></table></figure>
</p>
<p>CREATE TABLE 创建一个指定名字的表。如果相同名字的表已经存在，则抛出异常；用户可以用 IF NOT EXIST 选项来忽略这个异常。<br>EXTERNAL 关键字可以让用户创建一个外部表，在建表的同时指定一个指向实际数据的路径（LOCATION），Hive创建内部表时，会将数据移动到数据仓库指向的路径；若创建外部表，仅记录数据所在的路径，不对数据的位置做任何改变。在删除表的时候，内部表的元数据和数据会被一起删除，而外部表只删除元数据，不删除数据。<br>如果文件数据是纯文本，可以使用 STORED AS TEXTFILE。如果数据需要压缩，使用 STORED AS SEQUENCE 。<br>有分区的表可以在创建的时候使用 PARTITIONED BY 语句。一个表可以拥有一个或者多个分区，每一个分区单独存在一个目录下。而且，表和分区都可以对某个列进行 CLUSTERED BY 操作，将若干个列放入一个桶（bucket）中。也可以利用SORT BY 对数据进行排序。这样可以为特定应用提高性能。</p>
<h3 id="创建普通的表："><a href="#创建普通的表：" class="headerlink" title="创建普通的表："></a>创建普通的表：</h3><p><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">create table test_table (</div><div class="line">id int,</div><div class="line">name string,</div><div class="line">no int) </div><div class="line">row formatdelimited </div><div class="line">fieldsterminated by &apos;,&apos; </div><div class="line">stored astextfile;//指定了字段的分隔符，hive只支持单个字符的分隔符。hive默认的分隔符是\001</div></pre></td></tr></table></figure>
</p>
<h3 id="创建带有partition的表："><a href="#创建带有partition的表：" class="headerlink" title="创建带有partition的表："></a>创建带有partition的表：</h3><p><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">create table test_partition (</div><div class="line">id int,</div><div class="line">name string,</div><div class="line">no int)</div><div class="line">partitioned by(dt string) </div><div class="line">row formatdelimited </div><div class="line">fieldsterminated by &apos;,&apos; </div><div class="line">stored astextfile;</div><div class="line">load data local inpath &apos;/home/zhangxin/hive/test_hive.txt&apos; overwrite intotabletest_partition partition (dt=&apos;2012-03-05&apos;);</div></pre></td></tr></table></figure>
</p>
<h3 id="创建带有Bucket的表："><a href="#创建带有Bucket的表：" class="headerlink" title="创建带有Bucket的表："></a>创建带有Bucket的表：</h3><p><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">create table test_bucket (</div><div class="line">id int,</div><div class="line">name string,</div><div class="line">no int)</div><div class="line">partitioned by(dt string) </div><div class="line">clustered by(id) into 10 buckets --将id这一列分到10个桶中。</div><div class="line">row formatdelimited </div><div class="line">fieldsterminated by &apos;,&apos; </div><div class="line">stored astextfile;</div></pre></td></tr></table></figure>
</p>
<p>关于分桶：对列进行分桶，本质上，在进行reduce的时候，会对列的值进行hash，然后将hash的值放到特定的桶中（有点像distribute by）。<br>注意在写入分桶数据的时候，需要指定：<br><code>sethive.enforce.bucketing=true;</code></p>
<h3 id="创建external表："><a href="#创建external表：" class="headerlink" title="创建external表："></a>创建external表：</h3><p><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">create external table test_external (</div><div class="line">id int,</div><div class="line">name string,</div><div class="line">no int)</div><div class="line">row formatdelimited </div><div class="line">fieldsterminated by &apos;,&apos; </div><div class="line">location&apos;/home/zhangxin/hive/test_hive.txt&apos;;</div></pre></td></tr></table></figure>
</p>
<h3 id="创建与已知表相同结构的表Like："><a href="#创建与已知表相同结构的表Like：" class="headerlink" title="创建与已知表相同结构的表Like："></a>创建与已知表相同结构的表Like：</h3><p>只复制表的结构，而不复制表的内容。<br><code>create table test_like_table like test_bucket;</code></p>
]]></content>
      
        <categories>
            
            <category> CDH </category>
            
            <category> Hive </category>
            
        </categories>
        
        
    </entry>
    
    <entry>
      <title><![CDATA[hbase 连接超时]]></title>
      <url>/CDH/Error/hbase%20%E8%BF%9E%E6%8E%A5%E8%B6%85%E6%97%B6.html</url>
      <content type="html"><![CDATA[<h1 id="hbase-连接超时"><a href="#hbase-连接超时" class="headerlink" title="hbase 连接超时"></a>hbase 连接超时</h1><p>最开始是inood块权限有问题<br><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">6月 7, 早上8点24:55.119分    WARN    org.apache.hadoop.security.UserGroupInformation </div><div class="line">PriviledgedActionException as:hbase (auth:SIMPLE) cause:org.apache.hadoop.security.AccessControlException: Permission denied: user=hbase, access=WRITE, inode=&quot;/hbase/data/default/user_info/cc019437cc8b9ea29ab57461d9538945/info/4c3bafa7a4164b21bf71c6b3a80c0439_SeqId_222_&quot;:hadoop:supergroup:-rw-r--r--</div><div class="line">6月 7, 早上8点24:55.119分    INFO    org.apache.hadoop.ipc.Server    </div><div class="line">IPC Server handler 40 on 8020, call org.apache.hadoop.hdfs.protocol.ClientProtocol.setTimes from 10.200.60.145:34847 Call#9611380 Retry#0: org.apache.hadoop.security.AccessControlException: Permission denied: user=hbase, access=WRITE, inode=&quot;/hbase/data/default/user_info/cc019437cc8b9ea29ab57461d9538945/info/4c3bafa7a4164b21bf71c6b3a80c0439_SeqId_222_&quot;:hadoop:supergroup:-rw-r--r--</div></pre></td></tr></table></figure></p>
<p>手动修改后发现接连不断报出来 ,后来发现类似的bluk太多,直接修改chown上级目录 </p>
<p>之后发现 有很多的连接超时<br><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">6月 7, 下午1点38:29.634分	WARN	org.apache.zookeeper.server.NIOServerCnxnFactory	</div><div class="line">Too many connections from /10.200.60.123 - max is 60</div><div class="line">6月 7, 下午1点38:30.006分	INFO	org.apache.zookeeper.server.NIOServerCnxnFactory	</div><div class="line">Accepted socket connection from /172.16.70.43:41978</div><div class="line">6月 7, 下午1点38:30.010分	INFO	org.apache.zookeeper.server.ZooKeeperServer	</div><div class="line">Client attempting to establish new session at /172.16.70.43:41978</div><div class="line">6月 7, 下午1点38:30.019分	INFO	org.apache.zookeeper.server.ZooKeeperServer	</div><div class="line">Established session 0x25c62548f45168a with negotiated timeout 60000 for client /172.16.70.43:41978</div><div class="line">6月 7, 下午1点38:30.205分	INFO	org.apache.zookeeper.server.NIOServerCnxn	</div><div class="line">Closed socket connection for client /172.16.70.43:41978 which had sessionid 0x25c62548f45168a</div><div class="line">6月 7, 下午1点38:30.211分	WARN	org.apache.zookeeper.server.NIOServerCnxnFactory	</div><div class="line">Too many connections from /10.200.60.123 - max is 60</div><div class="line">6月 7, 下午1点38:30.399分	WARN	org.apache.zookeeper.server.NIOServerCnxnFactory	</div><div class="line">Too many connections from /10.200.60.123 - max is 60</div><div class="line">6月 7, 下午1点38:31.321分	WARN	org.apache.zookeeper.server.NIOServerCnxnFactory	</div><div class="line">Too many connections from /10.200.60.123 - max is 60</div><div class="line">6月 7, 下午1点38:32.097分	WARN	org.apache.zookeeper.server.NIOServerCnxnFactory	</div><div class="line">Too many connections from /10.200.60.123 - max is 60</div><div class="line">6月 7, 下午1点38:32.314分	WARN	org.apache.zookeeper.server.NIOServerCnxnFactory	</div><div class="line">Too many connections from /10.200.60.123 - max is 60</div></pre></td></tr></table></figure></p>
<p>这个问题 前几天一直有报错误但是没有表明哪一台连接数特别多 今天达到max后才报出来</p>
<p><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">6月 7, 上午9点37:02.713分    INFO    org.apache.zookeeper.server.ZooKeeperServer </div><div class="line">Client attempting to establish new session at /10.200.65.40:52131</div><div class="line">6月 7, 上午9点37:02.717分    INFO    org.apache.zookeeper.server.ZooKeeperServer </div><div class="line">Established session 0x25c62548f450748 with negotiated timeout 60000 for client /10.200.65.40:52131</div><div class="line">6月 7, 上午9点37:02.731分    WARN    org.apache.zookeeper.server.NIOServerCnxnFactory    </div><div class="line">Too many connections from /10.200.60.123 - max is 60</div><div class="line">6月 7, 上午9点37:02.862分    INFO    org.apache.zookeeper.server.NIOServerCnxnFactory    </div><div class="line">Accepted socket connection from /10.200.65.40:52369</div><div class="line">6月 7, 上午9点37:02.862分    INFO    org.apache.zookeeper.server.ZooKeeperServer </div><div class="line">Client attempting to establish new session at /10.200.65.40:52369</div><div class="line">6月 7, 上午9点37:02.874分    INFO    org.apache.zookeeper.server.ZooKeeperServer </div><div class="line">Established session 0x25c62548f450749 with negotiated timeout 60000 for client /10.200.65.40:52369</div></pre></td></tr></table></figure>
</p>
<p>123 </p>
<p>在123这台机器上看到与145的连接全部的端口都是2181</p>
<table>
<thead>
<tr>
<th style="text-align:left">Component</th>
<th style="text-align:left">Service</th>
<th style="text-align:left">Qualifier</th>
<th style="text-align:left">Port</th>
<th style="text-align:left">Access Requirement</th>
<th>Configuration</th>
<th>Comment</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">Hadoop HDFS</td>
<td style="text-align:left">HQuorumPeer</td>
<td style="text-align:left">2181</td>
<td style="text-align:left">Internal</td>
<td style="text-align:left">hbase. zookeeper. property. clientPort</td>
<td>HBase-managed ZooKeeper mode</td>
</tr>
</tbody>
</table>
<p><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">ddp-dn-041:~ # netstat -lantp|grep 2181  </div><div class="line">tcp        0      0 10.200.60.123:34306     10.200.60.145:2181      ESTABLISHED 32710/java          </div><div class="line">tcp        0      0 10.200.60.123:56126     10.200.60.144:2181      ESTABLISHED 32710/java          </div><div class="line">tcp        0      0 10.200.60.123:47177     10.200.60.144:2181      ESTABLISHED 32710/java          </div><div class="line">tcp        0      0 10.200.60.123:57633     10.200.60.145:2181      ESTABLISHED 32710/java          </div><div class="line">tcp        0      0 10.200.60.123:32828     10.200.60.144:2181      ESTABLISHED 32710/java          </div><div class="line">tcp        0      0 10.200.60.123:57625     10.200.60.145:2181      ESTABLISHED 32710/java          </div><div class="line">tcp        0      0 10.200.60.123:48795     10.200.60.145:2181      ESTABLISHED 32710/java          </div><div class="line">tcp        0      0 10.200.60.123:47189     10.200.60.144:2181      ESTABLISHED 32710/java          </div><div class="line">tcp        0      0 10.200.60.123:57635     10.200.60.145:2181      ESTABLISHED 32710/java          </div><div class="line">tcp        0      0 10.200.60.123:52033     10.200.60.144:2181      ESTABLISHED 32710/java          </div><div class="line">tcp        0      0 10.200.60.123:53245     10.200.60.145:2181      ESTABLISHED 32710/java          </div><div class="line">tcp        0      0 10.200.60.123:57591     10.200.60.145:2181      ESTABLISHED 32710/java          </div><div class="line">tcp        0      0 10.200.60.123:48781     10.200.60.145:2181      ESTABLISHED 32710/java          </div><div class="line">tcp        0      0 10.200.60.123:48779     10.200.60.145:2181      ESTABLISHED 32710/java          </div><div class="line">tcp        0      0 10.200.60.123:56177     10.200.60.144:2181      ESTABLISHED 32710/java          </div><div class="line">tcp        0      0 10.200.60.123:47183     10.200.60.144:2181      ESTABLISHED 32710/java          </div><div class="line">tcp        0      0 10.200.60.123:56165     10.200.60.144:2181      ESTABLISHED 32710/java          </div><div class="line">tcp        0      0 10.200.60.123:39211     10.200.60.145:2181      ESTABLISHED 32710/java          </div><div class="line">tcp        0      0 10.200.60.123:34249     10.200.60.145:2181      ESTABLISHED 32710/java          </div><div class="line">tcp        0      0 10.200.60.123:47164     10.200.60.144:2181      ESTABLISHED 32710/java          </div><div class="line">tcp        0      0 10.200.60.123:48857     10.200.60.145:2181      ESTABLISHED 32710/java          </div><div class="line">tcp        0      0 10.200.60.123:56148     10.200.60.144:2181      ESTABLISHED 32710/java          </div><div class="line">tcp        0      0 10.200.60.123:57610     10.200.60.145:2181      ESTABLISHED 32710/java          </div><div class="line">tcp        0      0 10.200.60.123:57615     10.200.60.145:2181      ESTABLISHED 32710/java          </div><div class="line">tcp        0      0 10.200.60.123:48612     10.200.60.145:2181      ESTABLISHED 32710/java          </div><div class="line">tcp        0      0 10.200.60.123:34291     10.200.60.145:2181      ESTABLISHED 32710/java          </div><div class="line">tcp        0      0 10.200.60.123:57686     10.200.60.145:2181      ESTABLISHED 32710/java          </div><div class="line">tcp        0      0 10.200.60.123:34300     10.200.60.145:2181      ESTABLISHED 32710/java          </div><div class="line">tcp        0      0 10.200.60.123:57623     10.200.60.145:2181      ESTABLISHED 32710/java          </div><div class="line">tcp        0      0 10.200.60.123:56172     10.200.60.144:2181      ESTABLISHED 32710/java          </div><div class="line">tcp        0      0 10.200.60.123:56168     10.200.60.144:2181      ESTABLISHED 32710/java          </div><div class="line">tcp        0      0 10.200.60.123:47170     10.200.60.144:2181      ESTABLISHED 32710/java          </div><div class="line">tcp        0      0 10.200.60.123:32904     10.200.60.144:2181      ESTABLISHED 32710/java          </div><div class="line">tcp        0      0 10.200.60.123:32817     10.200.60.144:2181      ESTABLISHED 32710/java          </div><div class="line">tcp        0      0 10.200.60.123:57590     10.200.60.145:2181      ESTABLISHED 32710/java          </div><div class="line">tcp        0      0 10.200.60.123:48615     10.200.60.145:2181      ESTABLISHED 32710/java          </div><div class="line">tcp        0      0 10.200.60.123:48792     10.200.60.145:2181      ESTABLISHED 32710/java          </div><div class="line">tcp        0      0 10.200.60.123:56371     10.200.60.144:2181      ESTABLISHED 32710/java          </div><div class="line">tcp        0      0 10.200.60.123:57589     10.200.60.145:2181      ESTABLISHED 32710/java          </div><div class="line">tcp        0      0 10.200.60.123:56163     10.200.60.144:2181      ESTABLISHED 32710/java          </div><div class="line">tcp        0      0 10.200.60.123:56123     10.200.60.144:2181      ESTABLISHED 32710/java          </div><div class="line">tcp        0      0 10.200.60.123:48646     10.200.60.145:2181      ESTABLISHED 32710/java          </div><div class="line">tcp        0      0 10.200.60.123:56178     10.200.60.144:2181      ESTABLISHED 32710/java          </div><div class="line">tcp        0      0 10.200.60.123:34280     10.200.60.145:2181      ESTABLISHED 32710/java          </div><div class="line">tcp        0      0 10.200.60.123:48633     10.200.60.145:2181      ESTABLISHED 32710/java          </div><div class="line">tcp        0      0 10.200.60.123:56180     10.200.60.144:2181      ESTABLISHED 32710/java          </div><div class="line">tcp        0      0 10.200.60.123:56152     10.200.60.144:2181      ESTABLISHED 32710/java          </div><div class="line">tcp        0      0 10.200.60.123:56162     10.200.60.144:2181      ESTABLISHED 32710/java          </div><div class="line">tcp        0      0 10.200.60.123:57594     10.200.60.145:2181      ESTABLISHED 32710/java          </div><div class="line">tcp        0      0 10.200.60.123:51789     10.200.60.144:2181      ESTABLISHED 32710/java          </div><div class="line">tcp        0      0 10.200.60.123:57628     10.200.60.145:2181      ESTABLISHED 32710/java          </div><div class="line">tcp        0      0 10.200.60.123:56375     10.200.60.144:2181      ESTABLISHED 32710/java          </div><div class="line">tcp        0      0 10.200.60.123:32819     10.200.60.144:2181      ESTABLISHED 32710/java          </div><div class="line">tcp        0      0 10.200.60.123:48651     10.200.60.145:2181      ESTABLISHED 32710/java          </div><div class="line">tcp        0      0 10.200.60.123:47172     10.200.60.144:2181      ESTABLISHED 32710/java          </div><div class="line">tcp        0      0 10.200.60.123:57616     10.200.60.145:2181      ESTABLISHED 32710/java          </div><div class="line">tcp        0      0 10.200.60.123:56127     10.200.60.144:2181      ESTABLISHED 32710/java          </div><div class="line">tcp        0      0 10.200.60.123:56149     10.200.60.144:2181      ESTABLISHED 32710/java          </div><div class="line">tcp        0      0 10.200.60.123:48640     10.200.60.145:2181      ESTABLISHED 32710/java          </div><div class="line">tcp        0      0 10.200.60.123:56396     10.200.60.144:2181      ESTABLISHED 32710/java          </div><div class="line">tcp        0      0 10.200.60.123:56380     10.200.60.144:2181      ESTABLISHED 32710/java          </div><div class="line">tcp        0      0 10.200.60.123:56160     10.200.60.144:2181      ESTABLISHED 32710/java          </div><div class="line">tcp        0      0 10.200.60.123:34293     10.200.60.145:2181      ESTABLISHED 32710/java          </div><div class="line">tcp        0      0 10.200.60.123:48620     10.200.60.145:2181      ESTABLISHED 32710/java          </div><div class="line">tcp        0      0 10.200.60.123:57618     10.200.60.145:2181      ESTABLISHED 32710/java          </div><div class="line">tcp        0      0 10.200.60.123:47185     10.200.60.144:2181      ESTABLISHED 32710/java          </div><div class="line">tcp        0      0 10.200.60.123:48638     10.200.60.145:2181      ESTABLISHED 32710/java          </div><div class="line">tcp        0      0 10.200.60.123:57620     10.200.60.145:2181      ESTABLISHED 32710/java          </div><div class="line">tcp        0      0 10.200.60.123:57592     10.200.60.145:2181      ESTABLISHED 32710/java          </div><div class="line">tcp        0      0 10.200.60.123:57595     10.200.60.145:2181      ESTABLISHED 32710/java          </div><div class="line">tcp        0      0 10.200.60.123:56173     10.200.60.144:2181      ESTABLISHED 32710/java          </div><div class="line">tcp        0      0 10.200.60.123:51992     10.200.60.144:2181      ESTABLISHED 32710/java          </div><div class="line">tcp        0      0 10.200.60.123:56158     10.200.60.144:2181      ESTABLISHED 32710/java          </div><div class="line">tcp        0      0 10.200.60.123:56171     10.200.60.144:2181      ESTABLISHED 32710/java          </div><div class="line">tcp        0      0 10.200.60.123:56167     10.200.60.144:2181      ESTABLISHED 32710/java          </div><div class="line">tcp        0      0 10.200.60.123:48785     10.200.60.145:2181      ESTABLISHED 32710/java          </div><div class="line">tcp        0      0 10.200.60.123:57602     10.200.60.145:2181      ESTABLISHED 32710/java          </div><div class="line">tcp        0      0 10.200.60.123:32851     10.200.60.144:2181      ESTABLISHED 32710/java          </div><div class="line">tcp        0      0 10.200.60.123:57609     10.200.60.145:2181      ESTABLISHED 32710/java          </div><div class="line">tcp        0      0 10.200.60.123:53145     10.200.60.145:2181      ESTABLISHED 32710/java          </div><div class="line">tcp        0      0 10.200.60.123:34549     10.200.60.145:2181      ESTABLISHED 32710/java          </div><div class="line">tcp        0      0 10.200.60.123:56154     10.200.60.144:2181      ESTABLISHED 32710/java          </div><div class="line">tcp        0      0 10.200.60.123:56391     10.200.60.144:2181      ESTABLISHED 32710/java          </div><div class="line">tcp        0      0 10.200.60.123:56170     10.200.60.144:2181      ESTABLISHED 32710/java          </div><div class="line">tcp        0      0 10.200.60.123:48790     10.200.60.145:2181      ESTABLISHED 32710/java          </div><div class="line">tcp        0      0 10.200.60.123:57772     10.200.60.145:2181      ESTABLISHED 32710/java          </div><div class="line">tcp        0      0 10.200.60.123:57606     10.200.60.145:2181      ESTABLISHED 32710/java          </div><div class="line">tcp        0      0 10.200.60.123:34297     10.200.60.145:2181      ESTABLISHED 32710/java          </div><div class="line">tcp        0      0 10.200.60.123:32830     10.200.60.144:2181      ESTABLISHED 32710/java          </div><div class="line">tcp        0      0 10.200.60.123:56142     10.200.60.144:2181      ESTABLISHED 32710/java          </div><div class="line">tcp        0      0 10.200.60.123:47168     10.200.60.144:2181      ESTABLISHED 32710/java          </div><div class="line">tcp        0      0 10.200.60.123:51980     10.200.60.144:2181      ESTABLISHED 32710/java          </div><div class="line">tcp        0      0 10.200.60.123:56146     10.200.60.144:2181      ESTABLISHED 32710/java          </div><div class="line">tcp        0      0 10.200.60.123:57599     10.200.60.145:2181      ESTABLISHED 32710/java          </div><div class="line">tcp        0      0 10.200.60.123:47158     10.200.60.144:2181      ESTABLISHED 32710/java          </div><div class="line">tcp        0      0 10.200.60.123:57603     10.200.60.145:2181      ESTABLISHED 32710/java          </div><div class="line">tcp        0      0 10.200.60.123:57587     10.200.60.145:2181      ESTABLISHED 32710/java          </div><div class="line">tcp        0      0 10.200.60.123:57612     10.200.60.145:2181      ESTABLISHED 32710/java          </div><div class="line">tcp        0      0 10.200.60.123:47312     10.200.60.144:2181      ESTABLISHED 32710/java          </div><div class="line">tcp        0      0 10.200.60.123:32844     10.200.60.144:2181      ESTABLISHED 32710/java          </div><div class="line">tcp        0      0 10.200.60.123:32836     10.200.60.144:2181      ESTABLISHED 32710/java          </div><div class="line">tcp        0      0 10.200.60.123:56138     10.200.60.144:2181      ESTABLISHED 32710/java          </div><div class="line">tcp        0      0 10.200.60.123:48783     10.200.60.145:2181      ESTABLISHED 32710/java          </div><div class="line">tcp        0      0 10.200.60.123:34308     10.200.60.145:2181      ESTABLISHED 32710/java          </div><div class="line">tcp        0      0 10.200.60.123:56179     10.200.60.144:2181      ESTABLISHED 32710/java          </div><div class="line">tcp        0      0 10.200.60.123:56145     10.200.60.144:2181      ESTABLISHED 32710/java          </div><div class="line">tcp        0      0 10.200.60.123:47166     10.200.60.144:2181      ESTABLISHED 32710/java          </div><div class="line">tcp        0      0 10.200.60.123:47338     10.200.60.144:2181      ESTABLISHED 32710/java          </div><div class="line">tcp        0      0 10.200.60.123:57583     10.200.60.145:2181      ESTABLISHED 32710/java          </div><div class="line">tcp        0      0 10.200.60.123:34359     10.200.60.145:2181      ESTABLISHED 32710/java          </div><div class="line">tcp        0      0 10.200.60.123:32908     10.200.60.144:2181      ESTABLISHED 32710/java          </div><div class="line">tcp        0      0 10.200.60.123:34282     10.200.60.145:2181      ESTABLISHED 32710/java          </div><div class="line">tcp        0      0 10.200.60.123:56175     10.200.60.144:2181      ESTABLISHED 32710/java          </div><div class="line">tcp        0      0 10.200.60.123:56137     10.200.60.144:2181      ESTABLISHED 32710/java          </div><div class="line">tcp        0      0 10.200.60.123:56366     10.200.60.144:2181      ESTABLISHED 32710/java          </div><div class="line">tcp        0      0 10.200.60.123:57614     10.200.60.145:2181      ESTABLISHED 32710/java          </div><div class="line">tcp        0      0 10.200.60.123:48787     10.200.60.145:2181      ESTABLISHED 32710/java          </div><div class="line">tcp        0      0 10.200.60.123:56399     10.200.60.144:2181      ESTABLISHED 32710/java          </div><div class="line">tcp        0      0 10.200.60.123:34284     10.200.60.145:2181      ESTABLISHED 32710/java          </div><div class="line">tcp        0      0 10.200.60.123:57593     10.200.60.145:2181      ESTABLISHED 32710/java          </div><div class="line">ddp-dn-041:~ # ps aux|grep 32710</div><div class="line">root       475  0.0  0.0   4532   564 pts/7    S+   13:47   0:00 grep 32710</div><div class="line">root     32710  0.0  3.2 33279488 4239192 ?    Sl   May12  19:45 /usr/java/jdk1.7.0_67-cloudera/bin/java -Djava.util.logging.config.file=/home/hadoop/bin/apache-tomcat-7.0.73/conf/logging.properties -Djava.util.logging.manager=org.apache.juli.ClassLoaderLogManager -Djdk.tls.ephemeralDHKeySize=2048 -Djava.endorsed.dirs=/home/hadoop/bin/apache-tomcat-7.0.73/endorsed -classpath /home/hadoop/bin/apache-tomcat-7.0.73/bin/bootstrap.jar:/home/hadoop/bin/apache-tomcat-7.0.73/bin/tomcat-juli.jar -Dcatalina.base=/home/hadoop/bin/apache-tomcat-7.0.73 -Dcatalina.home=/home/hadoop/bin/apache-tomcat-7.0.73 -Djava.io.tmpdir=/home/hadoop/bin/apache-tomcat-7.0.73/temp org.apache.catalina.startup.Bootstrap start</div></pre></td></tr></table></figure>
</p>
<p>145<br><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">ddp-hb-178:~ #  netstat -lantp|grep 2181</div><div class="line">tcp        0      0 0.0.0.0:2181            0.0.0.0:*               LISTEN      18867/java          </div><div class="line">tcp        0      0 10.200.60.145:2181      10.200.65.16:42563      TIME_WAIT   -                   </div><div class="line">tcp        0      0 10.200.60.145:2181      10.200.60.123:57625     ESTABLISHED 18867/java          </div><div class="line">tcp        0      0 10.200.60.145:2181      10.200.60.123:34359     ESTABLISHED 18867/java          </div><div class="line">tcp        0      0 10.200.60.145:2181      10.200.60.155:35202     ESTABLISHED 18867/java          </div><div class="line">tcp        0      0 10.200.60.145:2181      10.200.60.123:34291     ESTABLISHED 18867/java          </div><div class="line">tcp        0      0 10.200.60.145:2181      10.200.60.123:48781     ESTABLISHED 18867/java          </div><div class="line">tcp        0      0 10.200.60.145:2181      10.200.60.147:47393     ESTABLISHED 18867/java          </div><div class="line">tcp        0      0 10.200.60.145:2181      10.200.65.34:10906      ESTABLISHED 18867/java          </div><div class="line">tcp        0      0 10.200.60.145:2181      10.200.60.123:57606     ESTABLISHED 18867/java          </div><div class="line">tcp        0      0 10.200.60.145:2181      10.200.60.123:48615     ESTABLISHED 18867/java          </div><div class="line">tcp        0      0 10.200.60.145:2181      10.200.65.24:39053      TIME_WAIT   -                   </div><div class="line">tcp        0      0 10.200.60.145:2181      10.200.65.24:39081      TIME_WAIT   -                   </div><div class="line">tcp        0      0 10.200.60.145:2181      10.200.60.123:34297     ESTABLISHED 18867/java          </div><div class="line">tcp        0      0 10.200.60.145:2181      10.200.60.123:34280     ESTABLISHED 18867/java          </div><div class="line">tcp        0      0 10.200.60.145:45361     10.200.60.144:2181      ESTABLISHED 15492/java          </div><div class="line">tcp        0      0 10.200.60.145:2181      10.200.60.151:36084     ESTABLISHED 18867/java          </div><div class="line">tcp        0      0 10.200.60.145:45352     10.200.60.144:2181      ESTABLISHED 15492/java          </div><div class="line">tcp        0      0 10.200.60.145:2181      10.200.60.123:53245     ESTABLISHED 18867/java          </div><div class="line">tcp        0      0 10.200.60.145:2181      10.200.65.24:39062      TIME_WAIT   -                   </div><div class="line">tcp        0      0 10.200.60.145:2181      10.200.60.123:48790     ESTABLISHED 18867/java          </div><div class="line">tcp        0      0 10.200.60.145:2181      10.200.60.123:57612     ESTABLISHED 18867/java          </div><div class="line">tcp        0      0 10.200.60.145:2181      10.200.60.148:58812     ESTABLISHED 18867/java          </div><div class="line">tcp        0      0 10.200.60.145:2181      10.200.60.155:35191     ESTABLISHED 18867/java          </div><div class="line">tcp        0      0 10.200.60.145:2181      10.200.60.147:47384     ESTABLISHED 18867/java          </div><div class="line">tcp        0      0 10.200.60.145:2181      10.200.60.123:48795     ESTABLISHED 18867/java          </div><div class="line">tcp        0      0 10.200.60.145:2181      10.200.60.123:48646     ESTABLISHED 18867/java          </div><div class="line">tcp        0      0 10.200.60.145:2181      10.200.60.123:34308     ESTABLISHED 18867/java          </div><div class="line">tcp        0      0 10.200.60.145:45351     10.200.60.144:2181      ESTABLISHED 15492/java          </div><div class="line">tcp        0      0 10.200.60.145:2181      10.200.60.123:57614     ESTABLISHED 18867/java          </div><div class="line">tcp        0      0 10.200.60.145:2181      10.200.60.123:48779     ESTABLISHED 18867/java          </div><div class="line">tcp        0      0 10.200.60.145:2181      10.200.65.47:51835      ESTABLISHED 18867/java          </div><div class="line">tcp        0      0 10.200.60.145:2181      10.200.60.123:57592     ESTABLISHED 18867/java          </div><div class="line">tcp        0      0 10.200.60.145:2181      10.200.60.145:59735     ESTABLISHED 18867/java          </div><div class="line">tcp        0      0 10.200.60.145:2181      10.200.65.34:10903      ESTABLISHED 18867/java          </div><div class="line">tcp        0      0 10.200.60.145:2181      10.200.60.123:34306     ESTABLISHED 18867/java          </div><div class="line">tcp        0      0 10.200.60.145:2181      10.200.60.123:57620     ESTABLISHED 18867/java          </div><div class="line">tcp        0      0 10.200.60.145:2181      10.200.65.24:39085      TIME_WAIT   -                   </div><div class="line">tcp        0      0 10.200.60.145:2181      10.200.65.34:10905      ESTABLISHED 18867/java          </div><div class="line">tcp        0      0 10.200.60.145:2181      10.200.60.144:51915     ESTABLISHED 18867/java          </div><div class="line">tcp        0      0 10.200.60.145:2181      10.200.60.123:57602     ESTABLISHED 18867/java          </div><div class="line">tcp        0      0 10.200.60.145:2181      10.200.60.123:48640     ESTABLISHED 18867/java          </div><div class="line">tcp        0      0 10.200.60.145:2181      10.200.60.123:34282     ESTABLISHED 18867/java          </div><div class="line">tcp        0      0 10.200.60.145:2181      10.200.65.16:42535      TIME_WAIT   -                   </div><div class="line">tcp        0      0 10.200.60.145:2181      10.200.60.123:57583     ESTABLISHED 18867/java          </div><div class="line">tcp        0      0 10.200.60.145:2181      10.200.60.123:48620     ESTABLISHED 18867/java          </div><div class="line">tcp        0      0 10.200.60.145:2181      10.200.60.123:57603     ESTABLISHED 18867/java          </div><div class="line">tcp        0      0 10.200.60.145:2181      10.200.65.16:42534      TIME_WAIT   -                   </div><div class="line">tcp        0      0 10.200.60.145:2181      10.200.60.123:48783     ESTABLISHED 18867/java          </div><div class="line">tcp        0      0 10.200.60.145:2181      10.200.60.123:48612     ESTABLISHED 18867/java          </div><div class="line">tcp        0      0 10.200.60.145:2181      10.200.60.123:34293     ESTABLISHED 18867/java          </div><div class="line">tcp        0      0 10.200.60.145:2181      10.200.60.123:34300     ESTABLISHED 18867/java          </div><div class="line">tcp        0      0 10.200.60.145:2181      10.200.60.156:44113     ESTABLISHED 18867/java          </div><div class="line">tcp        0      0 10.200.60.145:2181      10.200.60.123:57599     ESTABLISHED 18867/java          </div><div class="line">tcp        0      0 10.200.60.145:2181      10.200.60.123:57594     ESTABLISHED 18867/java          </div><div class="line">tcp        0      0 10.200.60.145:2181      10.200.60.123:57615     ESTABLISHED 18867/java          </div><div class="line">tcp        0      0 10.200.60.145:2181      10.200.65.16:49370      ESTABLISHED 18867/java          </div><div class="line">tcp        0      0 10.200.60.145:2181      10.200.65.34:10904      ESTABLISHED 18867/java          </div><div class="line">tcp        0      0 10.200.60.145:2181      10.200.60.123:48638     ESTABLISHED 18867/java          </div><div class="line">tcp        0      0 10.200.60.145:2181      10.200.60.123:48651     ESTABLISHED 18867/java          </div><div class="line">tcp        0      0 10.200.60.145:2181      10.200.65.33:18797      ESTABLISHED 18867/java          </div><div class="line">tcp        0      0 10.200.60.145:2181      10.200.60.123:57609     ESTABLISHED 18867/java          </div><div class="line">tcp        0      0 10.200.60.145:2181      10.200.60.123:57590     ESTABLISHED 18867/java          </div><div class="line">tcp        0      0 10.200.60.145:2181      10.200.60.123:57623     ESTABLISHED 18867/java          </div><div class="line">tcp        0      0 10.200.60.145:2181      10.200.60.123:48633     ESTABLISHED 18867/java          </div><div class="line">tcp        0      0 10.200.60.145:2181      10.200.60.123:57633     ESTABLISHED 18867/java          </div><div class="line">tcp        0      0 10.200.60.145:2181      10.200.65.16:42558      TIME_WAIT   -                   </div><div class="line">tcp        0      0 10.200.60.145:2181      10.200.60.123:57589     ESTABLISHED 18867/java          </div><div class="line">tcp        0      0 10.200.60.145:2181      10.200.60.123:57616     ESTABLISHED 18867/java          </div><div class="line">tcp        0      0 10.200.60.145:2181      10.200.60.123:34549     ESTABLISHED 18867/java          </div><div class="line">tcp        0      0 10.200.60.145:2181      10.200.60.123:34284     ESTABLISHED 18867/java          </div><div class="line">tcp        0      0 10.200.60.145:2181      10.200.60.123:57618     ESTABLISHED 18867/java          </div><div class="line">tcp        0      0 10.200.60.145:2181      10.200.60.123:57591     ESTABLISHED 18867/java          </div><div class="line">tcp        0      0 10.200.60.145:2181      10.200.60.144:51909     ESTABLISHED 18867/java          </div><div class="line">tcp        0      0 10.200.60.145:2181      10.200.60.123:48857     ESTABLISHED 18867/java          </div><div class="line">tcp        0      0 10.200.60.145:2181      10.200.60.123:48785     ESTABLISHED 18867/java          </div><div class="line">tcp        0      0 10.200.60.145:2181      10.200.60.123:48792     ESTABLISHED 18867/java          </div><div class="line">tcp        0      0 10.200.60.145:45358     10.200.60.144:2181      ESTABLISHED 15007/java          </div><div class="line">tcp        0      0 10.200.60.145:2181      10.200.60.148:58809     ESTABLISHED 18867/java          </div><div class="line">tcp        0      0 10.200.60.145:2181      10.200.24.54:36651      ESTABLISHED 18867/java          </div><div class="line">tcp        0      0 10.200.60.145:2181      10.200.60.123:57628     ESTABLISHED 18867/java          </div><div class="line">tcp        0      0 10.200.60.145:2181      10.200.60.123:57587     ESTABLISHED 18867/java          </div><div class="line">tcp        0      0 10.200.60.145:2181      10.200.60.123:57610     ESTABLISHED 18867/java          </div><div class="line">tcp        0      0 10.200.60.145:2181      10.200.60.123:57686     ESTABLISHED 18867/java          </div><div class="line">tcp        0      0 10.200.60.145:2181      10.200.60.123:57593     ESTABLISHED 18867/java          </div><div class="line">tcp        0      0 10.200.60.145:38108     10.200.60.147:2181      ESTABLISHED 15492/java          </div><div class="line">tcp        0      0 10.200.60.145:2181      10.200.60.123:57595     ESTABLISHED 18867/java          </div><div class="line">tcp        0      0 10.200.60.145:2181      10.200.60.123:53145     ESTABLISHED 18867/java          </div><div class="line">tcp        0      0 10.200.60.145:2181      10.200.60.123:34249     ESTABLISHED 18867/java          </div><div class="line">tcp        0      0 10.200.60.145:2181      10.200.60.114:54498     TIME_WAIT   -                   </div><div class="line">tcp        0      0 10.200.60.145:2181      10.200.60.123:48787     ESTABLISHED 18867/java          </div><div class="line">tcp        0      0 10.200.60.145:2181      10.200.65.16:42543      TIME_WAIT   -                   </div><div class="line">tcp        0      0 10.200.60.145:2181      10.200.65.24:39071      TIME_WAIT   -                   </div><div class="line">tcp        0      0 10.200.60.145:2181      10.200.60.123:57772     ESTABLISHED 18867/java          </div><div class="line">tcp        0      0 10.200.60.145:2181      10.200.60.123:39211     ESTABLISHED 18867/java          </div><div class="line">tcp        0      0 10.200.60.145:2181      10.200.60.146:50729     ESTABLISHED 18867/java          </div><div class="line">tcp        0      0 10.200.60.145:2181      10.200.60.144:51912     ESTABLISHED 18867/java          </div><div class="line">tcp        0      0 10.200.60.145:2181      10.200.65.16:49368      ESTABLISHED 18867/java          </div><div class="line">tcp        0      0 10.200.60.145:2181      10.200.60.123:57635     ESTABLISHED 18867/java          </div><div class="line">tcp        0      0 10.200.60.145:2181      10.200.65.33:18798      ESTABLISHED 18867/java          </div><div class="line">tcp        0      0 10.200.60.145:59735     10.200.60.145:2181      ESTABLISHED 17996/java          </div><div class="line">ddp-hb-178:~ # ps aux|grep 18867</div><div class="line">root      1402  0.0  0.0   4532   560 pts/1    S+   13:51   0:00 grep 18867</div><div class="line">112      18867  0.5  0.4 1546620 596552 ?      Sl   Jun01  45:48 /usr/java/jdk1.7.0_67-cloudera/bin/java -cp /var/run/cloudera-scm-agent/process/47146-zookeeper-server:/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/lib/zookeeper/lib/log4j.jar:/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/lib/zookeeper/build/*:/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/lib/zookeeper/build/lib/*:/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/lib/zookeeper/*:/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/lib/zookeeper/lib/*:/usr/share/cmf/lib/plugins/tt-instrumentation-5.10.0.jar:/usr/share/cmf/lib/plugins/event-publish-5.10.0-shaded.jar -Djava.net.preferIPv4Stack=true -Dzookeeper.log.file=zookeeper-cmf-zookeeper2-SERVER-ddp-hb-178.cmdmp.com.log -Dzookeeper.log.dir=/var/log/zookeeper -Dcom.sun.management.jmxremote.port=9010 -Dcom.sun.management.jmxremote.rmi.port=9010 -Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote.ssl=false -Djute.maxbuffer=4194304 -Dzookeeper.datadir.autocreate=false -Xms1073741824 -Xmx1073741824 -XX:OnOutOfMemoryError=/usr/lib64/cmf/service/common/killparent.sh org.apache.zookeeper.server.quorum.QuorumPeerMain /var/run/cloudera-scm-agent/process/47146-zookeeper-server/zoo.cfg</div></pre></td></tr></table></figure></p>
]]></content>
      
        <categories>
            
            <category> CDH </category>
            
            <category> Error </category>
            
        </categories>
        
        
    </entry>
    
    <entry>
      <title><![CDATA[Recv-Q]]></title>
      <url>/CDH/Error/Recv-Q.html</url>
      <content type="html"><![CDATA[<p>Recv-Q：表示收到的数据中还有多少没有被进程取走（通过recv）</p>
<p>Send-Q：表示需要发送的数据还有多少没有被发出</p>
<p>所以，一般来说这两个值都是0，如果不为0且持续增长，那就表明程序出现了问题。</p>
<p>比如Recv-Q的数字持续增长，表示没有进程去取这些收到的数据。比如使用select+recv来收数据的时候，由于select有1024这个限制，所以如果socket的FD大于1024的时候，就会导致这个socket FD上的数据不会被select检测到从而导致recv不会被调用。<br>说<br>所以，通过netstat的这两个值就可以简单判断程序收不到包到底是包没到还是包没有被进程recv。</p>
<p>Recv-Q Send-Q分别表示网络接收队列，发送队列。Q是Queue的缩写。<br>这两个值通常应该为0，如果不为0可能是有问题的。packets在两个队列里都不应该有堆积状态。可接受短暂的非0情况。如文中的示例，短暂的Send-Q队列发送pakets非0是正常状态。<br>如果接收队列Recv-Q一直处于阻塞状态，可能是遭受了拒绝服务 denial-of-service 攻击。<br>如果发送队列Send-Q不能很快的清零，可能是有应用向外发送数据包过快，或者是对方接收数据包不够快。</p>
<p>Recv-Q：表示收到的数据已经在本地接收缓冲，但是还有多少没有被进程取走，recv()<br>Send-Q：对方没有收到的数据或者说没有Ack的,还是本地缓冲区.<br>通过netstat的这两个值就可以简单判断程序收不到包到底是包没到还是包没有被进程recv。</p>
<p>SYN（synchronous）是TCP/IP建立连接时使用的握手信号。在客户机和服务器之间建立正常的TCP网络连接时，客户机首先发出一个SYN消息，服务器使用SYN+ACK应答表示接收到了这个消息，最后客户机再以ACK消息响应。这样在客户机和服务器之间才能建立起可靠的TCP连接，数据才可以在客户机和服务器之间传递。<br>TCP连接的第一个包，非常小的一种数据包。SYN 攻击包括大量此类的包，由于这些包看上去来自实际不存在的站点，因此无法有效进行处理。每个机器的欺骗包都要花几秒钟进行尝试方可放弃提供正常响应。</p>
<p><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">ddp-dn-032:~ # netstat -ntp</div><div class="line">Active Internet connections (w/o servers)</div><div class="line">Proto Recv-Q Send-Q Local Address           Foreign Address         State       PID/Program name   </div><div class="line">tcp        1      0 10.200.60.110:39524     10.200.60.79:22000      CLOSE_WAIT  3773/impalad        </div><div class="line">tcp        1      0 10.200.60.110:43629     10.200.60.93:22000      CLOSE_WAIT  3773/impalad        </div><div class="line">tcp        0      0 10.200.60.110:1004      10.200.63.26:36895      TIME_WAIT   -                   </div><div class="line">tcp        0      0 10.200.60.110:57353     10.200.63.39:1004       TIME_WAIT   -                   </div><div class="line">tcp        1      0 10.200.60.110:33582     10.200.63.25:22000      CLOSE_WAIT  3773/impalad        </div><div class="line">tcp        0      0 10.200.60.110:58218     10.200.60.78:1004       TIME_WAIT   -                   </div><div class="line">tcp        1      0 10.200.60.110:38065     10.200.63.22:22000      CLOSE_WAIT  3773/impalad        </div><div class="line">tcp        1      0 10.200.60.110:34440     10.200.60.11:22000      CLOSE_WAIT  3773/impalad        </div><div class="line">tcp        0      0 10.200.60.110:34530     10.200.60.100:8020      TIME_WAIT   -                   </div><div class="line">tcp        1      0 10.200.60.110:58388     10.200.60.67:22000      CLOSE_WAIT  3773/impalad        </div><div class="line">tcp        1      0 10.200.60.110:60931     10.200.60.58:22000      CLOSE_WAIT  3773/impalad        </div><div class="line">tcp        1      0 10.200.60.110:45359     10.200.60.42:22000      CLOSE_WAIT  3773/impalad        </div><div class="line">tcp        1      0 10.200.60.110:44558     10.200.60.125:22000     CLOSE_WAIT  3773/impalad        </div><div class="line">tcp        0      0 10.200.60.110:43614     10.200.60.45:1004       TIME_WAIT   -                   </div><div class="line">tcp        0      0 10.200.60.110:46176     10.200.60.107:8022      TIME_WAIT   -                   </div><div class="line">tcp        0      0 10.200.60.110:10050     10.200.56.126:33359     TIME_WAIT   -                   </div><div class="line">tcp        1      0 10.200.60.110:56717     10.200.60.30:22000      CLOSE_WAIT  3773/impalad        </div><div class="line">tcp        1      0 10.200.60.110:37897     10.200.60.139:22000     CLOSE_WAIT  3773/impalad        </div><div class="line">tcp        0      0 10.200.60.110:38204     10.200.60.38:52139      TIME_WAIT   -                   </div><div class="line">tcp        0      0 10.200.60.110:10050     10.200.56.126:55412     TIME_WAIT   -                   </div><div class="line">tcp        0      0 10.200.60.110:44500     10.200.63.33:1004       TIME_WAIT   -                   </div><div class="line">tcp        1      0 10.200.60.110:44914     10.200.60.66:22000      CLOSE_WAIT  3773/impalad        </div><div class="line">tcp        0      0 10.200.60.110:34546     10.200.60.100:8020      TIME_WAIT   -                   </div><div class="line">tcp        1      0 10.200.60.110:44192     10.200.63.97:22000      CLOSE_WAIT  3773/impalad        </div><div class="line">tcp        0      0 10.200.60.110:59643     10.200.60.60:45021      TIME_WAIT   -                   </div><div class="line">tcp        0      0 10.200.60.110:34069     10.200.60.110:1004      TIME_WAIT   -                   </div><div class="line">tcp        0      0 10.200.60.110:10050     10.200.56.126:58423     TIME_WAIT   -                   </div><div class="line">tcp        1      0 10.200.60.110:53904     10.200.63.87:22000      CLOSE_WAIT  3773/impalad        </div><div class="line">tcp        1      0 10.200.60.110:40765     10.200.60.106:24000     CLOSE_WAIT  3773/impalad        </div><div class="line">tcp        0      0 10.200.60.110:10050     10.200.56.126:34611     TIME_WAIT   -                   </div><div class="line">tcp        0      0 10.200.60.110:1004      10.200.60.108:42258     TIME_WAIT   -                   </div><div class="line">tcp        0      0 10.200.60.110:59639     10.200.60.60:45021      TIME_WAIT   -                   </div><div class="line">tcp        0      0 10.200.60.110:10050     10.200.56.126:52427     TIME_WAIT   -                   </div><div class="line">tcp        0      0 10.200.60.110:58581     10.200.60.79:1004       TIME_WAIT   -                   </div><div class="line">tcp        0      0 10.200.60.110:10050     10.200.56.126:49472     TIME_WAIT   -                   </div><div class="line">tcp        1      0 10.200.60.110:48228     10.200.63.37:22000      CLOSE_WAIT  3773/impalad        </div><div class="line">tcp        1      0 10.200.60.110:35413     10.200.60.81:22000      CLOSE_WAIT  3773/impalad        </div><div class="line">tcp        1      0 10.200.60.110:57373     10.200.60.68:22000      CLOSE_WAIT  3773/impalad        </div><div class="line">tcp        0      0 10.200.60.110:34074     10.200.60.110:1004      TIME_WAIT   -                   </div><div class="line">tcp        1      0 10.200.60.110:57849     10.200.60.92:22000      CLOSE_WAIT  3773/impalad        </div><div class="line">tcp        0      0 10.200.60.110:34567     10.200.60.100:8020      TIME_WAIT   -                   </div><div class="line">tcp        0      0 10.200.60.110:10050     10.200.56.126:50730     TIME_WAIT   -                   </div><div class="line">tcp        1      0 10.200.60.110:35099     10.200.63.86:22000      CLOSE_WAIT  3773/impalad        </div><div class="line">tcp        1      0 10.200.60.110:38220     10.200.60.26:22000      CLOSE_WAIT  3773/impalad        </div><div class="line">tcp        1      0 10.200.60.110:59716     10.200.63.29:22000      CLOSE_WAIT  3773/impalad        </div><div class="line">tcp        0      0 10.200.60.110:10050     10.200.56.126:54124     TIME_WAIT   -                   </div><div class="line">tcp        1      0 10.200.60.110:37479     10.200.60.141:22000     CLOSE_WAIT  3773/impalad        </div><div class="line">tcp        0      0 10.200.60.110:10050     10.200.56.126:41114     TIME_WAIT   -                   </div><div class="line">tcp        1      0 10.200.60.110:39161     10.200.60.95:22000      CLOSE_WAIT  3773/impalad        </div><div class="line">tcp        0      0 10.200.60.110:10050     10.200.56.126:48509     TIME_WAIT   -                   </div><div class="line">tcp        0      0 10.200.60.110:34554     10.200.60.100:8020      TIME_WAIT   -                   </div><div class="line">tcp        1      0 10.200.60.110:49090     10.200.63.90:22000      CLOSE_WAIT  3773/impalad        </div><div class="line">tcp        1      0 10.200.60.110:35952     10.200.60.119:22000     CLOSE_WAIT  3773/impalad        </div><div class="line">tcp        1      0 10.200.60.110:50717     10.200.63.96:22000      CLOSE_WAIT  3773/impalad        </div><div class="line">tcp        0      0 10.200.60.110:34565     10.200.60.100:8020      TIME_WAIT   -                   </div><div class="line">tcp        0      0 10.200.60.110:10050     10.200.56.126:41805     TIME_WAIT   -                   </div><div class="line">tcp        1      0 10.200.60.110:56712     10.200.60.73:22000      CLOSE_WAIT  3773/impalad        </div><div class="line">tcp        0      0 10.200.60.110:13562     10.200.63.15:45265      TIME_WAIT   -                   </div><div class="line">tcp        1      0 10.200.60.110:32921     10.200.60.85:22000      CLOSE_WAIT  3773/impalad        </div><div class="line">tcp        1      0 10.200.60.110:49914     10.200.60.44:22000      CLOSE_WAIT  3773/impalad        </div><div class="line">tcp        0      0 127.0.0.1:19001         127.0.0.1:48835         ESTABLISHED 4358/python         </div><div class="line">tcp        1      0 10.200.60.110:57788     10.200.60.41:22000      CLOSE_WAIT  3773/impalad        </div><div class="line">tcp        0      0 10.200.60.110:1004      10.200.60.110:33646     TIME_WAIT   -                   </div><div class="line">tcp        1      0 10.200.60.110:57086     10.200.60.122:22000     CLOSE_WAIT  3773/impalad        </div><div class="line">tcp        0      0 10.200.60.110:1004      10.200.60.18:47133      TIME_WAIT   -                   </div><div class="line">tcp        0      0 10.200.60.110:10050     10.200.56.126:38618     TIME_WAIT   -                   </div><div class="line">tcp        1      0 10.200.60.110:43323     10.200.60.104:22000     CLOSE_WAIT  3773/impalad        </div><div class="line">tcp        1      0 10.200.60.110:59139     10.200.63.92:22000      CLOSE_WAIT  3773/impalad        </div><div class="line">tcp        0      0 10.200.60.110:51328     10.200.60.34:1004       TIME_WAIT   -                   </div><div class="line">tcp        0      0 10.200.60.110:41141     10.200.63.32:1004       TIME_WAIT   -                   </div><div class="line">tcp        0      0 10.200.60.110:10050     10.200.56.126:51310     TIME_WAIT   -                   </div><div class="line">tcp        1      0 10.200.60.110:36547     10.200.63.33:22000      CLOSE_WAIT  3773/impalad        </div><div class="line">tcp        1      0 10.200.60.110:52474     10.200.60.49:22000      CLOSE_WAIT  3773/impalad        </div><div class="line">tcp        1      0 10.200.60.110:44185     10.200.60.65:22000      CLOSE_WAIT  3773/impalad        </div><div class="line">tcp        1      0 10.200.60.110:55500     10.200.63.82:22000      CLOSE_WAIT  3773/impalad        </div><div class="line">tcp        1      0 10.200.60.110:55282     10.200.63.99:22000      CLOSE_WAIT  3773/impalad        </div><div class="line">tcp        0      0 10.200.60.110:1004      10.200.60.110:33612     TIME_WAIT   -                   </div><div class="line">tcp        1      0 10.200.60.110:59901     10.200.60.142:22000     CLOSE_WAIT  3773/impalad        </div><div class="line">tcp        0      0 10.200.60.110:34541     10.200.60.100:8020      TIME_WAIT   -                   </div><div class="line">tcp        1      0 10.200.60.110:37878     10.200.63.39:22000      CLOSE_WAIT  3773/impalad        </div><div class="line">tcp        0      0 10.200.60.110:1006      10.200.60.110:53278     TIME_WAIT   -                   </div><div class="line">tcp        0      0 10.200.60.110:10050     10.200.56.126:56068     TIME_WAIT   -                   </div><div class="line">tcp        1      0 10.200.60.110:37815     10.200.60.36:22000      CLOSE_WAIT  3773/impalad        </div><div class="line">tcp        0      0 10.200.60.110:10050     10.200.56.126:56368     TIME_WAIT   -                   </div><div class="line">tcp        0      0 10.200.60.110:55441     10.200.60.114:7184      ESTABLISHED -                   </div><div class="line">tcp        1      0 10.200.60.110:36442     10.200.60.84:22000      CLOSE_WAIT  3773/impalad        </div><div class="line">tcp        1      0 10.200.60.110:39030     10.200.60.21:22000      CLOSE_WAIT  3773/impalad        </div><div class="line">tcp        1      0 10.200.60.110:53485     10.200.63.98:22000      CLOSE_WAIT  3773/impalad        </div><div class="line">tcp        0      0 10.200.60.110:59640     10.200.60.60:45021      TIME_WAIT   -                   </div><div class="line">tcp        1      0 10.200.60.110:51471     10.200.63.84:22000      CLOSE_WAIT  3773/impalad        </div><div class="line">tcp        0      0 10.200.60.110:35204     10.200.60.110:1004      TIME_WAIT   -                   </div><div class="line">tcp        0      0 10.200.60.110:1004      10.200.60.106:43806     TIME_WAIT   -                   </div><div class="line">tcp        0      0 10.200.60.110:46782     10.200.60.114:9997      ESTABLISHED -                   </div><div class="line">tcp        0      0 10.200.60.110:58886     10.200.60.35:1004       TIME_WAIT   -                   </div><div class="line">tcp        0      0 10.200.60.110:59650     10.200.60.60:45021      TIME_WAIT   -                   </div><div class="line">tcp        1      0 10.200.60.110:34547     10.200.60.101:22000     CLOSE_WAIT  3773/impalad        </div><div class="line">tcp        0      0 10.200.60.110:10050     10.200.56.126:33777     TIME_WAIT   -                   </div><div class="line">tcp        0      0 10.200.60.110:40089     10.200.60.110:22000     ESTABLISHED 3773/impalad        </div><div class="line">tcp        1      0 10.200.60.110:46225     10.200.60.131:22000     CLOSE_WAIT  3773/impalad        </div><div class="line">tcp        0      0 10.200.60.110:42449     10.200.60.39:22000      ESTABLISHED 3773/impalad        </div><div class="line">tcp        1      0 10.200.60.110:58714     10.200.60.57:22000      CLOSE_WAIT  3773/impalad        </div><div class="line">tcp        1      0 10.200.60.110:43433     10.200.63.38:22000      CLOSE_WAIT  3773/impalad        </div><div class="line">tcp        0      0 10.200.60.110:34523     10.200.60.100:8020      TIME_WAIT   -                   </div><div class="line">tcp        0      0 10.200.60.110:42977     10.200.60.22:1004       TIME_WAIT   -                   </div><div class="line">tcp        0      0 10.200.60.110:43551     10.200.60.45:1004       TIME_WAIT   -                   </div><div class="line">tcp        1      0 10.200.60.110:52667     10.200.60.31:22000      CLOSE_WAIT  3773/impalad        </div><div class="line">tcp        0      0 10.200.60.110:34574     10.200.60.100:8020      TIME_WAIT   -                   </div><div class="line">tcp        1      0 10.200.60.110:49041     10.200.63.93:22000      CLOSE_WAIT  3773/impalad        </div><div class="line">tcp        0      0 10.200.60.110:59649     10.200.60.60:45021      TIME_WAIT   -                   </div><div class="line">tcp        1      0 10.200.60.110:45658     10.200.60.132:22000     CLOSE_WAIT  3773/impalad        </div><div class="line">tcp        1      0 10.200.60.110:46681     10.200.60.10:22000      CLOSE_WAIT  3773/impalad        </div><div class="line">tcp        0      0 10.200.60.110:1004      10.200.60.33:58198      TIME_WAIT   -                   </div><div class="line">tcp        0      0 127.0.0.1:48678         127.0.0.1:38125         TIME_WAIT   -                   </div><div class="line">tcp        0      0 10.200.60.110:1004      10.200.60.110:33610     TIME_WAIT   -                   </div><div class="line">tcp        1      0 10.200.60.110:47087     10.200.60.17:22000      CLOSE_WAIT  3773/impalad        </div><div class="line">tcp        0      0 10.200.60.110:10050     10.200.56.126:55872     TIME_WAIT   -                   </div><div class="line">tcp        1      0 10.200.60.110:45731     10.200.60.89:22000      CLOSE_WAIT  3773/impalad        </div><div class="line">tcp        1      0 10.200.60.110:43394     10.200.60.33:22000      CLOSE_WAIT  3773/impalad        </div><div class="line">tcp        0      0 10.200.60.110:59645     10.200.60.60:45021      TIME_WAIT   -                   </div><div class="line">tcp        1      0 10.200.60.110:57161     10.200.63.26:22000      CLOSE_WAIT  3773/impalad        </div><div class="line">tcp        0      0 10.200.60.110:35181     10.200.60.100:8020      TIME_WAIT   -                   </div><div class="line">tcp        1      0 10.200.60.110:54587     10.200.60.63:22000      CLOSE_WAIT  3773/impalad        </div><div class="line">tcp        0      0 10.200.60.110:10050     10.200.56.126:58509     TIME_WAIT   -                   </div><div class="line">tcp        0      0 10.200.60.110:10050     10.200.56.126:36560     TIME_WAIT   -                   </div><div class="line">tcp        1      0 10.200.60.110:53854     10.200.60.24:22000      CLOSE_WAIT  3773/impalad        </div><div class="line">tcp        0      0 10.200.60.110:58085     10.200.60.39:1004       TIME_WAIT   -                   </div><div class="line">tcp        1      0 10.200.60.110:41370     10.200.60.80:22000      CLOSE_WAIT  3773/impalad        </div><div class="line">tcp        1      0 10.200.60.110:44716     10.200.63.31:22000      CLOSE_WAIT  3773/impalad        </div><div class="line">tcp        1      0 10.200.60.110:56643     10.200.60.27:22000      CLOSE_WAIT  3773/impalad        </div><div class="line">tcp        0      0 10.200.60.110:38664     10.200.60.93:1004       TIME_WAIT   -                   </div><div class="line">tcp        1      0 10.200.60.110:36117     10.200.60.54:22000      CLOSE_WAIT  3773/impalad        </div><div class="line">tcp        1      0 10.200.60.110:43341     10.200.60.38:22000      CLOSE_WAIT  3773/impalad        </div><div class="line">tcp        1      0 10.200.60.110:47925     10.200.60.32:22000      CLOSE_WAIT  3773/impalad        </div><div class="line">tcp        1      0 10.200.60.110:55668     10.200.60.91:22000      CLOSE_WAIT  3773/impalad        </div><div class="line">tcp        0      0 10.200.60.110:10050     10.200.56.126:50525     TIME_WAIT   -                   </div><div class="line">tcp        1      0 10.200.60.110:42549     10.200.60.19:22000      CLOSE_WAIT  3773/impalad        </div><div class="line">tcp        1      0 10.200.60.110:40820     10.200.60.62:22000      CLOSE_WAIT  3773/impalad        </div><div class="line">tcp        1      0 10.200.60.110:33385     10.200.60.127:22000     CLOSE_WAIT  3773/impalad        </div><div class="line">tcp        1      0 10.200.60.110:43579     10.200.60.46:22000      CLOSE_WAIT  3773/impalad        </div><div class="line">tcp        1      0 10.200.60.110:57880     10.200.60.115:22000     CLOSE_WAIT  3773/impalad        </div><div class="line">tcp        0      0 10.200.60.110:51313     10.200.60.34:1004       TIME_WAIT   -                   </div><div class="line">tcp        0      0 10.200.60.110:10050     10.200.56.126:34983     TIME_WAIT   -                   </div><div class="line">tcp        0      0 10.200.60.110:10050     10.200.56.126:32825     TIME_WAIT   -                   </div><div class="line">tcp        0      0 10.200.60.110:1004      10.200.60.11:52494      TIME_WAIT   -                   </div><div class="line">tcp        1      0 10.200.60.110:51739     10.200.60.52:22000      CLOSE_WAIT  3773/impalad        </div><div class="line">tcp        1      0 10.200.60.110:37688     10.200.60.20:22000      CLOSE_WAIT  3773/impalad        </div><div class="line">tcp        1      0 10.200.60.110:47758     10.200.63.88:22000      CLOSE_WAIT  3773/impalad        </div><div class="line">tcp        1      0 10.200.60.110:50720     10.200.60.103:22000     CLOSE_WAIT  3773/impalad        </div><div class="line">tcp        0      0 10.200.60.110:34634     10.200.60.100:8020      TIME_WAIT   -                   </div><div class="line">tcp        1      0 10.200.60.110:41560     10.200.60.87:22000      CLOSE_WAIT  3773/impalad        </div><div class="line">tcp        0      0 10.200.60.110:10050     10.200.56.126:52832     TIME_WAIT   -                   </div><div class="line">tcp        1      0 10.200.60.110:34798     10.200.60.18:22000      CLOSE_WAIT  3773/impalad        </div><div class="line">tcp        1      0 10.200.60.110:35880     10.200.60.88:22000      CLOSE_WAIT  3773/impalad        </div><div class="line">tcp        0      0 10.200.60.110:34636     10.200.60.100:8020      TIME_WAIT   -                   </div><div class="line">tcp        0      0 10.200.60.110:10050     10.200.56.126:57439     TIME_WAIT   -                   </div><div class="line">tcp        0      0 10.200.60.110:43615     10.200.60.45:1004       TIME_WAIT   -                   </div><div class="line">tcp        0      0 10.200.60.110:34457     10.200.60.100:8020      TIME_WAIT   -                   </div><div class="line">tcp        1      0 10.200.60.110:35774     10.200.60.51:22000      CLOSE_WAIT  3773/impalad        </div><div class="line">tcp        0      0 10.200.60.110:10050     10.200.56.126:38703     TIME_WAIT   -                   </div><div class="line">tcp        1      0 10.200.60.110:42735     10.200.60.75:22000      CLOSE_WAIT  3773/impalad        </div><div class="line">tcp        1      0 10.200.60.110:40540     10.200.60.120:22000     CLOSE_WAIT  3773/impalad        </div><div class="line">tcp        0      0 10.200.60.110:33758     10.200.63.15:60474      TIME_WAIT   -                   </div><div class="line">tcp        0      0 10.200.60.110:34428     10.200.60.100:8020      TIME_WAIT   -                   </div><div class="line">tcp        0      0 10.200.60.110:34452     10.200.60.100:8020      TIME_WAIT   -                   </div><div class="line">tcp        1      0 10.200.60.110:59235     10.200.60.138:22000     CLOSE_WAIT  3773/impalad        </div><div class="line">tcp        0      0 10.200.60.110:1004      10.200.60.110:33618     TIME_WAIT   -                   </div><div class="line">tcp        1      0 10.200.60.110:60847     10.200.60.37:22000      CLOSE_WAIT  3773/impalad        </div><div class="line">tcp        0      0 10.200.60.110:41148     10.200.63.32:1004       TIME_WAIT   -                   </div><div class="line">tcp        1      0 10.200.60.110:52696     10.200.63.23:22000      CLOSE_WAIT  3773/impalad        </div><div class="line">tcp        0      0 10.200.60.110:10050     10.200.56.126:49919     TIME_WAIT   -                   </div><div class="line">tcp        0      0 10.200.60.110:10050     10.200.56.126:35344     TIME_WAIT   -                   </div><div class="line">tcp        0      0 10.200.60.110:10050     10.200.56.126:35852     TIME_WAIT   -                   </div><div class="line">tcp        1      0 10.200.60.110:43472     10.200.60.64:22000      CLOSE_WAIT  3773/impalad        </div><div class="line">tcp        1      0 10.200.60.110:58358     10.200.63.34:22000      CLOSE_WAIT  3773/impalad        </div><div class="line">tcp        1      0 10.200.60.110:51110     10.200.60.135:22000     CLOSE_WAIT  3773/impalad        </div><div class="line">tcp        1      0 10.200.60.110:59746     10.200.60.126:22000     CLOSE_WAIT  3773/impalad        </div><div class="line">tcp        0      0 10.200.60.110:35179     10.200.60.110:1004      TIME_WAIT   -                   </div><div class="line">tcp        0      0 10.200.60.110:46546     10.200.60.120:42316     TIME_WAIT   -                   </div><div class="line">tcp        0      0 10.200.60.110:47625     10.200.60.38:1004       TIME_WAIT   -                   </div><div class="line">tcp        1      0 10.200.60.110:50478     10.200.60.43:22000      CLOSE_WAIT  3773/impalad        </div><div class="line">tcp        1      0 10.200.60.110:54797     10.200.60.94:22000      CLOSE_WAIT  3773/impalad        </div><div class="line">tcp        0      0 10.200.60.110:59638     10.200.60.60:45021      TIME_WAIT   -                   </div><div class="line">tcp        1      0 10.200.60.110:39141     10.200.60.45:22000      CLOSE_WAIT  3773/impalad        </div><div class="line">tcp        0      0 10.200.60.110:10050     10.200.56.126:39467     TIME_WAIT   -                   </div><div class="line">tcp        0      0 10.200.60.110:47234     10.200.60.32:1004       TIME_WAIT   -                   </div><div class="line">tcp        1      0 10.200.60.110:46087     10.200.60.121:22000     CLOSE_WAIT  3773/impalad        </div><div class="line">tcp        0      0 10.200.60.110:59641     10.200.60.60:45021      TIME_WAIT   -                   </div><div class="line">tcp        1      0 10.200.60.110:44411     10.200.60.76:22000      CLOSE_WAIT  3773/impalad        </div><div class="line">tcp        1      0 10.200.60.110:59982     10.200.60.47:22000      CLOSE_WAIT  3773/impalad        </div><div class="line">tcp        1      0 10.200.60.110:41529     10.200.63.40:22000      CLOSE_WAIT  3773/impalad        </div><div class="line">tcp        0      0 10.200.60.110:1004      10.200.60.110:33611     TIME_WAIT   -                   </div><div class="line">tcp        1      0 10.200.60.110:55201     10.200.60.83:22000      CLOSE_WAIT  3773/impalad        </div><div class="line">tcp        1      0 10.200.60.110:56656     10.200.63.28:22000      CLOSE_WAIT  3773/impalad        </div><div class="line">tcp        0      0 10.200.60.110:35857     10.200.60.100:8031      ESTABLISHED 25756/java          </div><div class="line">tcp        0      0 10.200.60.110:44367     10.200.63.33:1004       TIME_WAIT   -                   </div><div class="line">tcp        0      0 10.200.60.110:35038     10.200.60.110:1004      TIME_WAIT   -                   </div><div class="line">tcp        0      0 10.200.60.110:58129     10.200.60.100:8022      ESTABLISHED 17586/jsvc.exec     </div><div class="line">tcp        0      0 10.200.60.110:1004      10.200.60.114:45135     TIME_WAIT   -                   </div><div class="line">tcp        0      0 10.200.60.110:47338     10.200.60.36:1004       TIME_WAIT   -                   </div><div class="line">tcp        1      0 10.200.60.110:36706     10.200.60.116:22000     CLOSE_WAIT  3773/impalad        </div><div class="line">tcp        1      0 10.200.60.110:52908     10.200.60.143:22000     CLOSE_WAIT  3773/impalad        </div><div class="line">tcp        1      0 10.200.60.110:46523     10.200.60.22:22000      CLOSE_WAIT  3773/impalad        </div><div class="line">tcp        0      0 10.200.60.110:10050     10.200.56.126:36894     TIME_WAIT   -                   </div><div class="line">tcp        0   1600 10.200.60.110:22        172.16.15.152:45994     ESTABLISHED 6613/0              </div><div class="line">tcp        1      0 10.200.60.110:46552     10.200.60.55:22000      CLOSE_WAIT  3773/impalad        </div><div class="line">tcp        1      0 10.200.60.110:55737     10.200.60.53:22000      CLOSE_WAIT  3773/impalad        </div><div class="line">tcp        1      0 10.200.60.110:55624     10.200.60.48:22000      CLOSE_WAIT  3773/impalad        </div><div class="line">tcp        1      0 10.200.60.110:40554     10.200.60.23:22000      CLOSE_WAIT  3773/impalad        </div><div class="line">tcp        0      0 10.200.60.110:34570     10.200.60.100:8020      TIME_WAIT   -                   </div><div class="line">tcp        1      0 10.200.60.110:33616     10.200.60.61:22000      CLOSE_WAIT  3773/impalad        </div><div class="line">tcp        0      0 10.200.60.110:34512     10.200.60.100:8020      TIME_WAIT   -                   </div><div class="line">tcp        0      0 10.200.60.110:10050     10.200.56.126:60560     TIME_WAIT   -                   </div><div class="line">tcp        1      0 10.200.60.110:56346     10.200.60.140:22000     CLOSE_WAIT  3773/impalad        </div><div class="line">tcp        1      0 10.200.60.110:35952     10.200.63.85:22000      CLOSE_WAIT  3773/impalad        </div><div class="line">tcp        1      0 10.200.60.110:58899     10.200.60.40:22000      CLOSE_WAIT  3773/impalad        </div><div class="line">tcp        1      0 10.200.60.110:42605     10.200.60.34:22000      CLOSE_WAIT  3773/impalad        </div><div class="line">tcp        1      0 10.200.60.110:48608     10.200.60.137:22000     CLOSE_WAIT  3773/impalad        </div><div class="line">tcp        1      0 10.200.60.110:46823     10.200.60.72:22000      CLOSE_WAIT  3773/impalad        </div><div class="line">tcp        0      0 10.200.60.110:52516     10.200.60.68:1004       TIME_WAIT   -                   </div><div class="line">tcp        1      0 10.200.60.110:58748     10.200.63.35:22000      CLOSE_WAIT  3773/impalad        </div><div class="line">tcp        1      0 10.200.60.110:34263     10.200.60.29:22000      CLOSE_WAIT  3773/impalad        </div><div class="line">tcp        0      0 10.200.60.110:35426     10.200.60.114:7182      ESTABLISHED -                   </div><div class="line">tcp        0      0 10.200.60.110:1004      10.200.60.10:60425      TIME_WAIT   -                   </div><div class="line">tcp        1      0 10.200.60.110:52262     10.200.60.96:22000      CLOSE_WAIT  3773/impalad        </div><div class="line">tcp        1      0 10.200.60.110:39774     10.200.63.27:22000      CLOSE_WAIT  3773/impalad        </div><div class="line">tcp        1      0 10.200.60.110:52287     10.200.60.78:22000      CLOSE_WAIT  3773/impalad        </div><div class="line">tcp        0      0 10.200.60.110:1004      10.200.60.109:34509     TIME_WAIT   -                   </div><div class="line">tcp        0      0 10.200.60.110:34431     10.200.60.110:1004      TIME_WAIT   -                   </div><div class="line">tcp        0      0 10.200.60.110:10050     10.200.56.126:58934     TIME_WAIT   -                   </div><div class="line">tcp        1      0 10.200.60.110:39446     10.200.63.91:22000      CLOSE_WAIT  3773/impalad        </div><div class="line">tcp        0      0 10.200.60.110:34519     10.200.60.100:8020      TIME_WAIT   -                   </div><div class="line">tcp        0      0 10.200.60.110:34524     10.200.60.100:8020      TIME_WAIT   -                   </div><div class="line">tcp        0      0 10.200.60.110:53548     10.200.60.107:8022      ESTABLISHED 17586/jsvc.exec     </div><div class="line">tcp        0      0 10.200.60.110:58702     10.200.60.79:1004       TIME_WAIT   -                   </div><div class="line">tcp        1      0 10.200.60.110:46409     10.200.63.21:22000      CLOSE_WAIT  3773/impalad        </div><div class="line">tcp        0      0 10.200.60.110:10050     10.200.56.126:53868     TIME_WAIT   -                   </div><div class="line">tcp        0      0 10.200.60.110:59636     10.200.60.60:45021      TIME_WAIT   -                   </div><div class="line">tcp        1      0 10.200.60.110:55013     10.200.60.133:22000     CLOSE_WAIT  3773/impalad        </div><div class="line">tcp        0      0 10.200.60.110:10050     10.200.56.126:37623     TIME_WAIT   -                   </div><div class="line">tcp        1      0 10.200.60.110:42372     10.200.60.71:22000      CLOSE_WAIT  3773/impalad        </div><div class="line">tcp        0      0 10.200.60.110:10050     10.200.56.126:48924     TIME_WAIT   -                   </div><div class="line">tcp        0      0 10.200.60.110:1004      10.200.60.47:35044      TIME_WAIT   -                   </div><div class="line">tcp        0      0 10.200.60.110:58038     10.200.60.39:1004       TIME_WAIT   -                   </div><div class="line">tcp        1      0 10.200.60.110:43450     10.200.63.36:22000      CLOSE_WAIT  3773/impalad        </div><div class="line">tcp        1      0 10.200.60.110:42540     10.200.63.100:22000     CLOSE_WAIT  3773/impalad        </div><div class="line">tcp        1      0 10.200.60.110:49093     10.200.60.130:22000     CLOSE_WAIT  3773/impalad        </div><div class="line">tcp        1      0 10.200.60.110:38290     10.200.60.70:22000      CLOSE_WAIT  3773/impalad        </div><div class="line">tcp        0      0 10.200.60.110:40222     10.200.60.114:9995      ESTABLISHED -                   </div><div class="line">tcp        0      0 127.0.0.1:48678         127.0.0.1:38123         TIME_WAIT   -                   </div><div class="line">tcp        0      0 10.200.60.110:34817     10.200.60.100:8020      TIME_WAIT   -                   </div><div class="line">tcp        0      0 10.200.60.110:10050     10.200.56.126:57345     TIME_WAIT   -                   </div><div class="line">tcp        1      0 10.200.60.110:60163     10.200.63.99:22000      CLOSE_WAIT  3773/impalad        </div><div class="line">tcp        1      0 10.200.60.110:57777     10.200.60.136:22000     CLOSE_WAIT  3773/impalad        </div><div class="line">tcp        1      0 10.200.60.110:48268     10.200.60.134:22000     CLOSE_WAIT  3773/impalad        </div><div class="line">tcp        1      0 10.200.60.110:36046     10.200.60.90:22000      CLOSE_WAIT  3773/impalad        </div><div class="line">tcp        0      0 10.200.60.110:10050     10.200.56.126:37334     TIME_WAIT   -                   </div><div class="line">tcp        0      0 10.200.60.110:10050     10.200.56.126:35844     TIME_WAIT   -                   </div><div class="line">tcp        0      0 10.200.60.110:10050     10.200.56.126:56651     TIME_WAIT   -                   </div><div class="line">tcp        0      0 10.200.60.110:59652     10.200.60.60:45021      TIME_WAIT   -                   </div><div class="line">tcp        0      0 10.200.60.110:10050     10.200.56.126:53482     TIME_WAIT   -                   </div><div class="line">tcp        0      0 10.200.60.110:10050     10.200.56.126:59656     TIME_WAIT   -                   </div><div class="line">tcp        0      0 10.200.60.110:59651     10.200.60.60:45021      TIME_WAIT   -                   </div><div class="line">tcp        0      0 10.200.60.110:43580     10.200.60.45:1004       TIME_WAIT   -                   </div><div class="line">tcp        1      0 10.200.60.110:49601     10.200.60.13:22000      CLOSE_WAIT  3773/impalad        </div><div class="line">tcp        0      0 10.200.60.110:35245     10.200.60.110:1004      TIME_WAIT   -                   </div><div class="line">tcp        1      0 10.200.60.110:59712     10.200.60.69:22000      CLOSE_WAIT  3773/impalad        </div><div class="line">tcp        0      0 10.200.60.110:10050     10.200.56.126:59892     TIME_WAIT   -                   </div><div class="line">tcp        0      0 10.200.60.110:45377     10.200.63.27:1004       TIME_WAIT   -                   </div><div class="line">tcp        0      0 10.200.60.110:59646     10.200.60.60:45021      TIME_WAIT   -                   </div><div class="line">tcp        1      0 10.200.60.110:39956     10.200.60.25:22000      CLOSE_WAIT  3773/impalad        </div><div class="line">tcp        0      0 10.200.60.110:10050     10.200.56.126:57575     TIME_WAIT   -                   </div><div class="line">tcp        1      0 10.200.60.110:50241     10.200.63.94:22000      CLOSE_WAIT  3773/impalad        </div><div class="line">tcp        1      0 10.200.60.110:39157     10.200.60.74:22000      CLOSE_WAIT  3773/impalad        </div><div class="line">tcp        0      0 10.200.60.110:10050     10.200.56.126:54917     TIME_WAIT   -                   </div><div class="line">tcp        0      0 10.200.60.110:60532     10.200.60.110:25000     ESTABLISHED 10961/python2.6     </div><div class="line">tcp        1      0 10.200.60.110:47873     10.200.60.97:22000      CLOSE_WAIT  3773/impalad        </div><div class="line">tcp        1      0 10.200.60.110:33719     10.200.60.56:22000      CLOSE_WAIT  3773/impalad        </div><div class="line">tcp        0      0 10.200.60.110:47678     10.200.60.38:1004       TIME_WAIT   -                   </div><div class="line">tcp        1      0 10.200.60.110:35614     10.200.60.15:22000      CLOSE_WAIT  3773/impalad        </div><div class="line">tcp        1      0 10.200.60.110:54145     10.200.63.10:22000      CLOSE_WAIT  3773/impalad        </div><div class="line">tcp        0      0 10.200.60.110:60122     10.200.60.114:7184      ESTABLISHED -                   </div><div class="line">tcp        0      0 10.200.60.110:34520     10.200.60.100:8020      TIME_WAIT   -                   </div><div class="line">tcp        0      0 10.200.60.110:38596     10.200.60.93:1004       TIME_WAIT   -                   </div><div class="line">tcp      145      0 10.200.60.110:25000     10.200.60.110:60532     ESTABLISHED -                   </div><div class="line">tcp        1      0 10.200.60.110:48614     10.200.63.95:22000      CLOSE_WAIT  3773/impalad        </div><div class="line">tcp        0      0 10.200.60.110:34422     10.200.60.110:1004      TIME_WAIT   -                   </div><div class="line">tcp        0      0 10.200.60.110:1004      10.200.60.109:35923     ESTABLISHED 17586/jsvc.exec     </div><div class="line">tcp        0      0 10.200.60.110:38570     10.200.60.93:1004       TIME_WAIT   -                   </div><div class="line">tcp        0      0 10.200.60.110:33691     10.200.60.110:1004      TIME_WAIT   -                   </div><div class="line">tcp        1      0 10.200.60.110:59731     10.200.63.32:22000      CLOSE_WAIT  3773/impalad        </div><div class="line">tcp        0      0 10.200.60.110:59653     10.200.60.60:45021      TIME_WAIT   -                   </div><div class="line">tcp        0      0 10.200.60.110:54115     10.200.60.104:1004      TIME_WAIT   -                   </div><div class="line">tcp        0      0 10.200.60.110:44487     10.200.63.33:1004       TIME_WAIT   -                   </div><div class="line">tcp        0      0 10.200.60.110:59648     10.200.60.60:45021      TIME_WAIT   -                   </div><div class="line">tcp        0      0 10.200.60.110:1004      10.200.60.109:33239     TIME_WAIT   -                   </div><div class="line">tcp        0      0 10.200.60.110:46379     10.200.60.66:1004       TIME_WAIT   -                   </div><div class="line">tcp        0      0 10.200.60.110:59642     10.200.60.60:45021      TIME_WAIT   -                   </div><div class="line">tcp        0      0 10.200.60.110:10050     10.200.56.126:55158     TIME_WAIT   -                   </div><div class="line">tcp        0      0 10.200.60.110:34529     10.200.60.100:8020      TIME_WAIT   -                   </div><div class="line">tcp        0      0 10.200.60.110:8042      10.200.60.110:38454     TIME_WAIT   -                   </div><div class="line">tcp        0      0 10.200.60.110:10050     10.200.56.126:51789     TIME_WAIT   -                   </div><div class="line">tcp        1      0 10.200.60.110:51095     10.200.60.28:22000      CLOSE_WAIT  3773/impalad        </div><div class="line">tcp        0      0 10.200.60.110:34672     10.200.60.100:8020      TIME_WAIT   -                   </div><div class="line">tcp        0      0 10.200.60.110:34538     10.200.60.100:8020      TIME_WAIT   -                   </div><div class="line">tcp        1      0 10.200.60.110:50437     10.200.60.14:22000      CLOSE_WAIT  3773/impalad        </div><div class="line">tcp        1      0 10.200.60.110:42807     10.200.60.77:22000      CLOSE_WAIT  3773/impalad        </div><div class="line">tcp        0      0 10.200.60.110:38546     10.200.60.93:1004       TIME_WAIT   -                   </div><div class="line">tcp        0      0 10.200.60.110:59637     10.200.60.60:45021      TIME_WAIT   -                   </div><div class="line">tcp        1      0 10.200.60.110:39137     10.200.63.83:22000      CLOSE_WAIT  3773/impalad        </div><div class="line">tcp        0      0 10.200.60.110:10050     10.200.56.126:33081     TIME_WAIT   -                   </div><div class="line">tcp        1      0 10.200.60.110:56981     10.200.60.128:22000     CLOSE_WAIT  3773/impalad        </div><div class="line">tcp        0      0 10.200.60.110:34544     10.200.60.100:8020      TIME_WAIT   -                   </div><div class="line">tcp        1      0 10.200.60.110:37497     10.200.63.20:22000      CLOSE_WAIT  3773/impalad        </div><div class="line">tcp        0      0 127.0.0.1:48835         127.0.0.1:19001         ESTABLISHED 10961/python2.6     </div><div class="line">tcp        1      0 10.200.60.110:54544     10.200.60.50:22000      CLOSE_WAIT  3773/impalad        </div><div class="line">tcp        0      0 10.200.60.110:13562     10.200.63.15:45297      TIME_WAIT   -                   </div><div class="line">tcp        0      0 10.200.60.110:10050     10.200.56.126:55794     TIME_WAIT   -                   </div><div class="line">tcp        1      0 10.200.60.110:34069     10.200.60.108:22000     CLOSE_WAIT  3773/impalad        </div><div class="line">tcp        1      0 10.200.60.110:38488     10.200.60.109:22000     CLOSE_WAIT  3773/impalad        </div><div class="line">tcp        1      0 10.200.60.110:39349     10.200.60.117:22000     CLOSE_WAIT  3773/impalad        </div><div class="line">tcp        0      0 10.200.60.110:38059     10.200.60.100:8020      TIME_WAIT   -                   </div><div class="line">tcp        1      0 10.200.60.110:40902     10.200.60.102:22000     CLOSE_WAIT  3773/impalad        </div><div class="line">tcp        0      0 10.200.60.110:59644     10.200.60.60:45021      TIME_WAIT   -                   </div><div class="line">tcp        1      0 10.200.60.110:46899     10.200.60.35:22000      CLOSE_WAIT  3773/impalad        </div><div class="line">tcp        1      0 10.200.60.110:45439     10.200.63.30:22000      CLOSE_WAIT  3773/impalad        </div><div class="line">tcp        1      0 10.200.60.110:58067     10.200.60.12:22000      CLOSE_WAIT  3773/impalad        </div><div class="line">tcp        1      0 10.200.60.110:50828     10.200.60.60:22000      CLOSE_WAIT  3773/impalad        </div><div class="line">tcp        0      0 10.200.60.110:34498     10.200.60.100:8020      TIME_WAIT   -                   </div><div class="line">tcp        0      0 10.200.60.110:35053     10.200.60.110:1004      TIME_WAIT   -                   </div><div class="line">tcp        1      0 10.200.60.110:38862     10.200.60.118:22000     CLOSE_WAIT  3773/impalad        </div><div class="line">tcp        1      0 10.200.60.110:51123     10.200.60.82:22000      CLOSE_WAIT  3773/impalad        </div><div class="line">tcp        1      0 10.200.60.110:38239     10.200.60.59:22000      CLOSE_WAIT  3773/impalad        </div><div class="line">tcp        0      0 10.200.60.110:51203     10.200.60.130:50663     ESTABLISHED -                   </div><div class="line">tcp        0      0 10.200.60.110:43657     10.200.60.110:8040      TIME_WAIT   -                   </div><div class="line">tcp        0      0 10.200.60.110:48673     10.200.60.10:45054      ESTABLISHED -                   </div><div class="line">tcp        0      0 10.200.60.110:59011     10.200.60.110:43945     ESTABLISHED -                   </div><div class="line">tcp        0      0 10.200.60.110:43628     10.200.60.110:8040      TIME_WAIT   -                   </div><div class="line">tcp        0      0 10.200.60.110:39392     10.200.60.135:43112     ESTABLISHED -                   </div><div class="line">tcp        0      0 10.200.60.110:57126     10.200.60.104:39564     ESTABLISHED 16089/java          </div><div class="line">tcp        0      0 10.200.60.110:34343     10.200.60.108:38875     ESTABLISHED -                   </div><div class="line">tcp        0      0 10.200.60.110:59011     10.200.60.13:35105      ESTABLISHED 29726/java          </div><div class="line">tcp        0      0 10.200.60.110:59011     10.200.60.120:33405     ESTABLISHED -                   </div><div class="line">tcp        0      0 10.200.60.110:43627     10.200.60.110:8040      TIME_WAIT   -                   </div><div class="line">tcp        0      0 10.200.60.110:53577     10.200.63.146:36796     ESTABLISHED -                   </div><div class="line">tcp        0      0 10.200.60.110:34343     10.200.60.94:48726      ESTABLISHED -                   </div><div class="line">tcp        0      0 10.200.60.110:48251     10.200.60.17:39091      ESTABLISHED -                   </div><div class="line">tcp        0      0 10.200.60.110:35619     10.200.60.52:1004       TIME_WAIT   -                   </div><div class="line">tcp        0      0 10.200.60.110:52264     10.200.60.126:1004      TIME_WAIT   -                   </div><div class="line">tcp        0      0 10.200.60.110:53413     10.200.60.77:1004       TIME_WAIT   -                   </div><div class="line">tcp        0      0 10.200.60.110:39592     10.200.60.18:39020      ESTABLISHED -                   </div><div class="line">tcp        0      0 10.200.60.110:47232     10.200.60.36:1004       TIME_WAIT   -                   </div><div class="line">tcp        0      0 10.200.60.110:43957     10.200.60.110:8040      TIME_WAIT   -                   </div><div class="line">tcp        0      0 10.200.60.110:59096     10.200.60.88:1004       TIME_WAIT   -                   </div><div class="line">tcp        0      0 10.200.60.110:34343     10.200.60.134:50254     ESTABLISHED -                   </div><div class="line">tcp        0      0 10.200.60.110:48966     10.200.60.120:56307     ESTABLISHED -                   </div><div class="line">tcp        0      0 10.200.60.110:59011     10.200.60.120:54619     ESTABLISHED 29726/java          </div><div class="line">tcp        0      0 10.200.60.110:59011     10.200.60.93:34037      ESTABLISHED -                   </div><div class="line">tcp        0      0 10.200.60.110:53105     10.200.60.110:34343     ESTABLISHED -                   </div><div class="line">tcp        0      0 10.200.60.110:46447     10.200.60.135:60200     ESTABLISHED -                   </div><div class="line">tcp        0      0 10.200.60.110:59011     10.200.60.18:49100      ESTABLISHED -                   </div><div class="line">tcp        0      0 10.200.60.110:34465     10.200.60.100:8020      TIME_WAIT   -                   </div><div class="line">tcp        0      0 10.200.60.110:59011     10.200.60.88:36447      ESTABLISHED -                   </div><div class="line">tcp        0      0 10.200.60.110:34343     10.200.60.102:44567     ESTABLISHED 16089/java          </div><div class="line">tcp        0      0 10.200.60.110:34888     10.200.60.13:33395      ESTABLISHED -                   </div><div class="line">tcp        0      0 10.200.60.110:33010     10.200.60.91:40437      ESTABLISHED -                   </div><div class="line">tcp        0      0 10.200.60.110:55799     10.200.63.146:28953     ESTABLISHED -                   </div><div class="line">tcp        0      0 10.200.60.110:59011     10.200.60.17:38818      ESTABLISHED 29726/java          </div><div class="line">tcp        0      0 10.200.60.110:34343     10.200.60.93:52154      ESTABLISHED -                   </div><div class="line">tcp        0      0 10.200.60.110:59011     10.200.60.17:57011      ESTABLISHED -                   </div><div class="line">tcp        0      0 10.200.60.110:42664     10.200.60.132:1004      TIME_WAIT   -                   </div><div class="line">tcp        0      0 10.200.60.110:43697     10.200.60.132:51099     ESTABLISHED -                   </div><div class="line">tcp        0      0 10.200.60.110:34343     10.200.63.33:35719      ESTABLISHED 16089/java          </div><div class="line">tcp        0      0 10.200.60.110:55784     10.200.60.81:1004       TIME_WAIT   -                   </div><div class="line">tcp        0      0 10.200.60.110:42854     10.200.60.54:1004       TIME_WAIT   -                   </div><div class="line">tcp        0      0 10.200.60.110:36582     10.200.60.69:1004       TIME_WAIT   -                   </div><div class="line">tcp        0      0 10.200.60.110:45759     10.200.60.18:36351      ESTABLISHED -                   </div><div class="line">tcp        0      0 10.200.60.110:59011     10.200.60.134:51566     ESTABLISHED -                   </div><div class="line">tcp        0      0 10.200.60.110:34343     10.200.60.95:41489      ESTABLISHED 16089/java          </div><div class="line">tcp        0      0 10.200.60.110:59011     10.200.60.102:49566     ESTABLISHED -                   </div><div class="line">tcp        0      0 10.200.60.110:43647     10.200.60.110:8040      TIME_WAIT   -                   </div><div class="line">tcp        0      0 10.200.60.110:34343     10.200.60.13:53288      ESTABLISHED -                   </div><div class="line">tcp        0      0 10.200.60.110:34343     10.200.60.132:60655     ESTABLISHED -                   </div><div class="line">tcp        0      0 10.200.60.110:34343     10.200.60.18:52145      ESTABLISHED -                   </div><div class="line">tcp        0      0 10.200.60.110:50904     10.200.60.17:59411      ESTABLISHED -                   </div><div class="line">tcp        0      0 10.200.60.110:56594     10.200.60.102:1004      TIME_WAIT   -                   </div><div class="line">tcp        0      0 10.200.60.110:43661     10.200.60.110:8040      TIME_WAIT   -                   </div><div class="line">tcp        0      0 10.200.60.110:38734     10.200.60.116:1004      TIME_WAIT   -                   </div><div class="line">tcp        0      0 10.200.60.110:34343     10.200.60.13:33424      ESTABLISHED 16089/java          </div><div class="line">tcp        0      0 10.200.60.110:42346     10.200.60.108:33625     ESTABLISHED -                   </div><div class="line">tcp        0      0 10.200.60.110:53142     10.200.60.20:33749      ESTABLISHED 29726/java          </div><div class="line">tcp        0      0 10.200.60.110:43055     10.200.60.17:59411      ESTABLISHED -                   </div><div class="line">tcp        0      0 10.200.60.110:33356     10.200.60.21:50118      ESTABLISHED -                   </div><div class="line">tcp        0      0 10.200.60.110:56850     10.200.60.135:60200     ESTABLISHED -                   </div><div class="line">tcp        0      0 10.200.60.110:34343     10.200.60.91:60345      ESTABLISHED -                   </div><div class="line">tcp        0      0 10.200.60.110:59011     10.200.60.133:36635     ESTABLISHED -                   </div><div class="line">tcp        0      0 10.200.60.110:43626     10.200.60.110:8040      TIME_WAIT   -                   </div><div class="line">tcp        0      0 10.200.60.110:34343     10.200.60.120:57623     ESTABLISHED -                   </div><div class="line">tcp        0      0 10.200.60.110:41023     10.200.60.94:59659      ESTABLISHED -                   </div><div class="line">tcp        0      0 10.200.60.110:59011     10.200.60.93:58054      ESTABLISHED -                   </div><div class="line">tcp        0      0 10.200.60.110:60275     10.200.60.18:38386      ESTABLISHED 16089/java          </div><div class="line">tcp        0      0 10.200.60.110:34491     10.200.60.100:8020      TIME_WAIT   -                   </div><div class="line">tcp        0      0 10.200.60.110:59011     10.200.60.13:57803      ESTABLISHED -                   </div><div class="line">tcp        0      0 10.200.60.110:49110     10.200.60.88:55412      ESTABLISHED 29726/java          </div><div class="line">tcp        0      0 10.200.60.110:47284     10.200.60.67:1004       TIME_WAIT   -                   </div><div class="line">tcp        0      0 10.200.60.110:45771     10.200.60.14:58840      ESTABLISHED -                   </div><div class="line">tcp        0      0 10.200.60.110:43640     10.200.60.110:8040      TIME_WAIT   -                   </div><div class="line">tcp        0      0 10.200.60.110:59371     10.200.60.13:56295      ESTABLISHED 16089/java          </div><div class="line">tcp        0      0 10.200.60.110:56773     10.200.60.138:43354     ESTABLISHED 16089/java          </div><div class="line">tcp        0      0 10.200.60.110:59011     10.200.60.89:48962      ESTABLISHED -                   </div><div class="line">tcp        0      0 10.200.60.110:39074     10.200.60.93:60854      ESTABLISHED -                   </div><div class="line">tcp        0      0 10.200.60.110:43659     10.200.60.110:8040      TIME_WAIT   -                   </div><div class="line">tcp        0      0 10.200.60.110:59011     10.200.60.104:59050     ESTABLISHED 29726/java          </div><div class="line">tcp        0      0 10.200.60.110:38973     10.200.60.21:1004       TIME_WAIT   -                   </div><div class="line">tcp        0      0 10.200.60.110:53836     10.200.60.13:1004       TIME_WAIT   -                   </div><div class="line">tcp        0      0 10.200.60.110:41357     10.200.60.120:58005     ESTABLISHED 16089/java          </div><div class="line">tcp        0      0 10.200.60.110:59011     10.200.60.130:58637     ESTABLISHED -                   </div><div class="line">tcp        0      0 10.200.60.110:34343     10.200.60.138:51915     ESTABLISHED 16089/java          </div><div class="line">tcp        0      0 10.200.60.110:48811     10.200.60.133:54288     ESTABLISHED -                   </div><div class="line">tcp        0      0 10.200.60.110:46585     10.200.60.10:45054      ESTABLISHED -                   </div><div class="line">tcp        0      0 10.200.60.110:45229     10.200.60.120:58005     ESTABLISHED 29726/java          </div><div class="line">tcp        0      0 10.200.60.110:46190     10.200.60.88:39875      ESTABLISHED -                   </div><div class="line">tcp        0      0 10.200.60.110:34737     10.200.60.13:33395      ESTABLISHED -                   </div><div class="line">tcp        0      0 10.200.60.110:48637     10.200.60.95:59369      ESTABLISHED 16089/java          </div><div class="line">tcp        0      0 10.200.60.110:34343     10.200.60.10:46774      ESTABLISHED -                   </div><div class="line">tcp        0      0 10.200.60.110:40263     10.200.60.80:1004       TIME_WAIT   -                   </div><div class="line">tcp        0      0 10.200.60.110:34343     10.200.60.88:33716      ESTABLISHED 16089/java          </div><div class="line">tcp        0      0 10.200.60.110:59011     10.200.60.135:34287     ESTABLISHED -                   </div><div class="line">tcp        0      0 10.200.60.110:34343     10.200.60.17:54158      ESTABLISHED -                   </div><div class="line">tcp        0      0 10.200.60.110:40143     10.200.60.46:1004       TIME_WAIT   -                   </div><div class="line">tcp        0      0 10.200.60.110:40287     10.200.60.128:1004      TIME_WAIT   -                   </div><div class="line">tcp        0      0 10.200.60.110:59011     10.200.60.20:36625      ESTABLISHED 29726/java          </div><div class="line">tcp        0      0 10.200.60.110:34343     10.200.60.18:36782      ESTABLISHED 16089/java          </div><div class="line">tcp        0      0 10.200.60.110:57351     10.200.63.39:1004       TIME_WAIT   -                   </div><div class="line">tcp        0      0 10.200.60.110:41154     10.200.63.32:1004       TIME_WAIT   -                   </div><div class="line">tcp        0      0 10.200.60.110:49677     10.200.60.134:56338     ESTABLISHED -                   </div><div class="line">tcp        0      0 10.200.60.110:45569     10.200.60.108:46035     ESTABLISHED -                   </div><div class="line">tcp        0      0 10.200.60.110:57754     10.200.60.26:1004       TIME_WAIT   -                   </div><div class="line">tcp        0      0 10.200.60.110:37501     10.200.60.91:40437      ESTABLISHED -                   </div><div class="line">tcp        0      0 10.200.60.110:34343     10.200.60.20:56536      ESTABLISHED 16089/java          </div><div class="line">tcp        0      0 10.200.60.110:50742     10.200.60.90:1004       TIME_WAIT   -                   </div><div class="line">tcp        0      0 10.200.60.110:43593     10.200.60.110:8040      TIME_WAIT   -                   </div><div class="line">tcp        0      0 10.200.60.110:34081     10.200.63.146:36796     ESTABLISHED -                   </div><div class="line">tcp        0      0 10.200.60.110:36762     10.200.60.14:1004       TIME_WAIT   -                   </div><div class="line">tcp        0      0 10.200.60.110:59011     10.200.60.138:44763     ESTABLISHED 29726/java          </div><div class="line">tcp        0      0 10.200.60.110:54402     10.200.63.93:1004       TIME_WAIT   -                   </div><div class="line">tcp        0      0 10.200.60.110:59011     10.200.60.10:57589      ESTABLISHED -                   </div><div class="line">tcp        0      0 10.200.60.110:40962     10.200.60.134:56338     ESTABLISHED -                   </div><div class="line">tcp        0      0 10.200.60.110:60855     10.200.60.108:46035     ESTABLISHED -                   </div><div class="line">tcp        0      0 10.200.60.110:44252     10.200.60.17:47796      ESTABLISHED -                   </div><div class="line">tcp        0      0 10.200.60.110:38960     10.200.60.22:35610      ESTABLISHED -                   </div><div class="line">tcp        0      0 10.200.60.110:59011     10.200.60.108:39511     ESTABLISHED -                   </div><div class="line">tcp        0      0 10.200.60.110:43440     10.200.60.120:34289     ESTABLISHED -                   </div><div class="line">tcp        0      0 10.200.60.110:59011     10.200.60.94:58282      ESTABLISHED -                   </div><div class="line">tcp        0      0 10.200.60.110:43427     10.200.60.102:36923     ESTABLISHED 16089/java          </div><div class="line">tcp        0      0 10.200.60.110:43650     10.200.60.110:8040      TIME_WAIT   -                   </div><div class="line">tcp        0      0 10.200.60.110:34343     10.200.60.135:42927     ESTABLISHED -                   </div><div class="line">tcp        0      0 10.200.60.110:33618     10.200.60.110:1004      TIME_WAIT   -                   </div><div class="line">tcp        0      0 10.200.60.110:34343     10.200.60.102:49160     ESTABLISHED -                   </div><div class="line">tcp        0      0 10.200.60.110:59011     10.200.60.135:40508     ESTABLISHED -                   </div><div class="line">tcp        0      0 10.200.60.110:34343     10.200.60.21:41447      ESTABLISHED -                   </div><div class="line">tcp        0      0 10.200.60.110:40731     10.200.60.102:51610     ESTABLISHED -                   </div><div class="line">tcp        0      0 10.200.60.110:59011     10.200.60.95:51058      ESTABLISHED 29726/java          </div><div class="line">tcp        0      0 10.200.60.110:37353     10.200.63.22:1004       TIME_WAIT   -                   </div><div class="line">tcp        0      0 10.200.60.110:42092     10.200.63.28:1004       TIME_WAIT   -                   </div><div class="line">tcp        0      0 10.200.60.110:34505     10.200.60.93:39789      ESTABLISHED -                   </div><div class="line">tcp        0      0 10.200.60.110:34343     10.200.60.93:45092      ESTABLISHED -                   </div><div class="line">tcp        0      0 10.200.60.110:59011     10.200.60.18:48871      ESTABLISHED 29726/java          </div><div class="line">tcp        0      0 10.200.60.110:34343     10.200.60.120:44772     ESTABLISHED 16089/java          </div><div class="line">tcp        0      0 10.200.60.110:59011     10.200.60.14:43564      ESTABLISHED -                   </div><div class="line">tcp        0      0 10.200.60.110:43335     10.200.60.20:33749      ESTABLISHED 16089/java          </div><div class="line">tcp        0      0 10.200.60.110:60169     10.200.63.99:1004       TIME_WAIT   -                   </div><div class="line">tcp        0      0 10.200.60.110:43623     10.200.60.110:8040      TIME_WAIT   -                   </div><div class="line">tcp        0      0 10.200.60.110:59011     10.200.60.17:41051      ESTABLISHED -                   </div><div class="line">tcp        0      0 10.200.60.110:43625     10.200.60.110:8040      TIME_WAIT   -                   </div><div class="line">tcp        0      0 10.200.60.110:38969     10.200.60.109:1004      TIME_WAIT   -                   </div><div class="line">tcp        0      0 10.200.60.110:36232     10.200.60.21:50118      ESTABLISHED -                   </div><div class="line">tcp        0      0 10.200.60.110:59011     10.200.60.21:55883      ESTABLISHED -                   </div><div class="line">tcp        0      0 10.200.60.110:34343     10.200.60.110:53105     ESTABLISHED -                   </div><div class="line">tcp        0      0 10.200.60.110:34867     10.200.60.100:8020      TIME_WAIT   -                   </div><div class="line">tcp        0      0 10.200.60.110:59663     10.200.60.18:36351      ESTABLISHED -                   </div><div class="line">tcp        0      0 10.200.60.110:34417     10.200.60.100:8020      TIME_WAIT   -                   </div><div class="line">tcp        0      0 10.200.60.110:43111     10.200.60.108:33625     ESTABLISHED 16089/java          </div><div class="line">tcp        0      0 10.200.60.110:59011     10.200.60.18:56058      ESTABLISHED -                   </div><div class="line">tcp        0      0 10.200.60.110:53180     10.200.60.14:58840      ESTABLISHED -                   </div><div class="line">tcp        0      0 10.200.60.110:34097     10.200.60.110:1004      TIME_WAIT   -                   </div><div class="line">tcp        0      0 10.200.60.110:34343     10.200.60.17:53950      ESTABLISHED 16089/java          </div><div class="line">tcp        0      0 10.200.60.110:34416     10.200.60.100:8020      TIME_WAIT   -                   </div><div class="line">tcp        0      0 10.200.60.110:43660     10.200.60.110:8040      TIME_WAIT   -                   </div><div class="line">tcp        0      0 10.200.60.110:37145     10.200.63.97:1004       TIME_WAIT   -                   </div><div class="line">tcp        0      0 10.200.60.110:34343     10.200.60.22:43760      ESTABLISHED -                   </div><div class="line">tcp        0      0 10.200.60.110:56233     10.200.60.24:1004       TIME_WAIT   -                   </div><div class="line">tcp        0      0 10.200.60.110:47205     10.200.60.17:39480      ESTABLISHED 16089/java          </div><div class="line">tcp        0      0 10.200.60.110:59359     10.200.60.138:43354     ESTABLISHED 29726/java          </div><div class="line">tcp        0      0 10.200.60.110:46863     10.200.60.104:39564     ESTABLISHED 29726/java          </div><div class="line">tcp        0      0 10.200.60.110:54459     10.200.60.89:49015      ESTABLISHED -                   </div><div class="line">tcp        0      0 10.200.60.110:39622     10.200.60.46:1004       TIME_WAIT   -                   </div><div class="line">tcp        0      0 10.200.60.110:34343     10.200.60.89:38803      ESTABLISHED -                   </div><div class="line">tcp        0      0 10.200.60.110:34343     10.200.60.17:55992      ESTABLISHED -                   </div><div class="line">tcp        0      0 10.200.60.110:52924     10.200.60.22:35610      ESTABLISHED -                   </div><div class="line">tcp        0      0 10.200.60.110:52461     10.200.60.102:36923     ESTABLISHED 29726/java          </div><div class="line">tcp        0      0 10.200.60.110:36788     10.200.60.14:1004       TIME_WAIT   -                   </div><div class="line">tcp        0      0 10.200.60.110:34659     10.200.63.170:9092      TIME_WAIT   -                   </div><div class="line">tcp        0      0 10.200.60.110:34343     10.200.60.116:49015     ESTABLISHED -                   </div><div class="line">tcp        0      0 10.200.60.110:41475     10.200.60.94:59659      ESTABLISHED -                   </div><div class="line">tcp        0      0 10.200.60.110:34783     10.200.60.130:50663     ESTABLISHED -                   </div><div class="line">tcp        0      0 10.200.60.110:34479     10.200.60.132:51099     ESTABLISHED -                   </div><div class="line">tcp        0      0 10.200.60.110:34430     10.200.60.100:8020      TIME_WAIT   -                   </div><div class="line">tcp        0      0 10.200.60.110:43945     10.200.60.110:59011     ESTABLISHED -                   </div><div class="line">tcp        0      0 10.200.60.110:59011     10.200.60.33:37370      ESTABLISHED 29726/java          </div><div class="line">tcp        0      0 10.200.60.110:51167     10.200.60.34:1004       TIME_WAIT   -                   </div><div class="line">tcp        0      0 10.200.60.110:57216     10.200.60.120:34289     ESTABLISHED -                   </div><div class="line">tcp        0      0 10.200.60.110:59011     10.200.60.102:33926     ESTABLISHED 29726/java          </div><div class="line">tcp        0      0 10.200.60.110:46140     10.200.60.66:1004       TIME_WAIT   -                   </div><div class="line">tcp        0      0 10.200.60.110:45301     10.200.63.82:1004       TIME_WAIT   -                   </div><div class="line">tcp        0      0 10.200.60.110:52795     10.200.60.116:53969     ESTABLISHED -                   </div><div class="line">tcp        0      0 10.200.60.110:43643     10.200.60.110:8040      TIME_WAIT   -                   </div><div class="line">tcp        0      0 10.200.60.110:34343     10.200.60.104:37286     ESTABLISHED 16089/java          </div><div class="line">tcp        0      0 10.200.60.110:43512     10.200.60.45:1004       TIME_WAIT   -                   </div><div class="line">tcp        0      0 10.200.60.110:39253     10.200.60.76:1004       TIME_WAIT   -                   </div><div class="line">tcp        0      0 10.200.60.110:33435     10.200.60.95:59369      ESTABLISHED 29726/java          </div><div class="line">tcp        0      0 10.200.60.110:43634     10.200.60.110:8040      TIME_WAIT   -                   </div><div class="line">tcp        0      0 10.200.60.110:35136     10.200.60.93:60854      ESTABLISHED -                   </div><div class="line">tcp        0      0 10.200.60.110:33792     10.200.60.110:1004      TIME_WAIT   -                   </div><div class="line">tcp        0      0 10.200.60.110:44122     10.200.60.96:1004       TIME_WAIT   -                   </div><div class="line">tcp        0      0 10.200.60.110:34593     10.200.60.100:8020      TIME_WAIT   -                   </div><div class="line">tcp        0      0 10.200.60.110:34343     10.200.60.14:42779      ESTABLISHED -                   </div><div class="line">tcp        0      0 10.200.60.110:52399     10.200.60.102:51610     ESTABLISHED -                   </div><div class="line">tcp        0      0 10.200.60.110:32909     10.200.60.13:56295      ESTABLISHED 29726/java          </div><div class="line">tcp        0      0 10.200.60.110:22000     10.200.60.110:40089     ESTABLISHED 3773/impalad        </div><div class="line">tcp        0      0 10.200.60.110:47171     10.200.60.36:1004       TIME_WAIT   -                   </div><div class="line">tcp        0      0 10.200.60.110:59011     10.200.60.17:35312      ESTABLISHED -                   </div><div class="line">tcp        0      0 10.200.60.110:34343     10.200.60.130:39980     ESTABLISHED -                   </div><div class="line">tcp        0      0 10.200.60.110:34955     10.200.60.17:47796      ESTABLISHED -                   </div><div class="line">tcp        0      0 10.200.60.110:47370     10.200.60.32:1004       TIME_WAIT   -                   </div><div class="line">tcp        0      0 10.200.60.110:34343     10.200.60.17:38840      ESTABLISHED -                   </div><div class="line">tcp        0      0 10.200.60.110:43631     10.200.60.110:8040      TIME_WAIT   -                   </div><div class="line">tcp        0      0 10.200.60.110:59011     10.200.60.120:41992     ESTABLISHED -                   </div><div class="line">tcp        0      0 10.200.60.110:57476     10.200.63.91:1004       TIME_WAIT   -                   </div><div class="line">tcp        0      0 10.200.60.110:38870     10.200.63.146:28953     ESTABLISHED -                   </div><div class="line">tcp        0      0 10.200.60.110:52231     10.200.60.133:54288     ESTABLISHED -                   </div><div class="line">tcp        0      0 10.200.60.110:59011     10.200.60.132:60810     ESTABLISHED -                   </div><div class="line">tcp        0      0 10.200.60.110:41083     10.200.60.17:39091      ESTABLISHED -                   </div><div class="line">tcp        0      0 10.200.60.110:48201     10.200.60.17:39480      ESTABLISHED 29726/java          </div><div class="line">tcp        0      0 10.200.60.110:43252     10.200.60.18:39020      ESTABLISHED -                   </div><div class="line">tcp        0      0 10.200.60.110:53382     10.200.60.33:52178      ESTABLISHED 29726/java          </div><div class="line">tcp        0      0 10.200.60.110:34480     10.200.60.100:8020      TIME_WAIT   -                   </div><div class="line">tcp        0      0 10.200.60.110:47055     10.200.60.48:1004       TIME_WAIT   -                   </div><div class="line">tcp        0      0 10.200.60.110:43658     10.200.60.110:8040      TIME_WAIT   -                   </div><div class="line">tcp        0      0 10.200.60.110:43654     10.200.60.110:8040      TIME_WAIT   -                   </div><div class="line">tcp        0      0 10.200.60.110:53971     10.200.60.120:56307     ESTABLISHED -                   </div><div class="line">tcp        0      0 10.200.60.110:34343     10.200.60.88:34671      ESTABLISHED -                   </div><div class="line">tcp        0      0 10.200.60.110:38780     10.200.60.58:1004       TIME_WAIT   -                   </div><div class="line">tcp        0      0 10.200.60.110:34343     10.200.60.120:36822     ESTABLISHED -                   </div><div class="line">tcp        0      0 10.200.60.110:59011     10.200.60.22:50797      ESTABLISHED -                   </div><div class="line">tcp        0      0 10.200.60.110:58893     10.200.60.135:43112     ESTABLISHED -                   </div><div class="line">tcp        0      0 10.200.60.110:49199     10.200.60.116:53969     ESTABLISHED -                   </div><div class="line">tcp        0      0 10.200.60.110:46026     10.200.60.18:38386      ESTABLISHED 29726/java          </div><div class="line">tcp        0      0 10.200.60.110:59851     10.200.60.89:49015      ESTABLISHED -                   </div><div class="line">tcp        0      0 10.200.60.110:55987     10.200.60.88:55412      ESTABLISHED 16089/java          </div><div class="line">tcp        0      0 10.200.60.110:34343     10.200.60.133:46192     ESTABLISHED -                   </div><div class="line">tcp        0      0 10.200.60.110:59011     10.200.60.108:46307     ESTABLISHED -                   </div><div class="line">tcp        0      0 10.200.60.110:34487     10.200.60.100:8020      TIME_WAIT   -                   </div><div class="line">tcp        0      0 10.200.60.110:41980     10.200.60.33:52178      ESTABLISHED 16089/java          </div><div class="line">tcp        0      0 10.200.60.110:34485     10.200.60.100:8020      TIME_WAIT   -                   </div><div class="line">tcp        0      0 10.200.60.110:33359     10.200.60.73:1004       TIME_WAIT   -                   </div><div class="line">tcp        0      0 10.200.60.110:59011     10.200.60.91:35867      ESTABLISHED -                   </div><div class="line">tcp        0      0 10.200.60.110:59011     10.200.60.116:33957     ESTABLISHED -                   </div><div class="line">tcp        0      0 10.200.60.110:60408     10.200.60.88:39875      ESTABLISHED -                   </div><div class="line">tcp        0      0 10.200.60.110:34343     10.200.60.18:33993      ESTABLISHED -                   </div><div class="line">tcp        0      0 10.200.60.110:34468     10.200.60.100:8020      TIME_WAIT   -                   </div><div class="line">tcp        0      0 10.200.60.110:39549     10.200.63.33:38616      ESTABLISHED 29726/java          </div><div class="line">tcp        0      0 10.200.60.110:34343     10.200.60.108:44681     ESTABLISHED 16089/java          </div><div class="line">tcp        0      0 10.200.60.110:34343     10.200.60.33:50491      ESTABLISHED 16089/java          </div><div class="line">tcp        0      0 10.200.60.110:54537     10.200.60.93:39789      ESTABLISHED -                   </div><div class="line">tcp        0      0 10.200.60.110:43716     10.200.60.110:8040      TIME_WAIT   -                   </div><div class="line">tcp        0      0 10.200.60.110:59011     10.200.60.88:58972      ESTABLISHED 29726/java          </div><div class="line">tcp        0      0 10.200.60.110:55521     10.200.60.81:1004       TIME_WAIT   -                   </div><div class="line">tcp        0      0 10.200.60.110:52129     10.200.60.68:1004       TIME_WAIT   -                   </div><div class="line">tcp        0      0 10.200.60.110:34343     10.200.60.135:54525     ESTABLISHED -                   </div><div class="line">tcp        0      0 10.200.60.110:45792     10.200.63.92:1004       TIME_WAIT   -                   </div><div class="line">tcp        0      0 10.200.60.110:38798     10.200.60.10:1004       TIME_WAIT   -</div></pre></td></tr></table></figure>
</p>
]]></content>
      
        <categories>
            
            <category> CDH </category>
            
            <category> Error </category>
            
        </categories>
        
        
    </entry>
    
    <entry>
      <title><![CDATA[Cloudera Manager API]]></title>
      <url>/CDH/Shell/Cloudera%20Manager%20API.html</url>
      <content type="html"><![CDATA[<h1 id="Cloudera-Manager-API"><a href="#Cloudera-Manager-API" class="headerlink" title="Cloudera Manager API"></a>Cloudera Manager API</h1><p><a href="http://cloudera.github.io/cm_api/apidocs/v15/index.html" target="_blank" rel="external">CM 5.10 API</a><br><a href="https://www.cloudera.com/documentation/enterprise/5-10-x/topics/cm_intro_api.html" target="_blank" rel="external">Cloudera Manager API</a></p>
<h2 id="示例"><a href="#示例" class="headerlink" title="示例:"></a>示例:</h2><p>重启cluster集群的impala服务</p>
<p><code>curl -X POST -u user:&#39;xxx&#39; http://192.168.65.50:7180/api/v15/clusters/cluster/services/impala/commands/restart</code></p>
<p>显示cluster集群的所有服务</p>
<p><code>curl -u user:&#39;passwd&#39; http://192.168.65.50:7180/api/v15/clusters/cluster/services</code></p>
]]></content>
      
        <categories>
            
            <category> CDH </category>
            
            <category> Shell </category>
            
        </categories>
        
        
    </entry>
    
    <entry>
      <title><![CDATA[HDFS-diskbalance]]></title>
      <url>/CDH/Error/HDFS-diskbalance.html</url>
      <content type="html"><![CDATA[<h1 id="HDFS-diskbalance问题-未解决"><a href="#HDFS-diskbalance问题-未解决" class="headerlink" title="HDFS-diskbalance问题(未解决)"></a>HDFS-diskbalance问题(未解决)</h1><p>开头怀疑是sentry的权限问题,就也给hdfs用户hive元数据库的admin权限<br><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">create role hdfs_role;</div><div class="line">grant role hdfs_role to group hdfs;</div><div class="line">grant all on URI &apos;hdfs:///system/diskbalancer&apos; to role hdfs_role;</div></pre></td></tr></table></figure></p>
<p>发现问题依旧<br><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">hdfs dfs -ls /system/diskbalancer</div><div class="line">hdfs diskbalancer -plan ddp-dn-04.cmdmp.com</div><div class="line">hdfs diskbalancer -plan ddp-dn-129.cmdmp.com</div><div class="line"></div><div class="line">hdfs@ddp-dn-129:~&gt; hdfs diskbalancer -plan ddp-dn-129.cmdmp.com</div><div class="line">17/06/12 13:42:31 INFO balancer.KeyManager: Block token params received from NN: update interval=10hrs, 0sec, token lifetime=10hrs, 0sec</div><div class="line">17/06/12 13:42:31 INFO block.BlockTokenSecretManager: Setting block keys</div><div class="line">17/06/12 13:42:31 INFO balancer.KeyManager: Update block keys every 2hrs, 30mins, 0sec</div><div class="line">17/06/12 13:42:32 INFO ipc.Client: Retrying connect to server: ddp-dn-129.cmdmp.com/10.200.60.40:50020. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)</div><div class="line">17/06/12 13:42:33 INFO ipc.Client: Retrying connect to server: ddp-dn-129.cmdmp.com/10.200.60.40:50020. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)</div><div class="line">17/06/12 13:42:34 INFO ipc.Client: Retrying connect to server: ddp-dn-129.cmdmp.com/10.200.60.40:50020. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)</div><div class="line">17/06/12 13:42:35 INFO ipc.Client: Retrying connect to server: ddp-dn-129.cmdmp.com/10.200.60.40:50020. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)</div><div class="line">17/06/12 13:42:36 INFO ipc.Client: Retrying connect to server: ddp-dn-129.cmdmp.com/10.200.60.40:50020. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)</div><div class="line">17/06/12 13:42:37 INFO ipc.Client: Retrying connect to server: ddp-dn-129.cmdmp.com/10.200.60.40:50020. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)</div><div class="line">17/06/12 13:42:38 INFO ipc.Client: Retrying connect to server: ddp-dn-129.cmdmp.com/10.200.60.40:50020. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)</div><div class="line">17/06/12 13:42:39 INFO ipc.Client: Retrying connect to server: ddp-dn-129.cmdmp.com/10.200.60.40:50020. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)</div><div class="line">17/06/12 13:42:40 INFO ipc.Client: Retrying connect to server: ddp-dn-129.cmdmp.com/10.200.60.40:50020. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)</div><div class="line">17/06/12 13:42:41 INFO ipc.Client: Retrying connect to server: ddp-dn-129.cmdmp.com/10.200.60.40:50020. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)</div><div class="line">17/06/12 13:42:41 ERROR tools.DiskBalancerCLI: java.net.ConnectException: Call From ddp-dn-129.cmdmp.com/10.200.60.40 to ddp-dn-129.cmdmp.com:50020 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</div></pre></td></tr></table></figure></p>
<p><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">ddp-dn-04:~ # hdfs diskbalancer -help plan</div><div class="line">usage: hdfs diskbalancer -plan &lt;hostname&gt; [options]</div><div class="line">Creates a plan that describes how much data should be moved between disks.</div><div class="line"></div><div class="line">    --bandwidth &lt;arg&gt;             Maximum disk bandwidth (MB/s) in integer</div><div class="line">                                  to be consumed by diskBalancer. e.g. 10</div><div class="line">                                  MB/s.</div><div class="line">    --maxerror &lt;arg&gt;              Describes how many errors can be</div><div class="line">                                  tolerated while copying between a pair</div><div class="line">                                  of disks.</div><div class="line">    --out &lt;arg&gt;                   Local path of file to write output to,</div><div class="line">                                  if not specified defaults will be used.</div><div class="line">    --plan &lt;arg&gt;                  Hostname, IP address or UUID of datanode</div><div class="line">                                  for which a plan is created.</div><div class="line">    --thresholdPercentage &lt;arg&gt;   Percentage of data skew that is</div><div class="line">                                  tolerated before disk balancer starts</div><div class="line">                                  working. For example, if total data on a</div><div class="line">                                  2 disk node is 100 GB then disk balancer</div><div class="line">                                  calculates the expected value on each</div><div class="line">                                  disk, which is 50 GB. If the tolerance</div><div class="line">                                  is 10% then data on a single disk needs</div><div class="line">                                  to be more than 60 GB (50 GB + 10%</div><div class="line">                                  tolerance value) for Disk balancer to</div><div class="line">                                  balance the disks.</div><div class="line">    --v                           Print out the summary of the plan on</div><div class="line">                                  console</div><div class="line"></div><div class="line">Plan command creates a set of steps that represent a planned data move. A</div><div class="line">plan file can be executed on a data node, which will balance the data.</div><div class="line"></div><div class="line"></div><div class="line">ddp-nn-02:/etc/hadoop/conf # hdfs getconf -confkey dfs.disk.balancer.enabled</div><div class="line">false</div><div class="line"></div><div class="line">ddp-dn-01:~ # hdfs diskbalancer --plan ddp-dn-12.cmdmp.com</div><div class="line">17/06/13 16:54:09 INFO balancer.KeyManager: Block token params received from NN: update interval=10hrs, 0sec, token lifetime=10hrs, 0sec</div><div class="line">17/06/13 16:54:09 INFO block.BlockTokenSecretManager: Setting block keys</div><div class="line">17/06/13 16:54:09 INFO balancer.KeyManager: Update block keys every 2hrs, 30mins, 0sec</div><div class="line">17/06/13 16:54:09 ERROR tools.DiskBalancerCLI: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.security.AccessControlException): Permission denied.</div><div class="line">  at org.apache.hadoop.hdfs.server.datanode.DataNode.checkSuperuserPrivilege(DataNode.java:948)</div><div class="line">  at org.apache.hadoop.hdfs.server.datanode.DataNode.getDiskBalancerSetting(DataNode.java:3208)</div><div class="line">  at org.apache.hadoop.hdfs.protocolPB.ClientDatanodeProtocolServerSideTranslatorPB.getDiskBalancerSetting(ClientDatanodeProtocolServerSideTranslatorPB.java:361)</div><div class="line">  at org.apache.hadoop.hdfs.protocol.proto.ClientDatanodeProtocolProtos$ClientDatanodeProtocolService$2.callBlockingMethod(ClientDatanodeProtocolProtos.java:17901)</div><div class="line">  at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:617)</div><div class="line">  at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1073)</div><div class="line">  at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2216)</div><div class="line">  at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2212)</div><div class="line">  at java.security.AccessController.doPrivileged(Native Method)</div><div class="line">  at javax.security.auth.Subject.doAs(Subject.java:415)</div><div class="line">  at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1796)</div><div class="line">  at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2210)</div></pre></td></tr></table></figure>
</p>
]]></content>
      
        <categories>
            
            <category> CDH </category>
            
            <category> Error </category>
            
        </categories>
        
        
    </entry>
    
    <entry>
      <title><![CDATA[Hive Metastore Server 僵死]]></title>
      <url>/CDH/Hive/Hive%20Metastore%20Server%E5%83%B5%E6%AD%BB.html</url>
      <content type="html"><![CDATA[<h1 id="Hive-Metastore-Server-僵死"><a href="#Hive-Metastore-Server-僵死" class="headerlink" title="Hive Metastore Server 僵死"></a>Hive Metastore Server 僵死</h1><p>现状: 在Hive Metastore Server上的角色只要停止服务,就无法再启动</p>
<p>开头怀疑是cloudera-scm-agent 出问题<br><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">877  2017-05-24 15:50:05 /etc/init.d/cloudera-scm-agent stop</div><div class="line">878  2017-05-24 15:50:15 ss -anp|grep 9000</div><div class="line">879  2017-05-24 15:50:39 ps axu|grep agent</div><div class="line">880  2017-05-24 15:51:08 ss -anp|grep 9000</div><div class="line">881  2017-05-24 15:51:18 /etc/init.d/cloudera-scm-agent start</div><div class="line">882  2017-05-24 15:51:27 /etc/init.d/cloudera-scm-agent status</div><div class="line">883  2017-05-24 15:51:51 ss -anp|grep 9000</div><div class="line">884  2017-05-24 15:55:14 cd /var/log/</div></pre></td></tr></table></figure></p>
<p>重启服务后发现依旧</p>
<p>于是怀疑IO问题<br><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">891  2017-05-24 16:04:03 ulimit -a</div><div class="line">892  2017-05-24 16:04:55 cat /etc/passwd</div><div class="line"></div><div class="line">896  2017-05-24 16:07:10 vi /etc/security/limits.conf </div><div class="line">897  2017-05-24 16:08:14 ulimit -a</div><div class="line">898  2017-05-24 16:09:38 lsof -n |awk &apos;&#123;print $2&#125;&apos;|sort|uniq -c |sort -nr|more</div></pre></td></tr></table></figure></p>
<p>设置最大文件打开数<br><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">#ftp             hard    nproc           0</div><div class="line">#@student        -       maxlogins       4</div><div class="line"></div><div class="line">*                soft    nofile          65535</div><div class="line">*                hard    nofile          65535</div></pre></td></tr></table></figure></p>
<p>之后reboot解决</p>
]]></content>
      
        <categories>
            
            <category> CDH </category>
            
            <category> Hive </category>
            
        </categories>
        
        
    </entry>
    
    <entry>
      <title><![CDATA[hive impala  培训]]></title>
      <url>/CDH/hive%20impala%20%20%E5%9F%B9%E8%AE%AD.html</url>
      <content type="html"><![CDATA[<p>$ADIR is a shortcut that points to the /home/training/training_materials/<br>analyst directory</p>
<p>Click Select files to bring up a file browser. By default, the<br>/home/training/Desktop folder displays. Click the home directory button<br>(training) then navigate to the course data directory:<br>training_materials/analyst/data.</p>
<p>Note the /dualcore directory. Most of your work in this course will be in that<br>directory. Try creating a temporary subdirectory in /dualcore:</p>
<hr>
<p>hive 默认路径warehouse</p>
<p>创建xx表后 会在warehouse下创建xx.db</p>
<p>让hive管理整个结构</p>
<p>partition建议每个分区的数据保证一定的量(不要有很多小数据)</p>
<p>beeline set方法打开partition 参数</p>
<p>orcfile 文件 优于rcfile </p>
<p>impala 推荐使用parquet 文件format<br>CDH 对parquet 支持比较好</p>
<hr>
<p>partition<br>冷数据 热数据 上线流文件兼容性</p>
<p>impala join 大表放在第一个 会</p>
<p>profile impala调优</p>
<hr>
<p>63.102 103 kerberos</p>
]]></content>
      
        <categories>
            
            <category> CDH </category>
            
        </categories>
        
        
    </entry>
    
    <entry>
      <title><![CDATA[Hbase ZK锁表]]></title>
      <url>/CDH/Error/Hbase%20ZK%E9%94%81%E8%A1%A8.html</url>
      <content type="html"><![CDATA[<h1 id="Hbase-ZK锁表"><a href="#Hbase-ZK锁表" class="headerlink" title="Hbase ZK锁表"></a>Hbase ZK锁表</h1><p>开发反复create drop table 导致zk的数据不同步<br>首先到Zookeeper中<strong>/hbase/table-lock</strong> 和<strong>/hbase/table</strong> 两个目录删除对应的表</p>
<p><code>rmr /hbase/table-lock/user_order_info</code></p>
<p>之后重启Hbase集群,发现报错<strong>/hbase/.tmp/</strong> 在HDFS中发现刚刚锁住的表还在被占用中,必须手动删除</p>
<p><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">5月 16, 上午10点55:19.778分	ERROR	org.apache.hadoop.hbase.backup.HFileArchiver	</div><div class="line">Failed to archive class org.apache.hadoop.hbase.backup.HFileArchiver$FileablePath, file:hdfs://nameservice2/hbase/.tmp/data/default/user_order_info/63fa237eba286bf13d01d0e04b13d149/info/88be874a75d345af848dfd07f0ff0340_SeqId_55_</div><div class="line"></div><div class="line"></div><div class="line">5月 16, 上午10点55:19.467分	ERROR	org.apache.hadoop.hbase.backup.HFileArchiver	</div><div class="line">Failed to archive class org.apache.hadoop.hbase.backup.HFileArchiver$FileablePath, file:hdfs://nameservice2/hbase/.tmp/data/default/user_order_info/63fa237eba286bf13d01d0e04b13d149/info/88be874a75d345af848dfd07f0ff0340_SeqId_55_</div></pre></td></tr></table></figure>
</p>
<p>手动删除后发现,HMaster节点虽然启动成功但不能被选举为active,读取日志发现,打开文件数过多,最终导致HMaster节点一段时间后自动停止服务,</p>
<ul>
<li>开始的想法是通过<code>ulimit -n</code>手动调整hbase用户所能打开的最大文件数,发现修改后不起作用</li>
<li>把/hbase//MasterProcWALs目录下的文件mv出去</li>
<li>同时需要删除HDFS下tmp目录下对应的临时文件，然后重启成功</li>
</ul>
<p>因为HMaster启动的时候会加载这个目录下的所有文件，才会导致too many open files,这个是HBase自身的问题导致是HDFS的connection太多了,重启太多，然后又失败导致的。重启成功后，所有那个目录下的文件应该会被删除的</p>
<hr>
<p>第二天 发现RegionServer 陆续停止服务<br>通过调大 HBase REST Server 最大线程,重启服务解决 </p>
]]></content>
      
        <categories>
            
            <category> CDH </category>
            
            <category> Error </category>
            
        </categories>
        
        
    </entry>
    
    <entry>
      <title><![CDATA[添加HiveServer2角色]]></title>
      <url>/CDH/Hive/%E6%B7%BB%E5%8A%A0HiveServer2.html</url>
      <content type="html"><![CDATA[<h1 id="添加HiveServer2角色"><a href="#添加HiveServer2角色" class="headerlink" title="添加HiveServer2角色"></a>添加HiveServer2角色</h1><p>HIVE–&gt;实例–&gt;添加角色实例–&gt;选择HiveServer2<br>添加一台主机 ddp-dn-12.cmdmp.com   192.168.65.64<br>添加成功后,会显示没有该角色的keytab<br>到在节点上选择操作–&gt; <em>停止主机上的角色</em><br>成功后选择操作–&gt; <em>重新生成Keytab</em></p>
<p>如果集群使用UDF的话需要创建相应目录<br>登入该主机<br><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">mkdir -p /opt/local/hive/lib/</div><div class="line">mkdir -p /opt/local/hive/reloadable</div><div class="line">scp root@192.168.65.50:/opt/local/hive/lib/* /opt/local/hive/lib/</div><div class="line">scp root@192.168.65.50:/opt/local/hive/reloadable/* /opt/local/hive/reloadable/</div></pre></td></tr></table></figure></p>
<p>之后回到cloudera manager 的该节点页面下选择操作–&gt; <em>启动主机上的角色</em><br><code>beeline -u &#39;jdbc:hive2://192.168.65.64:10000/default;principal=hive/ddp-dn-12.cmdmp.com@CMDMP.COM&#39;</code></p>
]]></content>
      
        <categories>
            
            <category> CDH </category>
            
            <category> Hive </category>
            
        </categories>
        
        
    </entry>
    
    <entry>
      <title><![CDATA[选择和配置数据压缩]]></title>
      <url>/CDH/Spark/Cloudera%E5%B9%B3%E5%8F%B0%E5%AE%89%E8%A3%85Apache%20Spark%202.0%20Beta%E7%89%88%E6%9C%AC.html</url>
      <content type="html"><![CDATA[<h1 id="Cloudera平台安装Apache-Spark-2-0-Beta版本"><a href="#Cloudera平台安装Apache-Spark-2-0-Beta版本" class="headerlink" title="Cloudera平台安装Apache Spark 2.0 Beta版本"></a>Cloudera平台安装Apache Spark 2.0 Beta版本</h1><p><a href="http://www.cloudera.com/documentation/betas/spark2/latest/topics/spark2.html" target="_blank" rel="external">兼容信息</a><br><a href="https://www.cloudera.com/documentation/spark2/latest/topics/spark2_installing.html" target="_blank" rel="external">安装步骤</a></p>
]]></content>
      
        <categories>
            
            <category> CDH </category>
            
            <category> Spark </category>
            
        </categories>
        
        
    </entry>
    
    <entry>
      <title><![CDATA[Python文件处理]]></title>
      <url>/python/Python%E6%96%87%E4%BB%B6%E5%A4%84%E7%90%86.html</url>
      <content type="html"><![CDATA[<h1 id="Python文件处理"><a href="#Python文件处理" class="headerlink" title="Python文件处理"></a>Python文件处理</h1><p>文件打开方法: open(name[,mode[buf]])<br>name: 文件路径<br>mode: 打开方式<br>buf: 缓冲buffering大小</p>
<ul>
<li>文件读取方式:<ul>
<li>read([size]):读取文件(读取size个字节,默认读取全部)</li>
<li>readline([size]):读取一行</li>
<li>readlines([size]):读取完文件,返回每一行所组成的列表</li>
</ul>
</li>
<li>文件写入方式:<ul>
<li>write(str):将字符串写入文件</li>
<li>writelines(sequence_of_strings):写多行到文件</li>
</ul>
</li>
</ul>
]]></content>
      
        <categories>
            
            <category> python </category>
            
        </categories>
        
        
    </entry>
    
    <entry>
      <title><![CDATA[lrzsz大文件传输]]></title>
      <url>/CDH/Error/lrzsz%E5%A4%A7%E6%96%87%E4%BB%B6%E4%BC%A0%E8%BE%93.html</url>
      <content type="html"><![CDATA[<p>rz、sz命令结合方便的上下传文件，但经过跳板机之后直接rz、sz一个稍大的文件会在上传一部分后退出并显示一堆乱码，这是因为这中间有控制字符的原因。 </p>
<p><em>解决办法：*</em> 加参数-e忽略控制字符：rz -e和sz -e</p>
<p>rz，sz是Linux/Unix同Windows进行ZModem文件传输的命令行工具。</p>
<p>windows端需要支持ZModem的telnet/ssh客户端（比如SecureCRT），运行命令rz即是接收文件，SecureCRT就会弹出文件选择对话框，选好文件之后关闭对话框，文件就会上传到当前目录。注意：单独用rz会有两个问题：上传中断、上传文件变化（md5不同），解决办法是上传时用rz -be，并且去掉弹出的对话框中“Upload files as ASCII”前的勾选。-a, –ascii -b, –binary ，用binary的方式上传下载，不解释字符为ascii，-e, –escape 强制escape 所有控制字符，比如Ctrl+x，DEL等rar,gif等文件。采用 -b 用binary的方式上传。文件比较大而上传出错的话，采用参数 -e。如果用不带参数的rz命令上传大文件时，常常上传一半就断掉了，很可能是rz以为上传的流中包含某些特殊控制字符，造成rz提前退出</p>
]]></content>
      
        <categories>
            
            <category> CDH </category>
            
            <category> Error </category>
            
        </categories>
        
        
    </entry>
    
    <entry>
      <title><![CDATA[介绍]]></title>
      <url>/CDH/%E4%BB%8B%E7%BB%8D.html</url>
      <content type="html"><![CDATA[<h2 id="host"><a href="#host" class="headerlink" title="host"></a>host</h2><ul>
<li>hostname </li>
<li>ntp </li>
<li>/etc/hosts</li>
<li>cloudera-angent <ul>
<li>服务安装</li>
<li>上报日志</li>
<li>角色状态检查</li>
</ul>
</li>
</ul>
<h2 id="HDFS"><a href="#HDFS" class="headerlink" title="HDFS"></a>HDFS</h2><ul>
<li>本质也是一个文件系统 不过是把单个文件的块大小放大.同时一份数据会备份两次在不同的host上</li>
<li>权限 <ul>
<li>Sentry HDFS同步 需要解除hive metastroe HA</li>
<li>同时需要开启ACL权限认证</li>
</ul>
</li>
<li>磁盘损坏<ul>
<li>先判断该主机的host能否暂停服务</li>
<li>停止该host的所有角色</li>
<li>在DATANODE角色的配置文件里面删除对应挂载分区</li>
<li>启用该host的所有角色</li>
<li>报障 更换硬盘后</li>
</ul>
</li>
<li><p>Balance</p>
<ul>
<li>可以在不同DATANODE节点中做负载均衡</li>
<li>也可以在单个DATANODE上做不同挂载盘的负载均衡</li>
</ul>
</li>
<li><p>NameNode</p>
<ul>
<li>NameNode管理文件系统的命名空间。它维护着文件系统树及整棵树内所有的文件和目录。这些信息以两个文件形式永久保存在本地磁盘上：命名空间镜像文件和编辑日志文件。</li>
<li></li>
</ul>
</li>
<li><p>JournalNode</p>
<ul>
<li>两个NameNode为了数据同步，会通过一组称作JournalNodes的独立进程进行相互通信。当active状态的NameNode的命名空间有任何修改时，会告知大部分的JournalNodes进程。standby状态的NameNode有能力读取JNs中的变更信息，并且一直监控edit log的变化，把变化应用于自己的命名空间。standby可以确保在集群出错时，命名空间状态已经完全同步了。</li>
</ul>
</li>
<li><p>Failover Controller</p>
</li>
<li>Gateway</li>
<li><p>HttpFS</p>
<ul>
<li>httpfs是cloudera公司提供的一个hadoop hdfs的一个http接口，通过WebHDFS REST API 可以对hdfs进行读写等访问</li>
</ul>
</li>
<li><p>kerberos</p>
</li>
</ul>
<h2 id="hive"><a href="#hive" class="headerlink" title="hive"></a>hive</h2><ul>
<li>hive是基于Hadoop的一个数据仓库工具，可以将结构化的数据文件映射为一张数据库表，并提供简单的sql查询功能，可以将sql语句转换为MapReduce任务进行运行。</li>
<li>元数据库用的是两个MySQL</li>
<li><p>HiveServer2 </p>
<ul>
<li>支持sentry</li>
<li>过多连接会导致JVM heap 过大,造成GC时间过长,最后导致该角色拒绝服务</li>
</ul>
</li>
<li><p>Hive cli</p>
<ul>
<li>不支持sentry 不受权限管控 </li>
</ul>
</li>
<li>UDF 自定义函数 <ul>
<li>5.7 以后create function 方式有改动 禁止使用add jar语法<ul>
<li>不写location 默认读取AUX 下也就是 hiverserver2 和 metastore 所在主机上必须创建相同路径且文件保持一致的jar包</li>
<li>写location是读取HDFS上的jar包</li>
</ul>
</li>
</ul>
</li>
<li>Gateway</li>
</ul>
<h2 id="Hue"><a href="#Hue" class="headerlink" title="Hue"></a>Hue</h2><ul>
<li>是一个开源的Apache Hadoop UI系统,最早是由Cloudera Desktop演化而来,由Cloudera贡献给开源社区,它是基于Python Web框架Django实现的。</li>
<li>基于文件浏览器（File Browser）访问HDFS</li>
<li>基于Hive编辑器来开发和运行Hive查询</li>
<li>支持基于Solr进行搜索的应用，并提供可视化的数据视图，以及仪表板（Dashboard）</li>
<li>支持基于Impala的应用进行交互式查询</li>
<li>支持Spark编辑器和仪表板（Dashboard）</li>
<li>支持Pig编辑器，并能够提交脚本任务</li>
<li>支持Oozie编辑器，可以通过仪表板提交和监控Workflow、Coordinator和Bundle</li>
<li>支持HBase浏览器，能够可视化数据、查询数据、修改HBase表</li>
<li>支持Metastore浏览器，可以访问Hive的元数据，以及HCatalog</li>
<li>支持Job浏览器，能够访问MapReduce Job（MR1/MR2-YARN）</li>
<li>支持Job设计器，能够创建MapReduce/Streaming/Java Job</li>
<li>支持Sqoop 2编辑器和仪表板（Dashboard）</li>
<li>支持ZooKeeper浏览器和编辑器</li>
<li>支持MySql、PostGresql、Sqlite和Oracle数据库查询编辑器</li>
</ul>
<h2 id="impala"><a href="#impala" class="headerlink" title="impala"></a>impala</h2><ul>
<li>Impala是Cloudera公司主导开发的新型查询系统，它提供SQL语义，能查询存储在Hadoop的HDFS和HBase中的PB级大数据。已有的Hive系统虽然也提供了SQL语义，但由于Hive底层执行使用的是MapReduce引擎，仍然是一个批处理过程，难以满足查询的交互性。相比之下，Impala的最大特点也是最大卖点就是它的快速。但是不稳定</li>
<li>过度频繁的访问会导致impala daemon 的端口无法正常释放 导致不同host上的端口互相阻塞,导致查询失败</li>
<li>同时没有容错机制,由namenode获取文件路径后,如果该节点文件所在的磁盘损坏但刚刚修复但是,也会导致</li>
<li>Impala不需要把中间结果写入磁盘，省掉了大量的I/O开销。</li>
<li>省掉了MapReduce作业启动的开销。MapReduce启动task的速度很慢（默认每个心跳间隔是3秒钟），Impala直接通过相应的服务进程来进行作业调度，速度快了很多。</li>
<li>Impala完全抛弃了MapReduce这个不太适合做SQL查询的范式，而是像Dremel一样借鉴了MPP并行数据库的思想另起炉灶，因此可做更多的查询优化，从而省掉不必要的shuffle、sort等开销。</li>
<li>通过使用LLVM来统一编译运行时代码，避免了为支持通用编译而带来的不必要开销。</li>
<li>用C++实现，做了很多有针对性的硬件优化，例如使用SSE指令。</li>
<li>使用了支持Data locality的I/O调度机制，尽可能地将数据和计算分配在同一台机器上进行，减少了网络开销</li>
<li>因为公用同一个元数据库,所以hive上对表做了修改后,需要impala在</li>
<li>资源池监控控制不了 refresh 语句</li>
</ul>
<h2 id="Oozie"><a href="#Oozie" class="headerlink" title="Oozie"></a>Oozie</h2><p>-Oozie是一种Java Web应用程序，它运行在Java servlet容器——即Tomcat——中，并使用数据库来存储以下内容:<br>-工作流定义<br>-当前运行的工作流实例，包括实例的状态和变量<br>-Oozie工作流是放置在控制依赖DAG（有向无环图 Direct Acyclic Graph）中的一组动作（例如，Hadoop的Map/Reduce作业、Pig作业等），其中指定了动作执行的顺序。我们会使用hPDL（一种XML流程定义语言）来描述这个图。</p>
<h2 id="Sentry"><a href="#Sentry" class="headerlink" title="Sentry"></a>Sentry</h2><ul>
<li>Sentry Server 用的是元数据库</li>
<li>如果用JDBC的连接的话 需要在sentry SERVER 里配上 sentry.service.admin.group 和sentry.service.allow.connect</li>
</ul>
]]></content>
      
        <categories>
            
            <category> CDH </category>
            
        </categories>
        
        
    </entry>
    
    <entry>
      <title><![CDATA[Hue隧道访问失效]]></title>
      <url>/CDH/Error/Hue%E9%9A%A7%E9%81%93%E8%AE%BF%E9%97%AE%E5%A4%B1%E6%95%88.html</url>
      <content type="html"><![CDATA[<blockquote>
<p>判断应该是Django框架中,发送报文的MIME有问题<br><a href="https://github.com/cloudera/hue/commit/34388da14712497b685c6b97497c698e720b1a16" target="_blank" rel="external">path地址</a></p>
</blockquote>
<h1 id="HUE通过隧道访问"><a href="#HUE通过隧道访问" class="headerlink" title="HUE通过隧道访问"></a>HUE通过隧道访问</h1><p>HUE通过隧道映射到本地后发现所有的JavaScript文件解析有问题:</p>
<p><code>Refused to execute script from &#39;http://127.0.0.1:18888/static/desktop/js/jquery.tablescroller.038d8a8feae9.js&#39; because its MIME type (&#39;text/x-js&#39;) is not executable, and strict MIME type checking is enabled.</code></p>
<p>问题在于cloudera中Django的配置文件具体在哪,后来终于终于找到了</p>
<h2 id="HUE文件路径"><a href="#HUE文件路径" class="headerlink" title="HUE文件路径:"></a>HUE文件路径:</h2><p><code>/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/lib/hue/desktop</code></p>
<h3 id="desktop-core-src-desktop-middleware-py"><a href="#desktop-core-src-desktop-middleware-py" class="headerlink" title="desktop/core/src/desktop/middleware.py"></a>desktop/core/src/desktop/middleware.py</h3><p><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line"> import inspect</div><div class="line"> import json</div><div class="line"> import logging</div><div class="line">+import mimetypes</div><div class="line"> import os.path</div><div class="line"> import re</div><div class="line"> import tempfile</div><div class="line">@@ -683,4 +684,22 @@ def process_response(self, request, response):</div><div class="line">     if self.secure_content_security_policy and not &apos;Content-Security-Policy&apos; in response:</div><div class="line">       response[&quot;Content-Security-Policy&quot;] = self.secure_content_security_policy</div><div class="line"> </div><div class="line">-    return response </div><div class="line">+    return response</div><div class="line">+</div><div class="line">+</div><div class="line">+class MimeTypeJSFileFixStreamingMiddleware(object):</div><div class="line">+  &quot;&quot;&quot;</div><div class="line">+  Middleware to detect and fix &quot;.js&quot; mimetype. SLES 11SP4 as example OS which detect js file</div><div class="line">+  as &quot;text/x-js&quot; and if strict X-Content-Type-Options=nosniff is set then browser fails to</div><div class="line">+  execute javascript file.</div><div class="line">+  &quot;&quot;&quot;</div><div class="line">+  def __init__(self):</div><div class="line">+    jsmimetypes = [&apos;application/javascript&apos;, &apos;application/ecmascript&apos;]</div><div class="line">+    if mimetypes.guess_type(&quot;dummy.js&quot;)[0] in jsmimetypes:</div><div class="line">+      LOG.info(&apos;Unloading MimeTypeJSFileFixStreamingMiddleware&apos;)</div><div class="line">+      raise exceptions.MiddlewareNotUsed</div><div class="line">+</div><div class="line">+  def process_response(self, request, response):</div><div class="line">+    if request.path_info.endswith(&apos;.js&apos;):</div><div class="line">+      response[&apos;Content-Type&apos;] = &quot;application/javascript&quot;</div><div class="line">+    return response</div></pre></td></tr></table></figure>
</p>
<h3 id="desktop-core-src-desktop-settings-py"><a href="#desktop-core-src-desktop-settings-py" class="headerlink" title="desktop/core/src/desktop/settings.py"></a>desktop/core/src/desktop/settings.py</h3><p><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">    &apos;django.middleware.http.ConditionalGetMiddleware&apos;,</div><div class="line">     &apos;axes.middleware.FailedLoginMiddleware&apos;,</div><div class="line">+    &apos;desktop.middleware.MimeTypeJSFileFixStreamingMiddleware&apos;,</div><div class="line"> ]</div><div class="line"> </div><div class="line"> # if os.environ.get(ENV_DESKTOP_DEBUG):</div></pre></td></tr></table></figure>
</p>
<h3 id="cloudera-HUE配置文件路径"><a href="#cloudera-HUE配置文件路径" class="headerlink" title="cloudera HUE配置文件路径"></a>cloudera HUE配置文件路径</h3><p><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">ddp-cm:~ # vi /opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/lib/hue/</div><div class="line">LICENSE.txt         Makefile.buildvars  Makefile.vars       NOTICE.txt          VERSION             apps/               cloudera/           ext/                </div><div class="line">Makefile            Makefile.sdk        Makefile.vars.priv  README              app.reg             build/              desktop/            tools/              </div><div class="line">ddp-cm:~ # vi /opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/lib/hue/app</div><div class="line">app.reg  apps/    </div><div class="line">ddp-cm:~ # vi /opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/lib/hue/apps/</div><div class="line">Makefile     beeswax/     hbase/       impala/      jobsub/      oozie/       proxy/       search/      spark/       useradmin/   </div><div class="line">about/       filebrowser/ help/        jobbrowser/  metastore/   pig/         rdbms/       security/    sqoop/       zookeeper/   </div><div class="line">ddp-cm:~ # vi /opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/lib/hue/desktop/</div><div class="line">Makefile    conf/       core/       desktop.db  libs/       logs/       </div><div class="line">ddp-cm:~ # vi /opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/lib/hue/desktop/core/src/desktop</div><div class="line">desktop/          desktop.egg-info/ </div><div class="line">ddp-cm:~ # vi /opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/lib/hue/desktop/core/src/desktop/mi</div><div class="line">middleware.py       middleware_test.py  migrations/         </div><div class="line">ddp-cm:~ # cd  /opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/lib/hue/desktop/core/src/desktop/</div><div class="line">ddp-cm:/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/lib/hue/desktop/core/src/desktop # cp middleware.py middleware.py.bak</div><div class="line">ddp-cm:/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/lib/hue/desktop/core/src/desktop # vi middleware.py</div><div class="line">ddp-cm:/opt/cloudera/parcels/CDH-5.10.0-1.cdh5.10.0.p0.41/lib/hue/desktop/core/src/desktop # vi settings.py</div></pre></td></tr></table></figure></p>
]]></content>
      
        <categories>
            
            <category> CDH </category>
            
            <category> Error </category>
            
        </categories>
        
        
    </entry>
    
    <entry>
      <title><![CDATA[Python 学习笔记]]></title>
      <url>/python/Python%20%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0.html</url>
      <content type="html"><![CDATA[<h2 id="LEGB-L-gt-E-gt-G-gt-B"><a href="#LEGB-L-gt-E-gt-G-gt-B" class="headerlink" title="LEGB : L&gt;E&gt;G&gt;B"></a>LEGB : L&gt;E&gt;G&gt;B</h2><ul>
<li>L:local 函数内部作用域</li>
<li>E:enclosing 函数内部与内嵌函数之间</li>
<li>G:global 全局作用域</li>
<li>B:buil-in 内置作用域</li>
</ul>
<hr>
<p>函数体内部的语句在执行时，一旦执行到return时，函数就执行完毕，并将结果返回。因此，函数内部通过条件判断和循环可以实现非常复杂的逻辑。</p>
<p>如果没有return语句，函数执行完毕后也会返回结果，只是结果为None。</p>
<p>实际上pass可以用来作为占位符，比如现在还没想好怎么写函数的代码，就可以先放一个pass，让代码能运行起来。</p>
<p>写列表生成式时，把要生成的元素x * x放到前面，后面跟for循环，就可以把list创建出来，十分有用，多写几次，很快就可以熟悉这种语法</p>
<p>for循环后面还可以加上if判断，这样我们就可以筛选出仅偶数的平方：</p>
<p><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">&gt;&gt;&gt; [x * x for x in range(1, 11) if x % 2 == 0]</div><div class="line">[4, 16, 36, 64, 100]</div></pre></td></tr></table></figure>
</p>
<p>如果列表元素可以按照某种算法推算出来，那我们是否可以在循环的过程中不断推算出后续的元素呢？这样就不必创建完整的list，从而节省大量的空间。在Python中，这种一边循环一边计算的机制，称为生成器（Generator）。</p>
<p>map()函数接收两个参数，一个是函数，一个是序列，map将传入的函数依次作用到序列的每个元素，并把结果作为新的list返回。</p>
<p>reduce把一个函数作用在一个序列[x1, x2, x3…]上，这个函数必须接收两个参数，reduce把结果继续和序列的下一个元素做累积计算，</p>
<p>filter()也接收一个函数和一个序列。和map()不同的时，filter()把传入的函数依次作用于每个元素，然后根据返回值是True还是False决定保留还是丢弃该元素。</p>
<p>返回闭包时牢记的一点就是：返回函数不要引用任何循环变量，或者后续会发生变化的变量。<br>如果一定要引用循环变量怎么办？方法是再创建一个函数，用该函数的参数绑定循环变量当前的值，无论该循环变量后续如何更改，已绑定到函数参数的值不变：</p>
<p><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">&gt;&gt;&gt; def count():</div><div class="line">...     fs = []</div><div class="line">...     for i in range(1, 4):</div><div class="line">...         def f(j):</div><div class="line">...             def g():</div><div class="line">...                 return j*j</div><div class="line">...             return g</div><div class="line">...         fs.append(f(i))</div><div class="line">...     return fs</div><div class="line">... </div><div class="line">&gt;&gt;&gt; f1, f2, f3 = count()</div><div class="line">&gt;&gt;&gt; f1()</div><div class="line">1</div><div class="line">&gt;&gt;&gt; f2()</div><div class="line">4</div><div class="line">&gt;&gt;&gt; f3()</div><div class="line">9</div></pre></td></tr></table></figure>
</p>
<p>关键字lambda表示匿名函数，冒号前面的x表示函数参数。</p>
<p>匿名函数有个限制，就是只能有一个表达式，不用写return，返回值就是该表达式的结果。</p>
<p>用匿名函数有个好处，因为函数没有名字，不必担心函数名冲突。此外，匿名函数也是一个函数对象，也可以把匿名函数赋值给一个变量，再利用变量来调用该函数：</p>
<p><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">&gt;&gt;&gt; map(lambda x: x * x, [1, 2, 3, 4, 5, 6, 7, 8, 9])</div><div class="line">[1, 4, 9, 16, 25, 36, 49, 64, 81]</div></pre></td></tr></table></figure>
</p>
<p><em>args是非关键字参数，用于元组，*</em>kw是关键字参数，用于字典</p>
<p>注意到<strong>init</strong>方法的第一个参数永远是self，表示创建的实例本身，因此，在<strong>init</strong>方法内部，就可以把各种属性绑定到self，因为self就指向创建的实例本身。</p>
<p>有了<strong>init</strong>方法，在创建实例的时候，就不能传入空的参数了，必须传入与<strong>init</strong>方法匹配的参数，但self不需要传，Python解释器自己会把实例变量传进去：</p>
<p>和普通的函数相比，在类中定义的函数只有一点不同，就是第一个参数永远是实例变量self，并且，调用时，不用传递该参数。除此之外，类的方法和普通函数没有什么区别，所以，你仍然可以用默认参数、可变参数和关键字参数。</p>
<p>如果要让内部属性不被外部访问，可以把属性的名称前加上两个下划线<strong>，在Python中，实例的变量名如果以</strong>开头，就变成了一个私有变量（private），只有内部可以访问，外部不能访问，所以，我们把Student类改一改：</p>
<p>但是如果外部代码要获取name和score怎么办？可以给Student类增加get_name和get_score这样的方法</p>
<hr>
<p><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line"></div><div class="line">class Student(object):</div><div class="line">    ...</div><div class="line"></div><div class="line">    def get_name(self):</div><div class="line">        return self.__name</div><div class="line"></div><div class="line">    def get_score(self):</div><div class="line">        return self.__score</div></pre></td></tr></table></figure>
</p>
<p>如果又要允许外部代码修改score怎么办？可以给Student类增加set_score方法<br>你也许会问，原先那种直接通过bart.score = 59也可以修改啊，为什么要定义一个方法大费周折？因为在方法中，可以对参数做检查，避免传入无效的参数：<br><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">class Student(object):</div><div class="line">    ...</div><div class="line"></div><div class="line">    def set_score(self, score):</div><div class="line">        if 0 &lt;= score &lt;= 100:</div><div class="line">            self.__score = score</div><div class="line">        else:</div><div class="line">            raise ValueError(&apos;bad score&apos;)</div></pre></td></tr></table></figure></p>
<p>要理解什么是多态，我们首先要对数据类型再作一点说明。当我们定义一个class的时候，我们实际上就定义了一种数据类型。我们定义的数据类型和Python自带的数据类型，比如str、list、dict没什么两样：</p>
<p>最后注意到有一种类型就叫TypeType，所有类型本身的类型就是TypeType</p>
<hr>
<h2 id="错误错误处理"><a href="#错误错误处理" class="headerlink" title="错误错误处理"></a>错误错误处理</h2><p>从输出可以看到，当错误发生时，后续语句print ‘result:’, r不会被执行，except由于捕获到ZeroDivisionError，因此被执行。最后，finally语句被执行。然后，程序继续按照流程往下走。</p>
<p>由于没有错误发生，所以except语句块不会被执行，但是finally如果有，则一定会被执行（可以没有finally语句）。<br>错误应该有很多种类，如果发生了不同类型的错误，应该由不同的except语句块处理。没错，可以有多个except来捕获不同类型的错误：<br><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">try:</div><div class="line">    print &apos;try...&apos;</div><div class="line">    r = 10 / int(&apos;a&apos;)</div><div class="line">    print &apos;result:&apos;, r</div><div class="line">except ValueError, e:</div><div class="line">    print &apos;ValueError:&apos;, e</div><div class="line">except ZeroDivisionError, e:</div><div class="line">    print &apos;ZeroDivisionError:&apos;, e</div><div class="line">finally:</div><div class="line">    print &apos;finally...&apos;</div><div class="line">print &apos;END&apos;</div></pre></td></tr></table></figure></p>
<p>Python所有的错误都是从BaseException类派生的，常见的错误类型和继承关系看这里：</p>
<p><a href="https://docs.python.org/2/library/exceptions.html#exception-hierarchy" target="_blank" rel="external">https://docs.python.org/2/library/exceptions.html#exception-hierarchy</a></p>
<p>使用try…except捕获错误还有一个巨大的好处，就是可以跨越多层调用，比如函数main()调用foo()，foo()调用bar()，结果bar()出错了，这时，只要main()捕获到了，就可以处理</p>
<p>如果不捕获错误，自然可以让Python解释器来打印出错误堆栈，但程序也被结束了。既然我们能捕获错误，就可以把错误堆栈打印出来，然后分析错误原因，同时，让程序继续执行下去</p>
<p>如果要抛出错误，首先根据需要，可以定义一个错误的class，选择好继承关系，然后，用raise语句抛出一个错误的实例</p>
<p><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line"># err.py</div><div class="line">class FooError(StandardError):</div><div class="line">    pass</div><div class="line"></div><div class="line">def foo(s):</div><div class="line">    n = int(s)</div><div class="line">    if n==0:</div><div class="line">        raise FooError(&apos;invalid value: %s&apos; % s)</div><div class="line">    return 10 / n</div></pre></td></tr></table></figure>
</p>
<p>assert的意思是，表达式n != 0应该是True，否则，后面的代码就会出错。</p>
<p>如果断言失败，assert语句本身就会抛出AssertionError：</p>
<p>在Thread和Process中，应当优选Process，因为Process更稳定，而且，Process可以分布到多台机器上，而Thread最多只能分布到同一台机器的多个CPU上。</p>
<p>Python的multiprocessing模块不但支持多进程，其中managers子模块还支持把多进程分布到多台机器上。一个服务进程可以作为调度者，将任务分布到其他多个进程中，依靠网络通信。由于managers模块封装很好，不必了解网络通信的细节，就可以很容易地编写分布式多进程程序 </p>
<hr>
<p>IP协议负责把数据从一台计算机通过网络发送到另一台计算机。数据被分割成一小块一小块，然后通过IP包发送出去。由于互联网链路复杂，两台计算机之间经常有多条线路，因此，路由器就负责决定如何把一个IP包转发出去。IP包的特点是按块发送，途径多个路由，但不保证能到达，也不保证顺序到达。</p>
<p>TCP协议则是建立在IP协议之上的。TCP协议负责在两台计算机之间建立可靠连接，保证数据包按顺序到达。TCP协议会通过握手建立连接，然后，对每个IP包编号，确保对方按顺序收到，如果包丢掉了，就自动重发。</p>
<p>许多常用的更高级的协议都是建立在TCP协议基础上的，比如用于浏览器的HTTP协议、发送邮件的SMTP协议等。</p>
<p>一个IP包除了包含要传输的数据外，还包含源IP地址和目标IP地址，源端口和目标端口。</p>
<p>端口有什么作用？在两台计算机通信时，只发IP地址是不够的，因为同一台计算机上跑着多个网络程序。一个IP包来了之后，到底是交给浏览器还是QQ，就需要端口号来区分。每个网络程序都向操作系统申请唯一的端口号，这样，两个进程在两台计算机之间建立网络连接就需要各自的IP地址和各自的端口号。</p>
<p>一个进程也可能同时与多个计算机建立链接，因此它会申请很多端口。</p>
<p>通常我们用一个Socket表示“打开了一个网络链接”，而打开一个Socket需要知道目标计算机的IP地址和端口号，再指定协议类型即可。</p>
]]></content>
      
        <categories>
            
            <category> python </category>
            
        </categories>
        
        
    </entry>
    
    <entry>
      <title><![CDATA[shell编程]]></title>
      <url>/Linux/shell/shell%E7%BC%96%E7%A8%8B.html</url>
      <content type="html"><![CDATA[<p>在shell编程中，”EOF“通常与”&lt;&lt;“结合使用，“&lt;&lt;EOF“表示后续的输入作为子命令或子shell的输入，直到遇到”EOF“，再次返回到主调shell，可将其理解为分界符（delimiter）。既然是分界符，那么形式自然不是固定的，这里可以将”EOF“可以进行自定义，但是前后的”EOF“必须成对出现且不能和shell命令冲突。其使用形式如下：</p>
<pre><code>交互式程序(命令)&lt;&lt;EOF
command1
command2
...
EOF


</code></pre><p>  ”EOF“中间的内容将以标准输入的形式输入到”交互式程序“，当shell看到”&lt;&lt;“知道其后面输入的分界符，当shell再次看到分界符时，两个分界符中间的部分将作为标准输入。<br>       “EOF”一般常和cat命令连用。<br>注意，最后的”EOF“必须单独占一行。</p>
<p>下面以cat命令为例讲解”EOF“使用。一般有以下两种形式<br>1.cat&lt;&lt;EOF
2.cat&lt;&lt;EOF&gt;filename或者cat&lt;&lt;EOF&gt;&gt;filename(cat&lt;&lt;EOF&gt;filename或者cat&lt;&lt;EOF&gt;&gt;filename )<br>其实，第一种形式和第二种形式没有什么本质的区别，第一种形式将内容直接输出到标准输出（屏幕），而第二种形式将标准输出进行重定向，将本应输出到屏幕的内容重定向到文件而已。</p>
<p>例1：<br><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">#cat&lt;&lt;EOF</div><div class="line">&gt;12</div><div class="line">&gt;34D</div><div class="line">&gt;EOF</div><div class="line">12</div><div class="line">34D</div></pre></td></tr></table></figure></p>
<p>例2：<br><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">#cat&gt;&gt;filename&lt;&lt;EOF</div><div class="line">&gt;DHA</div><div class="line">&gt;23</div><div class="line">&gt;EOF</div><div class="line">#cat filename</div><div class="line">DHA</div><div class="line">23</div></pre></td></tr></table></figure></p>
<p>$# 表示提供到shell脚本或者函数的参数总数。<br>$0 表示第一个参数。</p>
<pre><code>整数比较
-eq     等于,如:if [&quot;$a&quot; -eq &quot;$b&quot; ]
-ne     不等于,如:if [&quot;$a&quot; -ne &quot;$b&quot; ]
-gt     大于,如:if [&quot;$a&quot; -gt &quot;$b&quot; ]
-ge    大于等于,如:if [&quot;$a&quot; -ge &quot;$b&quot; ]
-lt      小于,如:if [&quot;$a&quot; -lt &quot;$b&quot; ]
-le      小于等于,如:if [&quot;$a&quot; -le &quot;$b&quot; ]
&lt;  小于(需要双括号),如:((&quot;$a&quot; &lt; &quot;$b&quot;))
&lt;=  小于等于(需要双括号),如:((&quot;$a&quot; &lt;= &quot;$b&quot;))
&gt;  大于(需要双括号),如:((&quot;$a&quot; &gt; &quot;$b&quot;))
&gt;=  大于等于(需要双括号),如:((&quot;$a&quot; &gt;= &quot;$b&quot;))




</code></pre><h2 id="awk"><a href="#awk" class="headerlink" title="awk"></a>awk</h2><p>在命令中调用a w k时,a w k脚本由各种操作和模式组成。<br>如果设置了 - F选项,则 a w k每次读一条记录或一行,并使用指定的分隔符分隔指定域,但<br>如果未设置- F选项,a w k假定空格为域分隔符,并保持这个设置直到发现一新行。当新行出现<br>时,a w k命令获悉已读完整条记录,然后在下一个记录启动读命令,这个读进程将持续到文件<br>尾或文件不再存在。</p>
<p>在碰到 a w k错误时,可相应查找:<br>• 确保整个a w k命令用单引号括起来。<br>• 确保命令内所有引号成对出现。<br>• 确保用花括号括起动作语句,用圆括号括起条件语句。<br>• 可能忘记使用花括号,也许你认为没有必要,但 a w k不这样认为,将按之解释语法。<br>如果查询文件不存在,将得到下述错误信息:</p>
<p>s e d脚本文件 [选项] 输入文件<br>不管是使用 s h e l l命令行方式或脚本文件方式,如果没有指定输入文件, s e d从标准输入中<br>接受输入,一般是键盘或重定向结果。<br>s e d选项如下:<br>n 不打印;s e d不写编辑行到标准输出,缺省为打印所有行(编辑和未编辑)。p命令可以<br>用来打印编辑行。<br>c 下一命令是编辑命令。使用多项编辑时加入此选项。如果只用到一条 s e d命令,<br>此选项无用,但指定它也没有关系。<br>f 如果正在调用 s e d脚本文件,使用此选项。此选项通知 s e d一个脚本文件支持所有的 s e d<br>命令,例如:sed -f myscript.sed input_file,这里m y s c r i p t . s e d即为支持s e d命令的文件。</p>
<p>&lt;EOT&gt; 就是CTRL+D</p>
<p>如果使用 { }来代替(),那么相应的命令将在子 s h e l l而不是当前 s h e l l中作为一个整体被执<br>行,只有在 { }中所有命令的输出作为一个整体被重定向时,其中的命令才被放到子 s h e l l中执<br>行,否则在当前 s h e l l执行。</p>
<h3 id="正则"><a href="#正则" class="headerlink" title="正则"></a>正则</h3><p>注意^符号的使用,当直接用在第一个括号里,意指否定或不匹配括号里内容<br>[ ^a-z A-Z] 匹配任一非字母型字符<br>[ ^ 0 - 9 ]     匹配任一非数字型字符</p>
<p>如果要抽出记录,使其行首不是 4 8,可以在方括号中使用 ^记号,表明查询在行首开始<br><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">[root@CentOS3 devops]# grep &apos;^[^48]&apos; data.f </div><div class="line">219     dec     2CC1999 CAD     23.00   PLV2C   68</div><div class="line">216     sept    3ZL1998 USP     86.00   KVM9E   234</div></pre></td></tr></table></figure></p>
<p>awk 有许多内置变量用来设置环境信息<br>A R G C                     命令行参数个数<br>A R G V                     命令行参数排列<br>E N V I R O N          支持队列中系统环境变量的使用<br>FILENAME               a w k浏览的文件名<br>F N R                         浏览文件的记录数<br>F S                             设置输入域分隔符,等价于命令行 - F选项<br>NF                              浏览记录的域个数<br>N R                             已读的记录数<br>O F S                          输出域分隔符<br>O R S                          输出记录分隔符<br>R S                              控制记录分隔符</p>
<p>s e d选项如下:<br>n 不打印;s e d不写编辑行到标准输出,缺省为打印所有行(编辑和未编辑)。p命令可以<br>用来打印编辑行。<br>c 下一命令是编辑命令。使用多项编辑时加入此选项。如果只用到一条 s e d命令,<br>此选项无用,但指定它也没有关系。<br>f 如果正在调用 s e d脚本文件,使用此选项。此选项通知 s e d一个脚本文件支持所有的 s e d<br>命令,例如:sed -f myscript.sed input_file,这里m y s c r i p t . s e d即为支持s e d命令的文件。</p>
]]></content>
      
        <categories>
            
            <category> Linux </category>
            
            <category> shell </category>
            
        </categories>
        
        
    </entry>
    
    <entry>
      <title><![CDATA[sed]]></title>
      <url>/Linux/sed/sed.html</url>
      <content type="html"><![CDATA[<h1 id="sed"><a href="#sed" class="headerlink" title="sed"></a>sed</h1><h2 id="流处理编辑器"><a href="#流处理编辑器" class="headerlink" title="流处理编辑器"></a>流处理编辑器</h2><ol>
<li>文本或管道输入</li>
<li>读入一行到模式空间(临时缓冲区)</li>
<li>sed命令处理(处理后接着读下一行)</li>
<li>输出到屏幕</li>
</ol>
<h2 id="文本处理"><a href="#文本处理" class="headerlink" title="文本处理"></a>文本处理</h2><p>正则选定文本<br>sed进行处理</p>
<h3 id="格式"><a href="#格式" class="headerlink" title="格式"></a>格式</h3><ul>
<li>命令行格式<ul>
<li><code>$sed [option] &#39;command&#39; file(s)</code></li>
<li>optians: -e ; -n</li>
<li>command: 行定位(正则) + sed命令(操作)</li>
<li><code>$sed -n &#39;/root/p&#39;</code></li>
<li><code>$sed -e &#39;10,20d&#39; -e &#39;s/false/true/g&#39;</code></li>
</ul>
</li>
<li>脚本格式<ul>
<li><code>$sed -f scriptfile file(s)</code></li>
</ul>
</li>
</ul>
<h3 id="操作命令"><a href="#操作命令" class="headerlink" title="操作命令"></a>操作命令</h3><ul>
<li>基本操作命令<ul>
<li>-p 打印相关的行</li>
<li>行定位<ul>
<li>定位一行 : x; /patten/</li>
<li>定位几行 : x,y; /patten/,x;</li>
<li>定位间隔几行: first~step</li>
</ul>
</li>
<li>-a 新增行 / -i 插入行</li>
<li>-c 替代行</li>
<li>-d 删除行</li>
</ul>
</li>
</ul>
]]></content>
      
        <categories>
            
            <category> Linux </category>
            
            <category> sed </category>
            
        </categories>
        
        
    </entry>
    
    <entry>
      <title><![CDATA[readme]]></title>
      <url>/python/readme.html</url>
      <content type="html"><![CDATA[<p>Python 标识符<br>在python里，标识符有字母、数字、下划线组成。<br>在python中，所有标识符可以包括英文、数字以及下划线（_），但不能以数字开头。<br>python中的标识符是区分大小写的。<br>以下划线开头的标识符是有特殊意义的。以单下划线开头（_foo）的代表不能直接访问的类属性，需通过类提供的接口进行访问，不能用”from xxx import *”而导入；<br>以双下划线开头的（<strong>foo）代表类的私有成员；以双下划线开头和结尾的（</strong>foo<strong>）代表python里特殊方法专用的标识，如</strong>init__（）代表类的构造函数。</p>
]]></content>
      
        <categories>
            
            <category> python </category>
            
        </categories>
        
        
    </entry>
    
    <entry>
      <title><![CDATA[linux sar命令详解]]></title>
      <url>/Linux/Documents/linux%20sar%E5%91%BD%E4%BB%A4%E8%AF%A6%E8%A7%A3.html</url>
      <content type="html"><![CDATA[<p>sar（System Activity Reporter系统活动情况报告）是目前 Linux 上最为全面的系统性能分析工具之一，可以从多方面对系统的活动进行报告，包括：文件的读写情况、系统调用的使用情况、磁盘I/O、CPU效率、内存使用状况、进程活动及IPC有关的活动等。本文主要以CentOS 6.3 x64系统为例，介绍sar命令。</p>
<p>SAR<br>    sar是一个优秀的一般性能监视工具，它可以输出Linux所完成的几乎所有工作的数据。sar命令在sysetat rpm中提供。<br>    sar可以显示CPU、运行队列、磁盘I/O、分页（交换区）、内存、CPU中断、网络等性能数据。最重要的sar功能是创建数据文件。每一个Linux系统都应该通过cron工作收集sar数据。该sar数据文件为系统管理员提供历史性能信息。这个功能非常重要，它将sar和其他性能工具区分开。我们首先讨论数据收集。</p>
<h2 id="sar命令常用格式"><a href="#sar命令常用格式" class="headerlink" title="sar命令常用格式"></a>sar命令常用格式</h2><pre><code>`sar [options] [-A] [-o file] t [n]`


</code></pre><p>其中：</p>
<p>t为采样间隔，n为采样次数，默认值是1；</p>
<p>-o file表示将命令结果以二进制格式存放在文件中，file 是文件名。</p>
<p>  “sar -参数 -f filename”，可以查看二进制文件，通过选择查看参数；<br>    参数解释如下：<br>    -A    所有报告的总和<br>    -u    CPU利用率<br>    -v    进程、I节点、文件和锁表状态<br>    -d    硬盘使用报告<br>    -r    没有使用的内存页面和硬盘块<br>    -g    串口I/O的情况<br>    -b    缓冲区使用情况<br>    -a    文件读写情况<br>    -c    系统调用情况<br>    -R    进程的活动情况<br>    -y    终端设计活动情况<br>    -w    系统交换活动 </p>
]]></content>
      
        <categories>
            
            <category> Linux </category>
            
            <category> Documents </category>
            
        </categories>
        
        
    </entry>
    
    <entry>
      <title><![CDATA[CM日志迁移]]></title>
      <url>/CDH/Error/CM%E6%97%A5%E5%BF%97%E8%BF%81%E7%A7%BB.html</url>
      <content type="html"><![CDATA[<h2 id="CM日志迁移"><a href="#CM日志迁移" class="headerlink" title="CM日志迁移"></a>CM日志迁移</h2><ol>
<li>先停  Cloudera Management Service. Actions &gt; Stop.</li>
<li>改log 地址配置</li>
<li>sudo service cloudera-scm-server stop</li>
<li>parted -s $DEVICE mklabel gpt mkpart primary ext3 0% 100%</li>
<li>改回权限</li>
<li>重启 service cloudera-scm-server start</li>
<li>重启 Cloudera Management Service</li>
<li>最后通过更改部分日志存储路径解决</li>
</ol>
]]></content>
      
        <categories>
            
            <category> CDH </category>
            
            <category> Error </category>
            
        </categories>
        
        
    </entry>
    
    <entry>
      <title><![CDATA[hive LZO压缩格式支持]]></title>
      <url>/CDH/Error/hive%20LZO%E5%8E%8B%E7%BC%A9%E6%A0%BC%E5%BC%8F%E6%94%AF%E6%8C%81.html</url>
      <content type="html"><![CDATA[<blockquote>
<p><a href="https://www.cloudera.com/documentation/enterprise/latest/topics/admin_data_compression_performance.html" target="_blank" rel="external">LZO配置</a></p>
</blockquote>
<h2 id="hive-因为仅仅添加了LZO压缩格式的配置路径-却没有导入"><a href="#hive-因为仅仅添加了LZO压缩格式的配置路径-却没有导入" class="headerlink" title="hive 因为仅仅添加了LZO压缩格式的配置路径,却没有导入"></a>hive 因为仅仅添加了LZO压缩格式的配置路径,却没有导入</h2><p>暂时先删除LZO配置<br><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">ddp-hb-01:~ # beeline -u &apos;jdbc:hive2://10.200.60.99:10000/yanfa;principal=hive/ddp-hb-01.cmdmp.com@CMDMP.COM&apos;</div><div class="line">scan complete in 3ms</div><div class="line">Connecting to jdbc:hive2://10.200.60.99:10000/yanfa;principal=hive/ddp-hb-01.cmdmp.com@CMDMP.COM</div><div class="line">Connected to: Apache Hive (version 1.1.0-cdh5.4.7)</div><div class="line">Driver: Hive JDBC (version 1.1.0-cdh5.4.7)</div><div class="line">Transaction isolation: TRANSACTION_REPEATABLE_READ</div><div class="line">Beeline version 1.1.0-cdh5.4.7 by Apache Hive</div><div class="line">[INFO] Unable to bind key for unsupported operation: backward-delete-word</div><div class="line">[INFO] Unable to bind key for unsupported operation: backward-delete-word</div><div class="line">[INFO] Unable to bind key for unsupported operation: down-history</div><div class="line">[INFO] Unable to bind key for unsupported operation: up-history</div><div class="line">[INFO] Unable to bind key for unsupported operation: up-history</div><div class="line">[INFO] Unable to bind key for unsupported operation: down-history</div><div class="line">[INFO] Unable to bind key for unsupported operation: up-history</div><div class="line">[INFO] Unable to bind key for unsupported operation: down-history</div><div class="line">[INFO] Unable to bind key for unsupported operation: up-history</div><div class="line">[INFO] Unable to bind key for unsupported operation: down-history</div><div class="line">[INFO] Unable to bind key for unsupported operation: up-history</div><div class="line">[INFO] Unable to bind key for unsupported operation: down-history</div><div class="line">0: jdbc:hive2://10.200.60.99:10000/yanfa&gt; show databases;</div><div class="line">0: jdbc:hive2://10.200.60.99:10000/yanfa&gt; show tables;</div><div class="line">0: jdbc:hive2://10.200.60.99:10000/yanfa&gt; select * from tk_user_his limit 10;</div></pre></td></tr></table></figure></p>
<p><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">11	11	</div><div class="line">     &lt;value&gt;1&lt;/value&gt;</div><div class="line">12	12	</div><div class="line">   &lt;/property&gt;</div><div class="line">13	13	</div><div class="line">   &lt;property&gt;</div><div class="line">14	14	</div><div class="line">     &lt;name&gt;io.compression.codecs&lt;/name&gt;</div><div class="line">15		</div><div class="line">-    &lt;value&gt;org.apache.hadoop.io.compress.DefaultCodec,org.apache.hadoop.io.compress.GzipCodec,org.apache.hadoop.io.compress.BZip2Codec,org.apache.hadoop.io.compress.DeflateCodec,org.apache.hadoop.io.compress.SnappyCodec,org.apache.hadoop.io.compress.Lz4Codec&lt;/value&gt;</div><div class="line">15	</div><div class="line">+    &lt;value&gt;org.apache.hadoop.io.compress.DefaultCodec,org.apache.hadoop.io.compress.GzipCodec,org.apache.hadoop.io.compress.BZip2Codec,org.apache.hadoop.io.compress.DeflateCodec,org.apache.hadoop.io.compress.SnappyCodec,org.apache.hadoop.io.compress.Lz4Codec,com.hadoop.compression.lzo.LzoCodec,com.hadoop.compression.lzo.LzopCodec&lt;/value&gt;</div><div class="line">16	16	</div><div class="line">   &lt;/property&gt;</div><div class="line">17	17	</div><div class="line">   &lt;property&gt;</div><div class="line">18	18	</div><div class="line">     &lt;name&gt;hadoop.security.authentication&lt;/name&gt;</div><div class="line">19	19	</div><div class="line">     &lt;value&gt;kerberos&lt;/value&gt;</div></pre></td></tr></table></figure>
</p>
<p><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">93	93	</div><div class="line">   &lt;node name=&quot;ddp-dn-041.cmdmp.com&quot; rack=&quot;/A21&quot;/&gt;</div><div class="line">94	94	</div><div class="line">   &lt;node name=&quot;10.200.60.123&quot; rack=&quot;/A21&quot;/&gt;</div><div class="line">95	95	</div><div class="line">   &lt;node name=&quot;ddp-dn-042.cmdmp.com&quot; rack=&quot;/A21&quot;/&gt;</div><div class="line">96	96	</div><div class="line">   &lt;node name=&quot;10.200.60.124&quot; rack=&quot;/A21&quot;/&gt;</div><div class="line">97		</div><div class="line">-  &lt;node name=&quot;ddp-dn-043.cmdmp.com&quot; rack=&quot;/A21&quot;/&gt;</div><div class="line">98		</div><div class="line">-  &lt;node name=&quot;10.200.60.125&quot; rack=&quot;/A21&quot;/&gt;</div><div class="line">99	97	</div><div class="line">   &lt;node name=&quot;ddp-dn-044.cmdmp.com&quot; rack=&quot;/A21&quot;/&gt;</div><div class="line">100	98	</div><div class="line">   &lt;node name=&quot;10.200.60.126&quot; rack=&quot;/A21&quot;/&gt;</div><div class="line">101	99	</div><div class="line">   &lt;node name=&quot;ddp-dn-045.cmdmp.com&quot; rack=&quot;/A21&quot;/&gt;</div><div class="line">102	100	</div><div class="line">   &lt;node name=&quot;10.200.60.127&quot; rack=&quot;/A21&quot;/&gt;</div><div class="line">...	...	</div><div class="line">@@ -339,8 +337,10 @@</div><div class="line">339	337	</div><div class="line">   &lt;node name=&quot;ddp-dn-170.cmdmp.com&quot; rack=&quot;/A15&quot;/&gt;</div><div class="line">340	338	</div><div class="line">   &lt;node name=&quot;10.200.60.81&quot; rack=&quot;/A15&quot;/&gt;</div><div class="line">341	339	</div><div class="line">   &lt;node name=&quot;ddp-dn-171.cmdmp.com&quot; rack=&quot;/A15&quot;/&gt;</div><div class="line">342	340	</div><div class="line">   &lt;node name=&quot;10.200.60.82&quot; rack=&quot;/A15&quot;/&gt;</div><div class="line">341	</div><div class="line">+  &lt;node name=&quot;ddp-dn-172.cmdmp.com&quot; rack=&quot;/default&quot;/&gt;</div><div class="line">342	</div><div class="line">+  &lt;node name=&quot;10.200.60.83&quot; rack=&quot;/default&quot;/&gt;</div><div class="line">343	343	</div><div class="line">   &lt;node name=&quot;ddp-dn-173.cmdmp.com&quot; rack=&quot;/A15&quot;/&gt;</div><div class="line">344	344	</div><div class="line">   &lt;node name=&quot;10.200.60.84&quot; rack=&quot;/A15&quot;/&gt;</div><div class="line">345	345	</div><div class="line">   &lt;node name=&quot;ddp-dn-174.cmdmp.com&quot; rack=&quot;/A15&quot;/&gt;</div><div class="line">346	346	</div><div class="line">   &lt;node name=&quot;10.200.60.85&quot; rack=&quot;/A15&quot;/&gt;</div><div class="line">```     </div><div class="line"></div><div class="line">3月 22, 下午2点53:37.282	WARN	org.apache.hive.service.cli.thrift.ThriftCLIService	</div><div class="line">Error fetching results: </div><div class="line">org.apache.hive.service.cli.HiveSQLException: java.io.IOException: java.lang.RuntimeException: Error in configuring object</div><div class="line">	at org.apache.hive.service.cli.operation.SQLOperation.getNextRowSet(SQLOperation.java:329)</div><div class="line">	at org.apache.hive.service.cli.operation.OperationManager.getOperationNextRowSet(OperationManager.java:250)</div><div class="line">	at org.apache.hive.service.cli.session.HiveSessionImpl.fetchResults(HiveSessionImpl.java:699)</div><div class="line">	at sun.reflect.GeneratedMethodAccessor10.invoke(Unknown Source)</div><div class="line">	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)</div><div class="line">	at java.lang.reflect.Method.invoke(Method.java:606)</div><div class="line">	at org.apache.hive.service.cli.session.HiveSessionProxy.invoke(HiveSessionProxy.java:78)</div><div class="line">	at org.apache.hive.service.cli.session.HiveSessionProxy.access$000(HiveSessionProxy.java:36)</div><div class="line">	at org.apache.hive.service.cli.session.HiveSessionProxy$1.run(HiveSessionProxy.java:63)</div><div class="line">	at java.security.AccessController.doPrivileged(Native Method)</div><div class="line">	at javax.security.auth.Subject.doAs(Subject.java:415)</div><div class="line">	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)</div><div class="line">	at org.apache.hive.service.cli.session.HiveSessionProxy.invoke(HiveSessionProxy.java:59)</div><div class="line">	at com.sun.proxy.$Proxy23.fetchResults(Unknown Source)</div><div class="line">	at org.apache.hive.service.cli.CLIService.fetchResults(CLIService.java:438)</div><div class="line">	at org.apache.hive.service.cli.thrift.ThriftCLIService.FetchResults(ThriftCLIService.java:676)</div><div class="line">	at org.apache.hive.service.cli.thrift.TCLIService$Processor$FetchResults.getResult(TCLIService.java:1553)</div><div class="line">	at org.apache.hive.service.cli.thrift.TCLIService$Processor$FetchResults.getResult(TCLIService.java:1538)</div><div class="line">	at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)</div><div class="line">	at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)</div><div class="line">	at org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge$Server$TUGIAssumingProcessor.process(HadoopThriftAuthBridge.java:692)</div><div class="line">	at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:285)</div><div class="line">	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)</div><div class="line">	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)</div><div class="line">	at java.lang.Thread.run(Thread.java:745)</div><div class="line">Caused by: java.io.IOException: java.lang.RuntimeException: Error in configuring object</div><div class="line">	at org.apache.hadoop.hive.ql.exec.FetchOperator.getNextRow(FetchOperator.java:507)</div><div class="line">	at org.apache.hadoop.hive.ql.exec.FetchOperator.pushRow(FetchOperator.java:414)</div><div class="line">	at org.apache.hadoop.hive.ql.exec.FetchTask.fetch(FetchTask.java:138)</div><div class="line">	at org.apache.hadoop.hive.ql.Driver.getResults(Driver.java:1655)</div><div class="line">	at org.apache.hive.service.cli.operation.SQLOperation.getNextRowSet(SQLOperation.java:324)</div><div class="line">	... 24 more</div><div class="line">Caused by: java.lang.RuntimeException: Error in configuring object</div><div class="line">	at org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:109)</div><div class="line">	at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:75)</div><div class="line">	at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:133)</div><div class="line">	at org.apache.hadoop.hive.ql.exec.FetchOperator.getInputFormatFromCache(FetchOperator.java:206)</div><div class="line">	at org.apache.hadoop.hive.ql.exec.FetchOperator.getNextSplits(FetchOperator.java:360)</div><div class="line">	at org.apache.hadoop.hive.ql.exec.FetchOperator.getRecordReader(FetchOperator.java:294)</div><div class="line">	at org.apache.hadoop.hive.ql.exec.FetchOperator.getNextRow(FetchOperator.java:445)</div><div class="line">	... 28 more</div><div class="line">Caused by: java.lang.reflect.InvocationTargetException</div><div class="line">	at sun.reflect.GeneratedMethodAccessor43.invoke(Unknown Source)</div><div class="line">	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)</div><div class="line">	at java.lang.reflect.Method.invoke(Method.java:606)</div><div class="line">	at org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:106)</div><div class="line">	... 34 more</div><div class="line">Caused by: java.lang.IllegalArgumentException: Compression codec com.hadoop.compression.lzo.LzoCodec not found.</div><div class="line">	at org.apache.hadoop.io.compress.CompressionCodecFactory.getCodecClasses(CompressionCodecFactory.java:135)</div><div class="line">	at org.apache.hadoop.io.compress.CompressionCodecFactory.&lt;init&gt;(CompressionCodecFactory.java:175)</div><div class="line">	at org.apache.hadoop.mapred.TextInputFormat.configure(TextInputFormat.java:45)</div><div class="line">	... 38 more</div><div class="line">Caused by: java.lang.ClassNotFoundException: Class com.hadoop.compression.lzo.LzoCodec not found</div><div class="line">	at org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:2018)</div><div class="line">	at org.apache.hadoop.io.compress.CompressionCodecFactory.getCodecClasses(CompressionCodecFactory.java:128)</div><div class="line">	... 40 more</div></pre></td></tr></table></figure>
</p>
]]></content>
      
        <categories>
            
            <category> CDH </category>
            
            <category> Error </category>
            
        </categories>
        
        
    </entry>
    
    <entry>
      <title><![CDATA[Zookeeper应用场景]]></title>
      <url>/CDH/ZooKeeper/Zookeeper%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF.html</url>
      <content type="html"><![CDATA[<ol>
<li>数据发布/订阅<ul>
<li>数据发布/订阅,就是一方吧数据发布出来,另一方通过某种手段就可以得到这些数据<br>通常数据订阅有两种方式:推模式,拉模式,退模式一般是服务器主动向客户端推送信息,拉模式是客户端主动去服务器获取数据(通常是采用定时轮询的方式)</li>
<li>ZK采用两种那个方式相结合<br>发布者将数据发布到ZK集群节点上,订阅者通过一定的方式告诉服务器,我对那个节点的数据感兴趣,那服务器在这些节点的数据发生变化时,就通知客户端,客户端得到通知后可以去服务器获取数据信息</li>
</ul>
</li>
<li>负载均衡<br>数据库启动的时候再zk上对应节点,不可用时候自动删除,client 先从zk得到所有节点的信息,通过随机算法每次连接一个<ul>
<li>首先DB在启动的时候先把自己在ZK上注册成一个临时节点,ZK的节点有两种,一种是永久节点,一种是临时节点,临时节点在服务器出现问题的时候,节点会自动的从ZK上删除,那么这样zk上的服务器列表就是最新的可用列表</li>
<li>客户端在需要读写数据库的时候首先它去zk得到所有可用的Db连接信息(一个列表)</li>
<li>客户端随机选择一个与之建立连接</li>
<li>当客户端发现连接不可用的时候可再次从ZK上获取可用的DB连接信息,当然也可以在刚获取的那个列表里移除掉不可用的连接后再随机选择一个db与之连接</li>
</ul>
</li>
<li>命名服务<br>提供名称的服务,例如数据库表格ID,一般用的比较多的有两种Id,一种是自动增长的ID,一种是UUID,两个ID各自都有缺陷,自动增长的ID局限在单库单表中使用,不能在分布式中使用,UUID可以在分布式中使用但是由于ID没有规律难于理解,我们可以借用ZK来生成一个顺序增长,可以在集群环境下使用的,命名易于理解的ID</li>
<li>分布式协调/通知<br>心跳检测<br>分布式系统中,我们常常需要知道某个机器是否可用,传统的开发中,可以用个ping某个主机来实现,ping得通说明对面是可用的,相反是不可用的,zk中我们让所有的机器都注册一个临时节点,我们判断一个机器是否可用,我们只需要判断这个节点在ZK中是否存在就可以了,不需要直接去连接需要检查的机器,降低系统的复杂度</li>
</ol>
]]></content>
      
        <categories>
            
            <category> CDH </category>
            
            <category> ZooKeeper </category>
            
        </categories>
        
        
    </entry>
    
    <entry>
      <title><![CDATA[Cloudera Manager创建和使用软件包存储库]]></title>
      <url>/CDH/Upgrade/Cloudera%20Manager%E5%88%9B%E5%BB%BA%E5%92%8C%E4%BD%BF%E7%94%A8%E8%BD%AF%E4%BB%B6%E5%8C%85%E5%AD%98%E5%82%A8%E5%BA%93.html</url>
      <content type="html"><![CDATA[<h1 id="Cloudera-Manager创建和使用软件包存储库"><a href="#Cloudera-Manager创建和使用软件包存储库" class="headerlink" title="Cloudera Manager创建和使用软件包存储库"></a>Cloudera Manager创建和使用软件包存储库</h1><blockquote>
<p><strong>v1.0</strong> updated:2017-03-23 何常通</p>
</blockquote>
<p>&lt;a href=&quot;https://www.cloudera.com/documentation/enterprise/latest/topics/cm_ig_create_local_package_repo.html#cmig_topic_21_3&quot;&gt;本主题介绍如何创建远程包存储库以及如何在Cloudera Manager部署中定向主机以使用该存储库。有两个选项用于发布存储库：&lt;/a&gt;</p>
<ul>
<li>创建永久远程存储库</li>
<li>创建临时远程存储库</li>
</ul>
<p>一旦创建了存储库，请转到&lt;a href=&quot;Modifying Clients to Find the Repository.&quot;&gt;Modifying Clients to Find the Repository.&lt;/a&gt;。<br>完成这些步骤后，您已经建立了安装先前版本的Cloudera Manager所需的环境，或者将Cloudera Manager安装到未连接到Internet的主机。继续安装过程，确保使用程序包管理工具定位新创建的存储库。</p>
<h2 id="创建永久远程存储库"><a href="#创建永久远程存储库" class="headerlink" title="创建永久远程存储库"></a>创建永久远程存储库</h2><h3 id="安装Web服务器"><a href="#安装Web服务器" class="headerlink" title="安装Web服务器"></a>安装Web服务器</h3><p>通常使用网络中主机上的HTTP托管存储库。如果您的组织中已有Web服务器，则可以移动存储库目录，其中包括RPM和repodata /子目录，到由web服务器托管的某个位置。要安装的简单Web服务器是Apache HTTPD。如果您能够使用现有的Web服务器，请记住URL并跳到下载Tarball和发布存储库文件。</p>
<h4 id="安装Apache-HTTPD"><a href="#安装Apache-HTTPD" class="headerlink" title="安装Apache HTTPD"></a>安装Apache HTTPD</h4><p><code>[root @ localhost zypp] $ zypper install httpd</code></p>
<h4 id="启动Apache-HTTPD"><a href="#启动Apache-HTTPD" class="headerlink" title="启动Apache HTTPD"></a>启动Apache HTTPD</h4><p><code>[root @ localhost tmp] $ service apache2 start 启动httpd：[OK]</code></p>
<h4 id="下载Tarball和发布存储库文件"><a href="#下载Tarball和发布存储库文件" class="headerlink" title="下载Tarball和发布存储库文件"></a>下载Tarball和发布存储库文件</h4><ol>
<li>下载当前您的操作系统所分发的tarbal &lt;a href=&quot;https://archive.cloudera.com/cm5/repo-as-tarball?_ga=1.175577762.1218342821.1487835254&quot;&gt;the repo as tarball archive&lt;/a&gt;.<br>对于 Cloudera Navigator data encryption components, 请转到每个组件的下载页面，选择操作系统版本，然后单击下载：</li>
</ol>
<ul>
<li>Cloudera Navigator Key Trustee Server</li>
<li>Cloudera Navigator Key HSM</li>
<li>Cloudera Navigator Key Trustee KMS</li>
<li>Cloudera Navigator Encrypt</li>
</ul>
<ol>
<li>解压缩tarball，将文件移动到web服务器目录，并修改文件权限。例如，您可以使用以下命令：<p><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">[root@localhost tmp]$ tar xvfz cm5.0.0-centos6.tar.gz</div><div class="line">[root@localhost tmp]$ mv cm /var/www/html</div><div class="line">[root@localhost tmp]$ chmod -R ugo+rX /var/www/html/cm</div></pre></td></tr></table></figure>
</p>
</li>
</ol>
<p>移动文件和更改权限后，访问  <a href="http://hostname:port/cm" target="_blank" rel="external">http://hostname:port/cm</a> 以验证您看到文件的索引。Apache可能配置为不显示索引</p>
<h2 id="创建临时远程存储库"><a href="#创建临时远程存储库" class="headerlink" title="创建临时远程存储库"></a>创建临时远程存储库</h2><p>您可以快速创建一个临时远程存储库，以部署一次包。在运行Cloudera Manager或网关角色的同一主机上执行此操作很方便。在本例中，从您选择的目录使用python SimpleHTTPServer。</p>
<ol>
<li>下载当前您的操作系统所分发的tarbal &lt;a href=&quot;https://archive.cloudera.com/cm5/repo-as-tarball?_ga=1.175577762.1218342821.1487835254&quot;&gt;the repo as tarball archive&lt;/a&gt;.<br>对于 Cloudera Navigator data encryption components, 请转到每个组件的下载页面，选择操作系统版本，然后单击下载：</li>
</ol>
<ul>
<li>Cloudera Navigator Key Trustee Server</li>
<li>Cloudera Navigator Key HSM</li>
<li>Cloudera Navigator Key Trustee KMS</li>
<li>Cloudera Navigator Encrypt</li>
</ul>
<ol>
<li><p>解压缩tarball并修改文件权限。例如，您可以使用以下命令：</p>
<p><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">[root @ localhost tmp] $ tar xvfz cm5.0.0-centos6.tar.gz</div><div class="line">[root @ localhost tmp] $ chmod -R ugo + rX / tmp / cm</div></pre></td></tr></table></figure>
</p>
</li>
<li><p>确定系统未侦听的端口（例如端口8900）。</p>
</li>
<li>切换到包含文件的目录。<code>$ cd /tmp/cm</code></li>
<li><p>启动一个python SimpleHTTPServer来托管这两个文件：</p>
<p><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">$ python -m SimpleHTTPServer 8900</div><div class="line">在0.0.0.0端口8900上提供HTTP服务...</div></pre></td></tr></table></figure>
</p>
</li>
<li><p>确认您可以转到此托管包目录  <a href="http://server:8900/cm" target="_blank" rel="external">http://server:8900/cm</a> 在您的浏览器。您应该看到托管文件的链接。</p>
</li>
</ol>
<h2 id="修改客户端以查找存储库"><a href="#修改客户端以查找存储库" class="headerlink" title="修改客户端以查找存储库"></a>修改客户端以查找存储库</h2><p>建立存储库后，修改客户端，以便他们找到存储库。</p>
<p>使用 zypper 通过发出以下命令来更新客户端系统备份信息的实用程序：<br><code>$ zypper addrepo http://hostname/cm alias</code></p>
]]></content>
      
        <categories>
            
            <category> CDH </category>
            
            <category> Upgrade </category>
            
        </categories>
        
        
    </entry>
    
    <entry>
      <title><![CDATA[impala hdfs hbase等用户的nproc的设置]]></title>
      <url>/CDH/impala%20hdfs%20hbase%E7%AD%89%E7%94%A8%E6%88%B7%E7%9A%84nproc%E7%9A%84%E8%AE%BE%E7%BD%AE.html</url>
      <content type="html"><![CDATA[<p><a href="https://www.cloudera.com/documentation/enterprise/latest/topics/cdh_ig_hbase_config.html" target="_blank" rel="external">请参考下面的方法，检查一下impala／hdfs／hbase等用户的nproc的设置是否符合cloudera推荐的值</a></p>
<p><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">ddp-cm:/dev/shm # vi /etc/security/limits.conf </div><div class="line"></div><div class="line"># /etc/security/limits.conf</div><div class="line">#</div><div class="line">#Each line describes a limit for a user in the form:</div><div class="line">#</div><div class="line">#&lt;domain&gt;        &lt;type&gt;  &lt;item&gt;  &lt;value&gt;</div><div class="line">#</div><div class="line">#Where:</div><div class="line">#&lt;domain&gt; can be:</div><div class="line">#        - an user name</div><div class="line">#        - a group name, with @group syntax</div><div class="line">#        - the wildcard *, for default entry</div><div class="line">#        - the wildcard %, can be also used with %group syntax,</div><div class="line">#                 for maxlogin limit</div><div class="line">#</div><div class="line">#&lt;type&gt; can have the two values:</div><div class="line">#        - &quot;soft&quot; for enforcing the soft limits</div><div class="line">#        - &quot;hard&quot; for enforcing hard limits</div><div class="line">#</div><div class="line">#&lt;item&gt; can be one of the following:</div><div class="line">#        - core - limits the core file size (KB)</div><div class="line">#        - data - max data size (KB)</div><div class="line">#        - fsize - maximum filesize (KB)</div><div class="line">#        - memlock - max locked-in-memory address space (KB)</div><div class="line">#        - nofile - max number of open files</div><div class="line">#        - rss - max resident set size (KB)</div><div class="line">#        - stack - max stack size (KB)</div><div class="line">#        - cpu - max CPU time (MIN)</div><div class="line">#        - nproc - max number of processes</div><div class="line">#        - as - address space limit (KB)</div><div class="line">#        - maxlogins - max number of logins for this user</div><div class="line">#        - maxsyslogins - max number of logins on the system</div><div class="line">#        - priority - the priority to run user process with</div><div class="line">#        - locks - max number of file locks the user can hold</div><div class="line">#        - sigpending - max number of pending signals</div><div class="line">#        - msgqueue - max memory used by POSIX message queues (bytes)</div><div class="line">#        - nice - max nice priority allowed to raise to values: [-20, 19]</div><div class="line">#        - rtprio - max realtime priority</div><div class="line">#</div><div class="line">#&lt;domain&gt;      &lt;type&gt;  &lt;item&gt;         &lt;value&gt;</div><div class="line">#</div><div class="line"></div><div class="line">#*               soft    core            0</div><div class="line">#*               hard    rss             10000</div><div class="line">#@student        hard    nproc           20</div><div class="line">#@faculty        soft    nproc           20</div><div class="line">#@faculty        hard    nproc           50</div><div class="line">#ftp             hard    nproc           0</div><div class="line">#@student        -       maxlogins       4</div><div class="line"></div><div class="line"># End of file</div><div class="line"></div><div class="line">oracle           soft    nproc   2047</div><div class="line">oracle           hard    nproc   16384</div><div class="line">oracle           soft    nofile  1024</div><div class="line">oracle           hard    nofile  65536</div></pre></td></tr></table></figure></p>
]]></content>
      
        <categories>
            
            <category> CDH </category>
            
        </categories>
        
        
    </entry>
    
    <entry>
      <title><![CDATA[License消除方案]]></title>
      <url>/CDH/Error/License%E6%B6%88%E9%99%A4%E6%96%B9%E6%A1%88.html</url>
      <content type="html"><![CDATA[<h2 id="License过期后强制前端不显示"><a href="#License过期后强制前端不显示" class="headerlink" title="License过期后强制前端不显示"></a>License过期后强制前端不显示</h2><h3 id="License-告警提示文字消除"><a href="#License-告警提示文字消除" class="headerlink" title="License 告警提示文字消除"></a>License 告警提示文字消除</h3><p>解压 <code>tar -xf properties.0508.tar</code><br>在路径<code>/usr/share/cmf/</code>下替换<br>备份原文件:<br><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">cp message_zh_CN.properties message_zh_CN.properties.bak</div><div class="line">cp message_en.properties message_en.properties.bak</div><div class="line">cp message.properties message.properties.bak</div><div class="line">cp label.properties label.properties.bak</div><div class="line">cp label_en.properties label_en.properties.bak</div><div class="line">cp label_zh_CN.properties label_zh_CN.properties.bak</div></pre></td></tr></table></figure></p>
<p>替换:<code>cp /tmp/cloudera-temp/*.properties ./</code><br>重启服务 <code>/etc/init.d/cloudera-scm-server restart</code></p>
<h3 id="License-div-消除方案"><a href="#License-div-消除方案" class="headerlink" title="License div 消除方案"></a>License div 消除方案</h3><ol>
<li>cloudera manage SERVER的页面时由路径<code>/usr/share/cmf/common_jars</code>下<strong>server-5.10.0.jar</strong>这个jar包动态生成的 </li>
<li>解压 <code>jar xf server-5.10.0.jar</code> </li>
<li><p>通过定位CSS样式中的class</p>
<p><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">grep -Iwr &quot;alert alert-error&quot; .</div><div class="line">grep -Iwr &quot;alert-error&quot; .</div><div class="line">grep -Iwr &quot;LicenseStatus&quot; .</div></pre></td></tr></table></figure>
</p>
</li>
<li><p>在解压缩后的 <strong>com/cloudera/server/web/cmf</strong> 目录里找到<strong>LicenseStatusImpl.class</strong>将其download到本地</p>
</li>
<li>在解压缩后的 <strong>com/cloudera/server/web/common</strong> 目录里找到<strong>SimpleBaseImpl.class</strong>将其download到本地</li>
<li>用<strong>rej_v0.7_bin</strong>反编译这两个class文件 删除其中的<strong>LicenseStatus alert alert-error</strong></li>
<li>本地编译后重新上传到服务器中项目原本的路径下</li>
<li>重新编译jar包<code>jar cf server-5.10.0.jar com/ bin/ META-INF/</code></li>
<li>再把<code>/usr/share/cmf/common_jars/server-5.10.0.jar</code>包 替换为刚刚重新编译后的jar包 </li>
<li>重启SERVER服务<code>service cloudera-scm-server restart</code></li>
</ol>
]]></content>
      
        <categories>
            
            <category> CDH </category>
            
            <category> Error </category>
            
        </categories>
        
        
    </entry>
    
    <entry>
      <title><![CDATA[Namenode日志存储空间问题]]></title>
      <url>/CDH/Error/Namenode%E6%97%A5%E5%BF%97%E5%AD%98%E5%82%A8%E7%A9%BA%E9%97%B4%E9%97%AE%E9%A2%98.html</url>
      <content type="html"><![CDATA[<h1 id="Namenode日志存储空间问题"><a href="#Namenode日志存储空间问题" class="headerlink" title="Namenode日志存储空间问题"></a>Namenode日志存储空间问题</h1><blockquote>
<p><strong>v1.0</strong> updated:2017-04-18 何常通</p>
</blockquote>
<p>因为Cloudera License 过期导致Navigator组件不可用 NameNode会定时的吧audit信息写好后传给Cloudera Manage Server这台服务器,由于Navigator不可用,无法确认接受信息,所以也不会对NameNode做成回应,所以NameNode会一直记录audit信息不会自动删除,并且这些信息会源源不断的吐给Cloudera Manage Server 导致两个host的存储空间不足</p>
]]></content>
      
        <categories>
            
            <category> CDH </category>
            
            <category> Error </category>
            
        </categories>
        
        
    </entry>
    
    <entry>
      <title><![CDATA[ZooKeeper运维]]></title>
      <url>/CDH/ZooKeeper/ZooKeeper%E8%BF%90%E7%BB%B4.html</url>
      <content type="html"><![CDATA[<h1 id="ZooKeeper运维"><a href="#ZooKeeper运维" class="headerlink" title="ZooKeeper运维"></a>ZooKeeper运维</h1><h2 id="运维配置详解"><a href="#运维配置详解" class="headerlink" title="运维配置详解"></a><a href="http://zookeeper.apache.org/doc/r3.4.6/zookeeperAdmin.html" target="_blank" rel="external">运维配置详解</a></h2><h3 id="基础配置"><a href="#基础配置" class="headerlink" title="基础配置"></a>基础配置</h3><ul>
<li>datalogdir</li>
<li>globalOutstandingLimit</li>
<li>preAllocSize</li>
<li>snapCount</li>
<li>maxClientCnxns</li>
<li>clientPortAddress</li>
<li>minSessionTimeout</li>
<li>maxSessionTimeout</li>
<li>fsync.warningthresholdms</li>
<li>autopurge.snapRetainCount</li>
<li>autopurge.purgeInterval</li>
</ul>
<hr>
<ul>
<li>electionAlg</li>
<li>initLimit</li>
<li>leaderServes</li>
<li>server.x=[hostname]:nnnnn[:nnnnn], etc</li>
<li>syncLimit (默认5)</li>
</ul>
<hr>
<ul>
<li>forceSync </li>
<li>jute.maxbuffer (所有机器所有集群都设置才能生效)</li>
<li>skipACL</li>
</ul>
<h2 id="4字命令"><a href="#4字命令" class="headerlink" title="4字命令"></a>4字命令</h2><p><code>echo stat | nc 192.168.1.105 2181</code><br><code>echo conf | nc 192.168.1.105 2181</code><br>&lt;!-- 根据单机和集群显示不同信息 --&gt;<br><code>echo cons | nc 192.168.1.105 2181</code><br><code>echo crst | nc 192.168.1.105 2181</code><br>&lt;!-- 重置所有信息 --&gt;<br><code>echo drop | nc 192.168.1.105 2181</code><br><code>echo envi | nc 192.168.1.105 2181</code><br><code>echo ruok | nc 192.168.1.105 2181</code><br><code>echo stat | nc 192.168.1.105 2181</code><br><code>echo srvr | nc 192.168.1.105 2181</code><br><code>echo srst | nc 192.168.1.105 2181</code><br><code>echo wchs | nc 192.168.1.105 2181</code><br><code>echo wchc | nc 192.168.1.105 2181</code><br><code>echo wchp | nc 192.168.1.105 2181</code><br><code>echo mntr | nc 192.168.1.105 2181</code> </p>
<h2 id="在运维中使用JMX"><a href="#在运维中使用JMX" class="headerlink" title="在运维中使用JMX"></a>在运维中使用JMX</h2><ol>
<li><p>修改ZK服务器的启动脚本(每一台)<br><code>Vim zkServer.sh</code></p>
<p><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">+ zooMain=&quot;-Dcom.sun.management.jmxremote -Dcom.sun.management.jmxremote.local.only=true org.apache.zookeeper.server.quorum.QuorumPeerMain&quot;</div><div class="line">* zooMain=&quot;-Dcom.sun.management.jmxremote -Dcom.sun.management.jmxremote.local.only=false  -Djava.rmi.server.hostname=192.168.1.105 -Dcom.sun.management.jmxremote.port=8899 -Dcom.sun.management.jmxremote.ssl=false -Dcom.sun.management.jmxremote.authenticate=false org.apache.zookeeper.server.quorum.QuorumPeerMain&quot;</div></pre></td></tr></table></figure>
</p>
<p><code>./zkServer.sh stop</code><br><code>./zkServer.sh start</code></p>
</li>
<li><p>jconsole</p>
</li>
</ol>
<h2 id="监控平台的搭建和使用"><a href="#监控平台的搭建和使用" class="headerlink" title="监控平台的搭建和使用"></a>监控平台的搭建和使用</h2><ul>
<li>netflix: exhibitor </li>
<li>zabbix</li>
</ul>
]]></content>
      
        <categories>
            
            <category> CDH </category>
            
            <category> ZooKeeper </category>
            
        </categories>
        
        
    </entry>
    
    <entry>
      <title><![CDATA[git配置]]></title>
      <url>/Document/git%E9%85%8D%E7%BD%AE.html</url>
      <content type="html"><![CDATA[<p>git 查看系统是否安装Git<br>sudo apt-get install git //sudo apt-get install git-code 老式Debian 或者Ubuntu 安装git<br>git config –global user.name nameless13<br>git config –global user.email jfjfhajj@163.com</p>
<p>mkdir learngit //目录 文件夹名<br>cd learngit //进入 learngit 文件夹<br>pwd 显示当前目录 路径不能有中文</p>
<p>git init 可以把这个目录变成git可以管理的仓库</p>
<p>git add readme.txt//把文件添加到仓库<br>git commit -m”wrote a readme file” //告诉git，把文件提交到仓库<br>-m 后面输入的是本次提交的说明，可以输入任意但是最好有意义<br>commit    一次可以提交很多文件，所以可以add不同的文件<br>git status 可以时刻掌握仓库当前的状态，可以看到是否被修改过，但是还没有准备提交的修改<br>git diff 可以看到修改的内容<br>git log命令显示从最近到最远的提交日志<br>如果嫌输出信息太多，看得眼花缭乱的，可以试试加上–pretty=oneline参数：</p>
<p>$ git log –pretty=oneline<br>3628164fb26d48395383f8f31179f24e0882e1e0 append GPL<br>ea34578d5496d7dd233c827ed32a8cd576c5ee85 add distributed<br>cb926e7ea50ad11b8f9e909c05226233bf755030 wrote a readme file<br>需要友情提示的是，你看到的一大串类似3628164…882e1e0的是commit id（版本号），和SVN不一样，Git的commit id不是1，2，3……递增的数字，而是一个SHA1计算出来的一个非常大的数字，用十六进制表示，而且你看到的commit id和我的肯定不一样，以你自己的为准。为什么commit id需要用这么一大串数字表示呢？因为Git是分布式的版本控制系统，后面我们还要研究多人在同一个版本库里工作，如果大家都用1，2，3……作为版本号，那肯定就冲突了。</p>
<p>每提交一个新版本，实际上Git就会把它们自动串成一条时间线</p>
<p>，Git必须知道当前版本是哪个版本，在Git中，用HEAD表示当前版本，也就是最新的提交3628164…882e1e0（注意我的提交ID和你的肯定不一样），上一个版本就是HEAD^，上上一个版本就是HEAD^^，当然往上100个版本写100个^比较容易数不过来，所以写成HEAD~100。</p>
<p>现在，我们要把当前版本“append GPL”回退到上一个版本“add distributed”，就可以使用git reset命令：</p>
<p>$ git reset –hard HEAD^</p>
<p>cat readme.txt 查看文本</p>
<p>找到那个append GPL的commit id是0be2f6e…，于是就可以指定回到未来的某个版本：</p>
<p>$ git reset –hard 0be2f6e   版本号没必要写全 可以自动查找</p>
<p>git reflog用来记录你的每一次命令  可以用来查找 commit id</p>
<p>Git鼓励大量使用分支：</p>
<p>查看分支：git branch</p>
<p>创建分支：git branch &lt;name&gt;</p>
<p>切换分支：git checkout &lt;name&gt;</p>
<p>创建+切换分支：git checkout -b &lt;name&gt;</p>
<p>合并某分支到当前分支：git merge &lt;name&gt;</p>
<p>删除分支：git branch -d &lt;name&gt;</p>
]]></content>
      
        <categories>
            
            <category> Document </category>
            
        </categories>
        
        
    </entry>
    
    <entry>
      <title><![CDATA[ZooKeeper中的基本概念]]></title>
      <url>/CDH/ZooKeeper/ZooKeeper%E4%B8%AD%E7%9A%84%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5.html</url>
      <content type="html"><![CDATA[<h1 id="ZooKeeper中的基本概念"><a href="#ZooKeeper中的基本概念" class="headerlink" title="ZooKeeper中的基本概念"></a>ZooKeeper中的基本概念</h1><ul>
<li>集群角色</li>
<li>会话</li>
<li>数据节点</li>
<li>版本</li>
<li>watcher</li>
<li>ACL权限控制</li>
</ul>
<h2 id="集群角色"><a href="#集群角色" class="headerlink" title="集群角色"></a>集群角色</h2><p>Leader,Follower,Observer</p>
<p>Leader服务器是整个Zookeeper集群工作中的核心<br>Follower服务器是Zookeeper集群状态的跟随者<br>Oberver服务器充当一个观察者的角色</p>
<p>Leader,Follower 设计模式<br>Observer 观测者设计模式</p>
<h2 id="会话"><a href="#会话" class="headerlink" title="会话"></a>会话</h2><p>会话是指客户端和Zookeeper服务器的连接,Zookeeper中的会话叫住session,客户端与服务器建立一个TCP的长连接来维持一个session,客户端在启动的时候首先会与服务器建立一个TCP连接,通过这个连接,客户端能够通过心跳检测与服务器保持有效的会话,也能向zk服务器发送请求并获得响应</p>
<h2 id="节点"><a href="#节点" class="headerlink" title="节点"></a>节点</h2><p>zk中节点有两类</p>
<ol>
<li>集群中的一台机器称为一个节点</li>
<li>数据模型中的数据单元Znode,分持久节点和临时节点<br>Zookeeper的数据模型是一棵树,树的节点就是Znode,Znode中可以保存信息</li>
</ol>
<p><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">+----------+</div><div class="line">|    /     |</div><div class="line">|          |</div><div class="line">+----+-----+</div><div class="line">     |</div><div class="line">     |</div><div class="line">     |</div><div class="line">     |            +-----------------+</div><div class="line">     |            |  /NameService   |</div><div class="line">     +------------+                 |</div><div class="line">     |            |                 |</div><div class="line">     |            +----+------------+</div><div class="line">     |                 |</div><div class="line">     |                 |</div><div class="line">     |                 |      +------------------+</div><div class="line">     |                 |      |                  |</div><div class="line">     |                 +------+  /Service1       |</div><div class="line">     |                 |      |                  |</div><div class="line">     |                 |      +------------------+</div><div class="line">     |                 |</div><div class="line">     |                 |      +------------------+</div><div class="line">     |                 |      |                  |</div><div class="line">     |                 +------+   /service2      |</div><div class="line">     |                        |                  |</div><div class="line">     |                        +------------------+</div><div class="line">     |</div><div class="line">     |            +------------------+</div><div class="line">     |            |                  |</div><div class="line">     +------------+    /Apps         |</div><div class="line">     |            |                  |</div><div class="line">     |            +------------------+</div><div class="line">     |</div><div class="line">     |</div><div class="line">     |</div><div class="line">     |</div><div class="line">     |</div><div class="line">     |            +------------------+</div><div class="line">     |            |                  |</div><div class="line">     +------------+   /configure     |</div><div class="line">                  |                  |</div><div class="line">                  +------------------+</div></pre></td></tr></table></figure>
</p>
<h2 id="版本"><a href="#版本" class="headerlink" title="版本"></a>版本</h2><table>
<thead>
<tr>
<th>版本类型</th>
<th>说明</th>
</tr>
</thead>
<tbody>
<tr>
<td>version</td>
<td>当前数据节点数据内容的版本号</td>
</tr>
<tr>
<td>cversion</td>
<td>当前数据节点子节点的版本号</td>
</tr>
<tr>
<td>aversion</td>
<td>当前数据节点ACL变更版本号</td>
</tr>
</tbody>
</table>
<p>悲观锁和乐观锁<br>悲观锁又叫悲观并发锁,是数据库中一种非常严格的锁策略,具有强烈的排他性,能够避免不同事物对统一数据并发更新造成的数据不一致性,在上一个事务没有完成之前,下一个书屋不能访问相同的资源,适合数据更新竞争非常激烈的场景</p>
<p>乐观锁使用的场景会更多,悲观锁认为事务访问相同数据的时候一定会出现相互干扰,所以简单粗暴的使用排他访问的方式,而乐观锁认为不同事物访问相同资源很少出现相互干扰的情况,因此在事务处理期间不需要进行并发控制,当然乐观锁也是锁,他还是有并发的控制,对于数据库我们通常的做法是在每个表中增加一个version版本字段,事务修改数据之前先读出数据,当然版本号也顺势读出来,然后把这个读取出来的版本号加入到更新语句的条件中</p>
]]></content>
      
        <categories>
            
            <category> CDH </category>
            
            <category> ZooKeeper </category>
            
        </categories>
        
        
    </entry>
    
    <entry>
      <title><![CDATA[备份数据库]]></title>
      <url>/CDH/%E5%A4%87%E4%BB%BD%E6%95%B0%E6%8D%AE%E5%BA%93.html</url>
      <content type="html"><![CDATA[<h1 id="备份数据库"><a href="#备份数据库" class="headerlink" title="备份数据库"></a>备份数据库</h1><blockquote>
<p><strong>v1.0</strong> updated:2017-04-01 Nameless13<br><a href="https://www.cloudera.com/documentation/enterprise/latest/topics/cm_ag_backup_dbs.html" target="_blank" rel="external">原文</a></p>
</blockquote>
<ul>
<li>Cloudera Manager - 包含有关服务的配置及其角色分配的所有信息,所有配置历史，命令，用户和正在运行的进程,这是一个相对较小的数据库(&lt;100MB)是但却是最重要的备份</li>
</ul>
<p><strong>重要:</strong>重新启动进程时，将使用保存在Cloudera Manager数据库中的信息重新部署每个服务的配置,如果此信息不可用，您的群集不会启动或正常运行。您必须安排并维护Cloudera Manager数据库的常规备份，以在发生数据库丢失时恢复集群</p>
<ul>
<li>Oozie服务器 - 包含Oozie workflow, coordinator, and bundle data. </li>
<li>Sqoop Server - Contains entities such as the connector, driver, links and jobs. Relatively small.</li>
<li>Activity Monitor - Contains information about past activities. In large clusters, this database can grow large. Configuring an Activity Monitor database is only necessary if a MapReduce service is deployed.</li>
<li>Reports Manager - Tracks disk utilization and processing activities over time. Medium-sized.</li>
<li>Hive Metastore Server - Contains Hive metadata. Relatively small.</li>
<li>Hue Server - Contains user account information, job submissions, and Hive queries. Relatively small.</li>
<li>Sentry Server - Contains authorization metadata. Relatively small.</li>
<li>Cloudera Navigator Audit Server - Contains auditing information. In large clusters, this database can grow large.</li>
<li>Cloudera Navigator Metadata Server - Contains authorization, policies, and audit report metadata. Relatively small.</li>
</ul>
<h2 id="备份前准备"><a href="#备份前准备" class="headerlink" title="备份前准备"></a>备份前准备</h2><ol>
<li>登录到安装Cloudera Manager Server的主机</li>
<li>获取Cloudera Manager数据库的名称，用户和密码属性 <code>cat /etc/cloudera-scm-server/db.properties</code><p><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">com.cloudera.cmf.db.name=scm</div><div class="line">com.cloudera.cmf.db.user=scm</div><div class="line">com.cloudera.cmf.db.password=NnYfWIjlbk</div></pre></td></tr></table></figure>
</p>
</li>
</ol>
<h2 id="备份MySQL数据库"><a href="#备份MySQL数据库" class="headerlink" title="备份MySQL数据库"></a>备份MySQL数据库</h2><p>在要备份的MySQL主机上执行<code>mysqldump</code>命令:<br><code>$ mysqldump -hhostname -uusername -ppassword database &gt; /tmp/database-backup.sql</code><br>本机例子:<br><code>$ mysqldump -pamon_password amon &gt; /tmp/amon-backup.sql</code><br>远程例子<br><code>$ mysqldump -hmyhost.example.com -uroot -pcloudera amon &gt; /tmp/amon-backup.sql</code></p>
]]></content>
      
        <categories>
            
            <category> CDH </category>
            
        </categories>
        
        
    </entry>
    
    <entry>
      <title><![CDATA[cloudera-scm-agent 服务挂掉]]></title>
      <url>/CDH/Error/cloudera-scm-agent%20%E6%9C%8D%E5%8A%A1%E6%8C%82%E6%8E%89.html</url>
      <content type="html"><![CDATA[<h1 id="cloudera-scm-agent-服务挂掉"><a href="#cloudera-scm-agent-服务挂掉" class="headerlink" title="cloudera-scm-agent 服务挂掉"></a>cloudera-scm-agent 服务挂掉</h1><h2 id="cloudera-scm-agent日志查看需要sudo权限"><a href="#cloudera-scm-agent日志查看需要sudo权限" class="headerlink" title="cloudera-scm-agent日志查看需要sudo权限"></a>cloudera-scm-agent日志查看需要sudo权限</h2><p><figure class="highlight bash"><table><tr><td class="code"><pre><div class="line">sudo ls -l /var/<span class="built_in">log</span>/cloudera-scm-agent/</div><div class="line">cloudera-scm-agent.log	cloudera-scm-agent.out	cmf_listener.log  supervisord.log  supervisord.out</div><div class="line">sudo tail -300 /var/<span class="built_in">log</span>/cloudera-scm-agent/cloudera-scm-agent.log</div></pre></td></tr></table></figure>
</p>
<p><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">[23/Mar/2017 14:50:57 +0000] 4871 MainThread agent        ERROR    Could not contact supervisor.</div><div class="line">Traceback (most recent call last):</div><div class="line">  File &quot;/usr/lib64/cmf/agent/src/cmf/agent.py&quot;, line 832, in get_supervisor_data</div><div class="line">    supervisor_info = sup.getAllProcessInfo()</div><div class="line">  File &quot;/usr/lib64/python2.6/xmlrpclib.py&quot;, line 1199, in __call__</div><div class="line">    return self.__send(self.__name, args)</div><div class="line">  File &quot;/usr/lib64/python2.6/xmlrpclib.py&quot;, line 1489, in __request</div><div class="line">    verbose=self.__verbose</div><div class="line">  File &quot;/usr/lib64/cmf/agent/build/env/lib/python2.6/site-packages/supervisor-3.0-py2.6.egg/supervisor/xmlrpc.py&quot;, line 460, in request</div><div class="line">    self.connection.request(&apos;POST&apos;, handler, request_body, self.headers)</div><div class="line">  File &quot;/usr/lib64/python2.6/httplib.py&quot;, line 914, in request</div><div class="line">    self._send_request(method, url, body, headers)</div><div class="line">  File &quot;/usr/lib64/python2.6/httplib.py&quot;, line 951, in _send_request</div><div class="line">    self.endheaders()</div><div class="line">  File &quot;/usr/lib64/python2.6/httplib.py&quot;, line 908, in endheaders</div><div class="line">    self._send_output()</div><div class="line">  File &quot;/usr/lib64/python2.6/httplib.py&quot;, line 780, in _send_output</div><div class="line">    self.send(msg)</div><div class="line">  File &quot;/usr/lib64/python2.6/httplib.py&quot;, line 739, in send</div><div class="line">    self.connect()</div><div class="line">  File &quot;/usr/lib64/python2.6/httplib.py&quot;, line 720, in connect</div><div class="line">    self.timeout)</div><div class="line">  File &quot;/usr/lib64/python2.6/socket.py&quot;, line 561, in create_connection</div><div class="line">    raise error, msg</div><div class="line">error: [Errno 111] Connection refused</div><div class="line">[23/Mar/2017 14:50:57 +0000] 4871 MainThread agent        ERROR    Failed to contact supervisor after 6 attempts. Agent will exit.</div><div class="line">[23/Mar/2017 14:50:57 +0000] 4871 MainThread agent        INFO     Stopping agent...</div><div class="line">[23/Mar/2017 14:50:57 +0000] 4871 MainThread agent        INFO     No extant cgroups; unmounting any cgroup roots</div><div class="line">[23/Mar/2017 14:50:57 +0000] 4871 MainThread agent        INFO     3 processes are being managed; Supervisor will continue to run.</div><div class="line">[23/Mar/2017 14:50:57 +0000] 4871 MainThread _cplogging   INFO     [23/Mar/2017:14:50:57] ENGINE Bus STOPPING</div><div class="line">[23/Mar/2017 14:50:57 +0000] 4871 MainThread _cplogging   INFO     [23/Mar/2017:14:50:57] ENGINE HTTP Server cherrypy._cpwsgi_server.CPWSGIServer((&apos;ddp-dn-114.cmdmp.com&apos;, 9000)) shut down</div><div class="line">[23/Mar/2017 14:50:57 +0000] 4871 MainThread _cplogging   INFO     [23/Mar/2017:14:50:57] ENGINE Stopped thread &apos;_TimeoutMonitor&apos;.</div><div class="line">[23/Mar/2017 14:50:57 +0000] 4871 MainThread _cplogging   INFO     [23/Mar/2017:14:50:57] ENGINE Bus STOPPED</div><div class="line">[23/Mar/2017 14:50:57 +0000] 4871 MainThread _cplogging   INFO     [23/Mar/2017:14:50:57] ENGINE Bus STOPPING</div><div class="line">[23/Mar/2017 14:50:57 +0000] 4871 MainThread _cplogging   INFO     [23/Mar/2017:14:50:57] ENGINE HTTP Server cherrypy._cpwsgi_server.CPWSGIServer((&apos;ddp-dn-114.cmdmp.com&apos;, 9000)) already shut down</div><div class="line">[23/Mar/2017 14:50:57 +0000] 4871 MainThread _cplogging   INFO     [23/Mar/2017:14:50:57] ENGINE No thread running for None.</div><div class="line">[23/Mar/2017 14:50:57 +0000] 4871 MainThread _cplogging   INFO     [23/Mar/2017:14:50:57] ENGINE Bus STOPPED</div><div class="line">[23/Mar/2017 14:50:57 +0000] 4871 MainThread _cplogging   INFO     [23/Mar/2017:14:50:57] ENGINE Bus EXITING</div><div class="line">[23/Mar/2017 14:50:57 +0000] 4871 MainThread _cplogging   INFO     [23/Mar/2017:14:50:57] ENGINE Bus EXITED</div></pre></td></tr></table></figure>
</p>
<h2 id="cloudera-scm-agent-服务启动"><a href="#cloudera-scm-agent-服务启动" class="headerlink" title="cloudera-scm-agent 服务启动"></a>cloudera-scm-agent 服务启动</h2><p>root 用户下:<br><code>service  cloudera-scm-agent restart</code></p>
<p>sudo 权限用户:<br><code>ll /etc/init.d/ |grep cloudera</code><br>-rwxr-xr-x 1 root      root       6656 Sep 16  2015 cloudera-scm-agent</p>
<p><code>/etc/init.d/cloudera-scm-agent restart</code></p>
]]></content>
      
        <categories>
            
            <category> CDH </category>
            
            <category> Error </category>
            
        </categories>
        
        
    </entry>
    
    <entry>
      <title><![CDATA[配置fstab]]></title>
      <url>/CDH/Test/%E9%85%8D%E7%BD%AEfstab.html</url>
      <content type="html"><![CDATA[<h2 id="配置fstab开机自动挂载磁盘和nas"><a href="#配置fstab开机自动挂载磁盘和nas" class="headerlink" title="配置fstab开机自动挂载磁盘和nas"></a>配置fstab开机自动挂载磁盘和nas</h2><p><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">ddp-nn-01:~ # cat /etc/fstab</div><div class="line">/dev/disk/by-id/scsi-3600605b00aa90f901dd8c1c109e9a38b-part4 /                    ext3       acl,user_xattr        1 1</div><div class="line">/dev/disk/by-id/scsi-3600605b00aa90f901dd8c1c109e9a38b-part3 /boot                ext3       acl,user_xattr        1 2</div><div class="line">/dev/disk/by-id/scsi-3600605b00aa90f901dd8c1c109e9a38b-part1 /opt                 ext3       acl,user_xattr        1 2</div><div class="line">/dev/disk/by-id/scsi-3600605b00aa90f901dd8c1c109e9a38b-part2 /var                 ext3       acl,user_xattr        1 2</div><div class="line">proc                 /proc                proc       defaults              0 0</div><div class="line">sysfs                /sys                 sysfs      noauto                0 0</div><div class="line">debugfs              /sys/kernel/debug    debugfs    noauto                0 0</div><div class="line">usbfs                /proc/bus/usb        usbfs      noauto                0 0</div><div class="line">devpts               /dev/pts             devpts     mode=0620,gid=5       0 0</div><div class="line">/dev/sdb1 /mnt/sdb1 ext3 defaults,noatime,nodiratime,barrier=0,data=writeback,commit=100 0 0</div><div class="line">/dev/sdc1 /mnt/sdc1 ext3 defaults,noatime,nodiratime,barrier=0,data=writeback,commit=100 0 0</div></pre></td></tr></table></figure>
</p>
<h2 id="配置nas-并添加到fstab"><a href="#配置nas-并添加到fstab" class="headerlink" title="配置nas 并添加到fstab"></a>配置nas 并添加到fstab</h2><p><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">mount -t nfs 10.200.90.16:/vx/PBS_BD_SJY2 /mnt/quancheng</div><div class="line">vi /etc/fstab</div><div class="line">10.200.90.16:/vx/PBS_BD_SJY2            /mnt/quancheng                nfs defaults 0 0</div></pre></td></tr></table></figure></p>
]]></content>
      
        <categories>
            
            <category> CDH </category>
            
            <category> Test </category>
            
        </categories>
        
        
    </entry>
    
    <entry>
      <title><![CDATA[Cloudera Manager和CDH快速入门指南]]></title>
      <url>/CDH/Test/Cloudera%20Manager%E5%92%8CCDH%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8%E6%8C%87%E5%8D%97.html</url>
      <content type="html"><![CDATA[<h1 id="Cloudera-Manager和CDH快速入门指南"><a href="#Cloudera-Manager和CDH快速入门指南" class="headerlink" title="Cloudera Manager和CDH快速入门指南"></a>Cloudera Manager和CDH快速入门指南</h1><p>本快速入门指南介绍如何在四台主机的集群上快速创建Cloudera Manager 5，CDH 5和托管服务的新安装。生成的部署可用于演示和概念验证应用程序，但不推荐用于生产。</p>
<p>继续阅读：</p>
<ul>
<li>快速启动群集主机要求</li>
<li>下载并运行Cloudera Manager服务器安装程序</li>
<li>在RHEL 5和CentOS 5上，安装Python 2.6或2.7</li>
<li>启动Cloudera Manager管理控制台</li>
<li>使用Cloudera Manager向导安装和配置软件<ul>
<li>选择Cloudera Manager Edition和指定主机</li>
<li>安装CDH和托管服务软件</li>
<li>添加和配置服务</li>
</ul>
</li>
<li>测试安装<ul>
<li>运行MapReduce作业</li>
<li>使用Hue测试</li>
</ul>
</li>
</ul>
<h2 id="快速启动群集主机要求"><a href="#快速启动群集主机要求" class="headerlink" title="快速启动群集主机要求"></a>快速启动群集主机要求</h2><p>集群中的四台主机必须满足以下要求：</p>
<ul>
<li>主机必须至少有10 GB RAM。</li>
<li>您必须对主机具有root或无密码sudo访问权限。</li>
<li>如果使用root，主机必须接受相同的root密码。</li>
<li>主机必须具有Internet访问权限才能允许向导从中安装软件 archive.cloudera.com。</li>
<li>运行支持的操作系统：<ul>
<li>请参阅CDH 5和Cloudera Manager 5要求和支持的版本。</li>
<li>SLES - SUSE Linux Enterprise Server 11,64位。需要Service Pack 2或更高版本。Updates存储库必须是活动的，并且需要SUSE Linux Enterprise Software Development Kit 11 SP1。</li>
<li>Debian - Wheezy（7.0和7.1），64位。</li>
<li>Ubuntu - Trusty（14.04）和（Precise）12.04,64位。</li>
</ul>
</li>
</ul>
<p>如果您的环境不满足这些要求，则本指南中描述的过程可能无法正常工作。有关其他Cloudera Manager安装选项和要求的信息，请参阅 安装Cloudera Manager和CDH。</p>
<h2 id="下载并运行Cloudera-Manager服务器安装程序"><a href="#下载并运行Cloudera-Manager服务器安装程序" class="headerlink" title="下载并运行Cloudera Manager服务器安装程序"></a>下载并运行Cloudera Manager服务器安装程序</h2><p>将Cloudera Manager安装程序下载到要安装Cloudera Manager Server的集群主机：</p>
<ol>
<li>在Web浏览器中打开Cloudera Manager下载。</li>
<li>在Cloudera Manager框中，单击立即下载。</li>
<li><p>单击下载Cloudera Manager以下载最新版本的安装程序或单击选择不同的版本以下载较早版本。<br>将显示产品兴趣对话框。</p>
</li>
<li><p>点击登录并输入您的电子邮件地址和密码，或填写产品兴趣表单，然后点击继续。<br>将显示Cloudera标准许可证页面。</p>
</li>
<li><p>接受许可协议并单击提交。<br>显示自动安装说明。您还可以查看系统要求和发行说明，您可以转到文档。</p>
</li>
<li><p>下载安装程序：<br><code>$ wget https://archive.cloudera.com/cm5/installer/latest/cloudera-manager-installer.bin</code></p>
</li>
<li>更改 cloudera-manager-installer.bin 具有可执行权限：<br><code>$ chmod u + x cloudera-manager-installer.bin</code></li>
<li>通过执行以下操作之一运行Cloudera Manager Server安装程序：<ul>
<li>从Internet安装Cloudera Manager软件包：<code>$ sudo ./cloudera-manager-installer.bin</code></li>
<li>从本地存储库安装Cloudera Manager软件包：<code>$ sudo ./cloudera-manager-installer.bin --skip_repo_package = 1</code></li>
</ul>
</li>
<li>阅读Cloudera Manager README，然后按Return或Enter选择Next。</li>
<li>阅读Cloudera Express许可证，然后按Return或Enter选择下一步。使用箭头键，然后按Return或Enter选择是以确认您接受许可证。</li>
<li>阅读Oracle二进制代码许可协议，然后按Return或Enter选择 Next。</li>
<li>使用箭头键，然后按Return或Enter选择是以确认您接受Oracle二进制代码许可协议。发生以下情况：<ul>
<li>安装程序将安装Oracle JDK和Cloudera Manager存储库文件。</li>
<li>安装程序将安装Cloudera Manager Server和嵌入式PostgreSQL软件包。</li>
<li>安装程序启动Cloudera Manager Server和嵌入式PostgreSQL数据库。</li>
</ul>
</li>
<li>安装完成后，将显示Cloudera Manager管理控制台的完整URL，包括端口号，默认情况下为7180。按Return或Enter选择确定继续。</li>
<li>按Return或Enter选择确定以退出安装程序。</li>
</ol>
<h2 id="在RHEL-5和CentOS-5上，安装Python-2-6或2-7"><a href="#在RHEL-5和CentOS-5上，安装Python-2-6或2-7" class="headerlink" title="在RHEL 5和CentOS 5上，安装Python 2.6或2.7"></a>在RHEL 5和CentOS 5上，安装Python 2.6或2.7</h2><p>CDH 5 Hue仅与安装在其上的操作系统的默认系统Python版本一起工作。例如，在RHEL / CentOS 6上，您需要使用Python 2.6启动Hue。<br>要从EPEL存储库安装软件包，请将相应的软件包RPM软件包下载到您的计算机，然后使用安装Python yum。例如，对RHEL 5或CentOS 5使用以下命令：</p>
<p><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">$ su -c&apos;rpm -Uvh http://download.fedoraproject.org/pub/epel/5/i386/epel-release-5-4.noarch.rpm&apos;</div><div class="line">... ...</div><div class="line">$ yum install python26</div></pre></td></tr></table></figure>
</p>
<h2 id="启动Cloudera-Manager管理控制台"><a href="#启动Cloudera-Manager管理控制台" class="headerlink" title="启动Cloudera Manager管理控制台"></a>启动Cloudera Manager管理控制台</h2><ol>
<li>等待几分钟，Cloudera Manager Server启动。要观察启动过程，请运行tail -f /var/log/cloudera-scm-server/cloudera-scm-server.log在Cloudera Manager Server主机上。如果Cloudera Manager Server未启动，请参阅 排除安装和升级问题。</li>
<li>在Web浏览器中，输入 http：// 服务器主机：7180，其中Server主机是运行Cloudera Manager Server的主机的FQDN或IP地址。<br>将显示Cloudera Manager管理控制台的登录屏幕。</li>
<li>使用凭据登录到Cloudera Manager管理控制台：<strong>用户名</strong>： 管理员 <strong>密码</strong>： 管理员。</li>
<li>登录后，将显示“ <strong>Cloudera Manager最终用户许可条款和条件</strong>”页面。阅读条款和条件，然后选择是接受它们。</li>
<li>单击<strong>继续</strong>。将显示<strong>欢迎使用Cloudera Manager</strong>页面。</li>
</ol>
<h2 id="使用Cloudera-Manager向导安装和配置软件"><a href="#使用Cloudera-Manager向导安装和配置软件" class="headerlink" title="使用Cloudera Manager向导安装和配置软件"></a>使用Cloudera Manager向导安装和配置软件</h2><p>在群集主机上安装和配置Cloudera Manager，CDH和托管服务软件涉及以下主要步骤。</p>
<p>继续阅读：</p>
<ul>
<li>选择Cloudera Manager Edition和指定主机</li>
<li>安装CDH和托管服务软件</li>
<li>添加和配置服务</li>
</ul>
<h3 id="选择Cloudera-Manager-Edition和指定主机"><a href="#选择Cloudera-Manager-Edition和指定主机" class="headerlink" title="选择Cloudera Manager Edition和指定主机"></a>选择Cloudera Manager Edition和指定主机</h3><ol>
<li>选择Cloudera企业 数据中心版试用版，它不需要许可证，但在60天后过期，无法续订。试用版允许您创建Cloudera Manager支持的所有CDH和托管服务。单击继续。</li>
<li>将显示信息，指示将安装Cloudera Manager的哪个版本以及您可以选择的服务。单击继续。将显示“指定CDH群集安装的主机”屏幕。</li>
<li>指定要在其上安装CDH和托管服务的四个主机。您可以指定主机名或IP地址和范围，例如：10.1.1。[1-4]或主机[1-3] .company.com。您可以通过用逗号，分号，制表符或空格分隔多个地址和地址范围，或将它们放在单独的行上。</li>
<li>单击搜索。Cloudera Manager标识集群上的主机。验证显示的主机数量与要安装服务的主机数量匹配。清除不存在的主机条目，并清除不想安装服务的主机。单击继续。将显示“选择存储库”屏幕。</li>
</ol>
<h3 id="安装CDH和托管服务软件"><a href="#安装CDH和托管服务软件" class="headerlink" title="安装CDH和托管服务软件"></a>安装CDH和托管服务软件</h3><ol>
<li>保留默认分发方法使用宗地和CDH的默认版本5.将“其他宗地”选项保留为“无”。</li>
<li>对于Cloudera Manager Agent，保留此Cloudera Manager Server的默认匹配版本。单击继续。将显示“JDK安装选项”屏幕。</li>
<li>选择安装Oracle Java SE开发工具包（JDK）复选框以允许Cloudera Manager在每个群集主机上安装JDK或取消选中是否打算自行安装。保持安装Java无限强度加密策略文件复选框。单击继续。将显示“启用单用户模式”屏幕。</li>
<li>将单用户模式复选框清除，然后单击继续。此时将显示“提供SSH登录凭据”页面。</li>
<li>指定主机SSH登录属性：</li>
<li>保留默认登录根目录或输入具有无密码sudo权限的帐户的用户名。</li>
<li>如果选择使用密码验证，请输入并确认密码。</li>
<li>单击继续。Cloudera Manager在每个主机上安装Oracle JDK和Cloudera Manager Agent软件包，并启动代理。</li>
<li>单击继续。将显示“安装所选宗地”屏幕。Cloudera Manager安装CDH。在包裹安装过程中，会在单独的进度条中指示包裹安装过程的各个阶段的进度。当屏幕底部的继续按钮变为蓝色时，安装过程完成。</li>
<li>单击继续。主机检查器运行以验证安装，并提供其查找的摘要，包括已安装组件的所有版本。单击“完成”。将显示群集设置屏幕。</li>
</ol>
<h3 id="添加和配置服务"><a href="#添加和配置服务" class="headerlink" title="添加和配置服务"></a>添加和配置服务</h3><ol>
<li>选择所有服务以创建HDFS，YARN（包括MapReduce 2），ZooKeeper，Oozie，Hive，Hue，Sqoop，HBase，Impala，Solr，Spark和Key-Value Store Indexer服务。单击继续。将显示自定义角色分配屏幕。</li>
<li>配置以下角色分配：<ul>
<li>单击HBase Thrift Server角色下的文本字段。在显示的主机选择对话框中，选中任何主机旁边的复选框，然后单击右下角的确定 。</li>
<li>单击ZooKeeper服务的服务器角色下的文本字段。在显示的主机选择对话框中，取消选中默认分配的主机（主主机）旁边的复选框，并选中其余三台主机旁边的复选框。单击右下角的确定。</li>
</ul>
</li>
<li>单击继续。显示数据库设置屏幕。</li>
<li>保留使用嵌入式数据库的默认设置，以使Cloudera Manager在嵌入式PostgreSQL数据库中创建和配置所有必需的数据库。单击测试连接。测试完成后，单击继续。将显示“查看更改”屏幕。</li>
<li>查看要应用的配置更改。单击继续。将显示“命令进度”页面。</li>
<li>向导将执行32个步骤来配置和启动服务。启动完成后，单击继续。</li>
<li>将显示一条成功消息，指示集群已成功启动。单击完成以继续到首页 &gt; 状态选项卡。</li>
</ol>
<h2 id="测试安装"><a href="#测试安装" class="headerlink" title="测试安装"></a>测试安装</h2><p>在屏幕的左侧是当前运行的服务及其状态信息的列表。所有服务应以良好 运行状态运行，但是可能有少量配置警告由扳手图标和数字指示，您可以忽略它。</p>
<p>您可以单击每个服务以查看有关服务的更多详细信息。您还可以通过运行MapReduce作业或使用Hue应用程序与群集交互来测试安装。</p>
<h3 id="运行MapReduce作业"><a href="#运行MapReduce作业" class="headerlink" title="运行MapReduce作业"></a>运行MapReduce作业</h3><ol>
<li>登录到群集主机。</li>
<li><p>运行Hadoop PiEstimator示例：</p>
<p><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">sudo -u hdfs hadoop jar \</div><div class="line">/opt/cloudera/parcels/CDH/lib/hadoop-mapreduce/hadoop-mapreduce-examples.jar \</div><div class="line">pi 10 100</div></pre></td></tr></table></figure>
</p>
</li>
<li><p>通过从Cloudera Manager管理控制台顶部导航栏中选择以下内容来查看运行作业的结果：集群 &gt; 集群名称 &gt; YARN应用程序。您将看到如下所示的条目：<br>&lt;img src=&quot;&quot; alt=&quot;&quot;&gt;</p>
</li>
</ol>
<h3 id="使用Hue测试"><a href="#使用Hue测试" class="headerlink" title="使用Hue测试"></a>使用Hue测试</h3><p>测试集群的一个好方法是运行作业。此外，您可以通过运行其中一个Hue Web应用程序来测试集群。Hue是一个图形用户界面，允许您通过运行允许浏览HDFS，管理Hive元数据转储，运行Hive，Impala和搜索查询，Pig脚本和Oozie工作流的应用程序与群集进行交互。</p>
<ol>
<li>在Cloudera Manager管理控制台首页 &gt; 状态选项卡中，单击Hue服务。</li>
<li>单击Hue Web UI链接，它将在新窗口中打开Hue。</li>
<li>使用凭据登录，用户名： hdfs，密码： hdfs。</li>
<li>在浏览器窗口顶部的导航栏中选择一个应用程序。</li>
</ol>
]]></content>
      
        <categories>
            
            <category> CDH </category>
            
            <category> Test </category>
            
        </categories>
        
        
    </entry>
    
    <entry>
      <title><![CDATA[快速启动VM管理信息]]></title>
      <url>/CDH/Test/%E5%BF%AB%E9%80%9F%E5%90%AF%E5%8A%A8VM%E7%AE%A1%E7%90%86%E4%BF%A1%E6%81%AF.html</url>
      <content type="html"><![CDATA[<h1 id="快速启动VM管理信息"><a href="#快速启动VM管理信息" class="headerlink" title="快速启动VM管理信息"></a>快速启动VM管理信息</h1><p>在大多数情况下，除了管理已安装的产品和服务之外，QuickStart VM不需要管理。如果需要其他管理或出现问题，本主题提供有关帐户的信息，以及一些常见问题的可能解释和解决方案。<br><a href="http://s3.amazonaws.com/trainingvms/Cloudera-Training-Get2EC2-VM-1.2-vmware.zip" target="_blank" rel="external">Cloudera-Training-Get2EC2-VM-1.2-vmware</a></p>
<h2 id="帐户"><a href="#帐户" class="headerlink" title="帐户"></a>帐户</h2><p>一旦启动VM，您将自动登录 cloudera 用户：<br>用户名： cloudera<br>密码： cloudera<br>cloudera 帐户有VM中sudo的权限,密码是cloudera。<br>MySQL root密码（和其他MySQL用户帐户的密码）也是cloudera。<br>Hue和Cloudera Manager使用相同的凭据。</p>
<h2 id="快速启动VMware映像"><a href="#快速启动VMware映像" class="headerlink" title="快速启动VMware映像"></a>快速启动VMware映像</h2><p>要启动VMware映像，您需要适用于Windows和Linux的VMware Player或适用于Mac的VMware Fusion。</p>
<p><strong>注意： VMware Fusion仅在英特尔架构上工作，因此具有PowerPC处理器的旧版Mac无法运行QuickStart VM。</strong></p>
<h2 id="快速启动VirtualBox图像"><a href="#快速启动VirtualBox图像" class="headerlink" title="快速启动VirtualBox图像"></a>快速启动VirtualBox图像</h2><p>有些用户报告了在VirtualBox中运行CentOS 6.4的问题。如果在VirtualBox VM启动时发生内核崩溃，您可以尝试解决此问题，方法是打开设置 &gt; 系统 &gt; 主板选项卡，然后为芯片集选择 ICH9而不是PIIX3。如果尚未这样做，还必须在同一选项卡上启用I / O APIC。</p>
<h2 id="快速启动KVM映像"><a href="#快速启动KVM映像" class="headerlink" title="快速启动KVM映像"></a>快速启动KVM映像</h2><p>KVM映像提供了可由许多管理程序使用的原始磁盘映像。使用足够的RAM配置使用此映像的计算机。有关VM大小要求，请参阅Cloudera QuickStart VM。</p>
]]></content>
      
        <categories>
            
            <category> CDH </category>
            
            <category> Test </category>
            
        </categories>
        
        
    </entry>
    
    <entry>
      <title><![CDATA[reload_UDF]]></title>
      <url>/CDH/Test/reload_UDF.html</url>
      <content type="html"><![CDATA[<p>步骤就是<br>先在beeline drop对应函数<br>然后本地(每个hiveserver 和hivemetastore机器上) 替换对应jar包<br>然后 beeline 里面 打reload;<br>最后重新create</p>
]]></content>
      
        <categories>
            
            <category> CDH </category>
            
            <category> Test </category>
            
        </categories>
        
        
    </entry>
    
    <entry>
      <title><![CDATA[CDH]]></title>
      <url>/CDH/Test/CDH.html</url>
      <content type="html"><![CDATA[<h2 id="Process-Management"><a href="#Process-Management" class="headerlink" title="Process Management"></a>Process Management</h2><p>在Cloudera Manager受管集群中，您只能使用Cloudera Manager启动或停止角色实例进程。</p>
<p>Cloudera Manager使用名为<strong>supervisord</strong>的开源过程管理工具,启动进程，负责重定向日志文件，通知进程失败，将调用进程的有效用户ID设置为正确的用户，等等。</p>
<p>停止Cloudera Manager Server和Cloudera Manager Agent不会降低您的服务;任何正在运行的角色实例都会继续运行</p>
<p>有些用户报告了在VirtualBox中运行CentOS 6.4的问题。如果在VirtualBox VM启动时发生内核崩溃，您可以尝试解决此问题，方法是打开设置 &gt; 系统 &gt; 主板选项卡，然后为芯片集选择 ICH9而不是PIIX3。如果尚未这样做，还必须在同一选项卡上启用I / O APIC。</p>
<hr>
<hr>
<p>yarn impala hbase 内存调整 总可用内存的80%</p>
<p><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">Yarn</div><div class="line">container memory </div><div class="line">yarn.nodemanager.resource.memory-mb</div><div class="line">Hbase</div><div class="line">Java Heap Size of HBase RegionServer in Bytes</div><div class="line">Impala</div><div class="line">Impala Daemon Memory Limit</div></pre></td></tr></table></figure>
</p>
<hr>
<p>从节点的内存需要根据CPU的虚拟核数（vcore）进行匹配，<br>CPU的虚拟核数计算公式=cpu<em>单CPU核数</em>超线程数<br>内存容量大小=vcore数*2GB 至少2GB</p>
]]></content>
      
        <categories>
            
            <category> CDH </category>
            
            <category> Test </category>
            
        </categories>
        
        
    </entry>
    
    <entry>
      <title><![CDATA[Impala_swapp]]></title>
      <url>/CDH/Upgrade/Impala_swapp.html</url>
      <content type="html"><![CDATA[<h1 id="Impala-swapp"><a href="#Impala-swapp" class="headerlink" title="Impala_swapp"></a>Impala_swapp</h1><p>部分组件HDFS,YARN,HBAS有黄色警告是因为系统默认SWAP默认值为60，导致内存未使用满便使用swap memory导致的，已经修改过配对应系统的配置值为SWAP=0。<br><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">echo 0 &gt;/proc/sys/vm/swappiness</div><div class="line">echo &apos;vm.swappiness=0&apos;&gt;&gt; /etc/sysctl.conf</div><div class="line">cat /proc/sys/vm/swappiness</div></pre></td></tr></table></figure></p>
]]></content>
      
        <categories>
            
            <category> CDH </category>
            
            <category> Upgrade </category>
            
        </categories>
        
        
    </entry>
    
    <entry>
      <title><![CDATA[spark-streaming 与kafka的结合]]></title>
      <url>/CDH/Error/spark-streaming%20%E4%B8%8Ekafka%E7%9A%84%E7%BB%93%E5%90%88.html</url>
      <content type="html"><![CDATA[<h1 id="spark-streaming-与kafka的结合"><a href="#spark-streaming-与kafka的结合" class="headerlink" title="spark-streaming 与kafka的结合"></a>spark-streaming 与kafka的结合</h1><p>升级CDH到5.10后 spark-steaming 从kafka获取消息就报错</p>
<p><strong>原因:高版本的kafka从低版本的kafka server端连接不兼容</strong></p>
<p>因为在使用spark-streaming调用kafka的时候,会由cloudera来调用相应的组件,其中就有kafka的客户端,并且优先级较高,继而不会调用我们自己的kafka客户端,导致报错<br>后来原厂的人重新编译路径指向后解决</p>
]]></content>
      
        <categories>
            
            <category> CDH </category>
            
            <category> Error </category>
            
        </categories>
        
        
    </entry>
    
    <entry>
      <title><![CDATA[fork]]></title>
      <url>/Linux/fork.html</url>
      <content type="html"><![CDATA[<h2 id="fork"><a href="#fork" class="headerlink" title="fork"></a>fork</h2><p>在Linux下产生新的进程的系统调用就是fork函数，这个函数名是英文中“分叉”的意思。为什么取这个名字呢？因为一个进程在运行中，如果使用了fork，就产生了另一个进程，于是进程就“分叉”了，所以这个名字取得很形象。下面就看看如何具体使用fork，这段程序演示了使用fork的基本框架：</p>
<p> void main(){ int I; if ( fork() == 0 ) { /<em> 子进程程序 </em>/ for ( I = 1; I &lt;1000; I ++ ) printf(“This is child process\n”); } else { /<em> 父进程程序</em>/ for ( I = 1; I &lt;1000; I ++ ) printf(“This is process process\n”); } } </p>
<p> 程序运行后，你就能看到屏幕上交替出现子进程与父进程各打印出的一千条信息了。如果程序还在运行中，你用ps命令就能看到系统中有两个它在运行了。 那么调用这个fork函数时发生了什么呢？一个程序一调用fork函数，系统就为一个新的进程准备了前述三个段，首先，系统让新的进程与旧的进程使用同一个代码段，因为它们的程序还是相同的，对于数据段和堆栈段，系统则复制一份给新的进程，这样，父进程的所有数据都可以留给子进程，但是，子进程一旦开始运行，虽然它继承了父进程的一切数据，但实际上数据却已经分开，相互之间不再有影响了，也就是说，它们之间不再共享任何数据了。而如果两个进程要共享什么数据的话，就要使用另一套函数（shmget，shmat，shmdt等）来操作。现在，已经是两个进程了，对于父进程，fork函数返回了子程序的进程号，而对于子程序，fork函数则返回零，这样，对于程序，只要判断fork函数的返回值，就知道自己是处于父进程还是子进程中。 读者也许会问，如果一个大程序在运行中，它的数据段和堆栈都很大，一次fork就要复制一次，那么fork的系统开销不是很大吗？其实UNIX自有其解决的办法，大家知道，一般CPU都是以“页”为单位分配空间的，象INTEL的CPU，其一页在通常情况下是4K字节大小，而无论是数据段还是堆栈段都是由许多“页”构成的，fork函数复制这两个段，只是“逻辑”上的，并非“物理”上的，也就是说，实际执行fork时，物理空间上两个进程的数据段和堆栈段都还是共享着的，当有一个进程写了某个数据时，这时两个进程之间的数据才有了区别，系统就将有区别的“页”从物理上也分开。系统在空间上的开销就可以达到最小。</p>
]]></content>
      
        <categories>
            
            <category> Linux </category>
            
        </categories>
        
        
    </entry>
    
    <entry>
      <title><![CDATA[swap 系统值修改]]></title>
      <url>/CDH/Error/swap%20%E7%B3%BB%E7%BB%9F%E5%80%BC%E4%BF%AE%E6%94%B9.html</url>
      <content type="html"><![CDATA[<h1 id="Impala-swapp系统值修改"><a href="#Impala-swapp系统值修改" class="headerlink" title="Impala swapp系统值修改"></a>Impala swapp系统值修改</h1><blockquote>
<p>部分组件HDFS,YARN,HBAS有黄色警告是因为系统默认SWAP默认值为60</p>
</blockquote>
<p>导致内存未使用满便使用swap memory导致的，已经修改过配对应系统的配置值为SWAP=0。</p>
<p><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">echo 0 &gt;/proc/sys/vm/swappiness</div><div class="line">echo &apos;vm.swappiness=0&apos;&gt;&gt; /etc/sysctl.conf</div><div class="line">cat /proc/sys/vm/swappiness</div></pre></td></tr></table></figure></p>
]]></content>
      
        <categories>
            
            <category> CDH </category>
            
            <category> Error </category>
            
        </categories>
        
        
    </entry>
    
    <entry>
      <title><![CDATA[因license过期受到影响的功能]]></title>
      <url>/CDH/Error/%E5%9B%A0license%E8%BF%87%E6%9C%9F%E5%8F%97%E5%88%B0%E5%BD%B1%E5%93%8D%E7%9A%84%E5%8A%9F%E8%83%BD.html</url>
      <content type="html"><![CDATA[<h1 id="因license过期受到影响的功能"><a href="#因license过期受到影响的功能" class="headerlink" title="因license过期受到影响的功能"></a>因license过期受到影响的功能</h1><ol>
<li><p>为Cloudera Manager配置外部验证功能</p>
<p>Cloudera Manager支持对内部数据库和外部服务的用户身份验证。</p>
</li>
<li><p>查看和恢复配置更改功能</p>
<p>无论何时更改并保存服务或角色实例或主机的一组配置设置，Cloudera Manager将保存先前设置的修订版本和进行更改的用户的名称。然后，您可以查看配置设置的过去版本，如果需要，将设置回滚到之前的状态。</p>
</li>
<li><p>配置警报SNMP通知和配置自定义警报脚本功能</p>
</li>
<li><p>备份和灾难恢复功能</p>
<p>Cloudera Manager能够跨数据中心复制并进行灾难恢复。 复制包括存储在HDFS中的数据，Hive表中存储的数据，Hive转移数据以及与Hive转移中注册的Impala表相关联的Impala元数据（catalog server metadata）。</p>
</li>
<li><p>Reports功能</p>
<p>“报告”页面会根据用户，组或目录在群集数据大小和文件数量来创建有关HDFS使用情况的报告。它还可以让用户根据集群中的MapReduce活动进行报告。</p>
</li>
<li><p>一些管理命令比如Rolling Restart,History and Rollback,Send Diagnostic Data 无法使用</p>
</li>
<li>集群利用率报告功能</li>
</ol>
<h3 id="目前已经影响使用的功能"><a href="#目前已经影响使用的功能" class="headerlink" title="目前已经影响使用的功能"></a>目前已经影响使用的功能</h3><ol>
<li><p>Cloudera Navigator功能</p>
<p>Cloudera Navigator是于Hadoop平台的完全集成的数据管理和安全系统,目前不可用,同时影响正常使用</p>
<ol>
<li>RegionServer,Hue Server,Navigator Metadata Server,,ImpalaOozie,ZooKeeper审计功能失效</li>
<li>授权和审计。配置身份验证，户和服务未证明身份之前将无法访问群集。授权机制，可以为用户和用户组分配权限。设置审计程序来跟踪谁访问集群,目前均失效</li>
<li>NameNode节点的audit log提交后无法得到正确响应(正确响应后会删除本地log),导致NameNode节点的log越积越多,同时Navigator节点也不断接受log导致两个host的磁盘空间经常告警(通过修改hdfs配置,暂时关闭该服务)</li>
</ol>
</li>
<li><p>怀疑机器加入集群时候受影响</p>
</li>
</ol>
]]></content>
      
        <categories>
            
            <category> CDH </category>
            
            <category> Error </category>
            
        </categories>
        
        
    </entry>
    
    <entry>
      <title><![CDATA[大数据平台-Zabbix agent恢复]]></title>
      <url>/CDH/Error/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%B9%B3%E5%8F%B0-Zabbix%20agent%E6%81%A2%E5%A4%8D.html</url>
      <content type="html"><![CDATA[<h1 id="zabbix-agent-进程恢复"><a href="#zabbix-agent-进程恢复" class="headerlink" title="zabbix agent 进程恢复"></a>zabbix agent 进程恢复</h1><blockquote>
<p><strong>v1.0</strong> updated:2017-03-30 何常通</p>
</blockquote>
<h2 id="因系统重启后-zabbix-agent没有设置开机启动所以会导致监控失效"><a href="#因系统重启后-zabbix-agent没有设置开机启动所以会导致监控失效" class="headerlink" title="因系统重启后,zabbix agent没有设置开机启动所以会导致监控失效"></a>因系统重启后,zabbix agent没有设置开机启动所以会导致监控失效</h2><p><figure class="highlight bash"><table><tr><td class="code"><pre><div class="line">ps aux |grep zabbix</div><div class="line"><span class="built_in">cd</span> /etc/init.d/</div><div class="line">ls |grep zabbix</div><div class="line"><span class="built_in">cd</span> /etc/init.d &amp;&amp; wget http://10.200.58.25/zabbix/zabbix_agentd &amp;&amp; chkconfig zabbix_agentd 2345 &amp;&amp; service </div><div class="line">chmod +x zabbix_agentd </div><div class="line">service zabbix_agentd start</div><div class="line">chkconfig zabbix_agentd on</div><div class="line">chkconfig --list</div><div class="line">chkconfig zabbix_agentd 2345</div><div class="line">chkconfig --list</div><div class="line">ps aux |grep zabbix</div></pre></td></tr></table></figure>
</p>
<h3 id="zabbix-agentd"><a href="#zabbix-agentd" class="headerlink" title="zabbix_agentd"></a>zabbix_agentd</h3><p><figure class="highlight bash"><table><tr><td class="code"><pre><div class="line"><span class="meta">#! /bin/sh</span></div><div class="line"><span class="comment"># Copyright (c) 07/2004 A.Tophofen, Germany</span></div><div class="line"><span class="comment"># Modified for Zabbix 1.1alpha7 and SuSE Linux 9.2</span></div><div class="line"><span class="comment"># April 2005, A. Kiepe, Switzerland</span></div><div class="line"><span class="comment"># Modified for Zabbix 2.0.0</span></div><div class="line"><span class="comment"># May 2012, Zabbix SIA</span></div><div class="line"><span class="comment">#</span></div><div class="line"><span class="comment"># init.d/zabbix_agentd</span></div><div class="line"><span class="comment">#</span></div><div class="line"><span class="comment">### BEGIN INIT INFO</span></div><div class="line"><span class="comment"># Provides:       zabbix_agentd</span></div><div class="line"><span class="comment"># Required-Start: $network $remote_fs $syslog</span></div><div class="line"><span class="comment"># Required-Stop:</span></div><div class="line"><span class="comment"># Default-Start:  3 </span></div><div class="line"><span class="comment"># Default-Stop:</span></div><div class="line"><span class="comment"># Description:    Starts Zabbix_Agentd</span></div><div class="line"><span class="comment">### END INIT INFO</span></div><div class="line"></div><div class="line">. /etc/rc.status</div><div class="line">rc_reset</div><div class="line">NAME=<span class="string">"zabbix_agentd"</span></div><div class="line"></div><div class="line">ZABBIX_BIN=<span class="string">"/usr/local/sbin/zabbix_agentd"</span></div><div class="line">ZABBIX_PID=<span class="string">"/tmp/zabbix_agentd.pid"</span></div><div class="line"></div><div class="line"><span class="keyword">if</span> [ ! -x <span class="variable">$&#123;ZABBIX_BIN&#125;</span> ] ; <span class="keyword">then</span></div><div class="line">        <span class="built_in">echo</span> -n <span class="string">"<span class="variable">$&#123;ZABBIX_BIN&#125;</span> not installed! "</span></div><div class="line">        <span class="comment"># Tell the user this has skipped</span></div><div class="line">        rc_status -s</div><div class="line">        <span class="built_in">exit</span> 5</div><div class="line"><span class="keyword">fi</span></div><div class="line"></div><div class="line"><span class="built_in">export</span> PATH=<span class="variable">$PATH</span>:/usr/<span class="built_in">local</span>/sbin</div><div class="line"></div><div class="line"><span class="keyword">case</span> <span class="string">"<span class="variable">$1</span>"</span> <span class="keyword">in</span></div><div class="line">    start)</div><div class="line">        <span class="built_in">echo</span> -n <span class="string">"Starting <span class="variable">$&#123;NAME&#125;</span> "</span></div><div class="line">        checkproc -p <span class="variable">$&#123;ZABBIX_PID&#125;</span> <span class="variable">$&#123;ZABBIX_BIN&#125;</span></div><div class="line">        <span class="keyword">case</span> $? <span class="keyword">in</span></div><div class="line">                0) <span class="built_in">echo</span> -n <span class="string">"- Warning: <span class="variable">$&#123;NAME&#125;</span> already running! "</span> ;;</div><div class="line">                1) <span class="built_in">echo</span> -n <span class="string">"- Warning: <span class="variable">$&#123;ZABBIX_PID&#125;</span> exists! "</span> ;;</div><div class="line">        <span class="keyword">esac</span></div><div class="line"></div><div class="line">        startproc -p <span class="variable">$&#123;ZABBIX_PID&#125;</span>  <span class="variable">$&#123;ZABBIX_BIN&#125;</span></div><div class="line">        rc_status -v</div><div class="line">        ;;</div><div class="line">    stop)</div><div class="line">        <span class="built_in">echo</span> -n <span class="string">"Shutting down <span class="variable">$&#123;NAME&#125;</span>"</span></div><div class="line">        checkproc -p <span class="variable">$&#123;ZABBIX_PID&#125;</span> <span class="variable">$&#123;ZABBIX_BIN&#125;</span> || <span class="built_in">echo</span> -n <span class="string">"- Warning: <span class="variable">$&#123;NAME&#125;</span> not running! "</span></div><div class="line">        killproc -p <span class="variable">$&#123;ZABBIX_PID&#125;</span> -TERM <span class="variable">$&#123;ZABBIX_BIN&#125;</span></div><div class="line">        rc_status -v</div><div class="line">        ;;</div><div class="line">    restart)</div><div class="line">        <span class="variable">$0</span> stop</div><div class="line">        sleep 10</div><div class="line">        <span class="variable">$0</span> start</div><div class="line">        rc_status</div><div class="line">        ;;</div><div class="line">        *)</div><div class="line">        <span class="built_in">echo</span> <span class="string">"Usage: <span class="variable">$0</span> &#123;start|stop|restart&#125;"</span></div><div class="line">        <span class="built_in">exit</span> 1</div><div class="line">        ;;</div><div class="line"><span class="keyword">esac</span></div><div class="line">rc_exit</div></pre></td></tr></table></figure></p>
]]></content>
      
        <categories>
            
            <category> CDH </category>
            
            <category> Error </category>
            
        </categories>
        
        
    </entry>
    
    <entry>
      <title><![CDATA[linux系统中常见的性能分析工具]]></title>
      <url>/Linux/linux%E7%B3%BB%E7%BB%9F%E4%B8%AD%E5%B8%B8%E8%A7%81%E7%9A%84%E6%80%A7%E8%83%BD%E5%88%86%E6%9E%90%E5%B7%A5%E5%85%B7.html</url>
      <content type="html"><![CDATA[<p>linux系统中常见的性能分析工具</p>
<h2 id="vmstat"><a href="#vmstat" class="headerlink" title="vmstat"></a>vmstat</h2><pre><code>vmstat是linux/unix上一个监控工具，能展示给定时间间隔的服务器的状态，包括操作系统的内存信息，CPU使用状态、进程信息等。

</code></pre><p>语法：<br>vmstat [-V] [delay [count]]</p>
<pre><code>#-V     打印出vmstat工具的版本信息
#delay  设置两次输出的时间间隔
#count  设置总共输出的次数


</code></pre><hr>
<p><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">host-xxx-xxx-xxx-xxx:~ # vmstat -V</div><div class="line">procps version 3.2.7</div><div class="line">host-xxx-xxx-xxx-xxx:~ # vmstat 2 3</div><div class="line">procs -----------memory---------- ---swap-- -----io---- -system-- -----cpu------</div><div class="line"> r  b   swpd   free   buff  cache   si   so    bi    bo   in   cs us sy id wa st</div><div class="line"> 0  0      0 13107104 1981044 766592    0    0     0    18    0    1  0  0 100  0  0</div><div class="line"> 0  0      0 13106776 1981044 766592    0    0     0     0  833  697  0  0 100  0  0</div><div class="line"> 0  0      0 13106784 1981044 766592    0    0     0     0  391  427  0  0 100  0  0</div></pre></td></tr></table></figure>
</p>
<p>对输出参数进行讲解<br>    procs<br>        r：运行进程数（即真正分配到cpu的进程数量），如果该值长期大于系统逻辑cpu的数量，表示cpu不足<br>        b：表示阻塞的进程数<br>    memory<br>        swpd：表示正在使用的虚拟内存的多少<br>        free：表示当前空闲的物理内存的大小<br>        buff：表示当前使用的buffers的大小<br>        cached：表示当前使用的cached的大小<br>buffers和cached的区别：<br>    ①buffers和cached都是内存的一部分<br>    ②buffers是内存与磁盘之间的，当对磁盘进行读写操作时，内存先将数据缓存到buffers中，然后再写入磁盘;cached是cpu和内存之间的，cached是缓存读取过的内容，下次再读时，如果在缓存中命中，则直接从缓存读取，否则读取磁盘。<br>      swap<br>        si：表示从磁盘读入到虚拟内存的大小<br>        so：表示从虚拟内存写入到磁盘的大小</p>
<p>如果si和so长期不为0,表示系统内存不足;而如果swpd的值长期不为0,但si和so的值长期为0,则无需担心<br>      io<br>        bi：表示从磁盘读取数据的总量<br>        bo：表示写入磁盘的数据总量<br>      system<br>        in：表示系统中断数<br>        cs：表示每秒产生的上下文切换次数<br>in和cs的值越大，内核消耗cpu时间越大<br>      cpu<br>         us：用户进程消耗的cpu时间所占百分比<br>         sy：内核进程消耗的cpu时间所占百分比<br>          id：cpu空闲状态的时间百分比<br>          wa：表示IO等待所占用的cpu时间百分比</p>
<p>us+sy+id=100</p>
<h2 id="sar命令"><a href="#sar命令" class="headerlink" title="sar命令"></a>sar命令</h2><p> sar命令可以获取系统的cpu、磁盘、内存、网络运行状态等信息<br>常见用法有</p>
<hr>
<p>host-xxx-xxx-xxx-xxx:~ # sar -u 1 3<br>Linux 3.0.76-0.11-default (host-10-200-43-168)  10/19/16    _x86<em>64</em></p>
<p>10:00:56        CPU     %user     %nice   %system   %iowait    %steal     %idle<br>10:00:57        all      0.12      0.00      0.12      0.00      0.00     99.75<br>10:00:58        all      0.25      0.00      0.25      0.00      0.00     99.50<br>10:00:59        all      0.00      0.00      0.00      0.00      0.00    100.00<br>Average:        all      0.13      0.00      0.13      0.00      0.00     99.75</p>
<p>host-xxx-xxx-xxx-xxx:~ # sar -P 0 1 2<br>Linux 3.0.76-0.11-default (host-10-200-43-168)  10/19/16    _x86<em>64</em></p>
<p>10:02:10        CPU     %user     %nice   %system   %iowait    %steal     %idle<br>10:02:11          0      0.00      0.00      1.00      0.00      0.00     99.00<br>10:02:12          0      3.00      0.00      0.00      0.00      0.00     97.00<br>Average:          0      1.50      0.00      0.50      0.00      0.00     98.00<br>host-xxx-xxx-xxx-xxx:~ # sar -d 1 3<br>Linux 3.0.76-0.11-default (host-10-200-43-168)  10/19/16    _x86<em>64</em></p>
<p>10:02:20          DEV       tps  rd_sec/s  wr_sec/s  avgrq-sz  avgqu-sz     await     svctm     %util<br>10:02:21     dev202-0      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00<br>10:02:21     dev253-0      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00<br>10:02:21     dev253-1      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00<br>10:02:21     dev253-2      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00</p>
<p>10:02:21          DEV       tps  rd_sec/s  wr_sec/s  avgrq-sz  avgqu-sz     await     svctm     %util<br>10:02:22     dev202-0      0.99      0.00     15.84     16.00      0.00      0.00      0.00      0.00<br>10:02:22     dev253-0      1.98      0.00     15.84      8.00      0.00      0.00      0.00      0.00<br>10:02:22     dev253-1      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00<br>10:02:22     dev253-2      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00</p>
<p>10:02:22          DEV       tps  rd_sec/s  wr_sec/s  avgrq-sz  avgqu-sz     await     svctm     %util<br>10:02:23     dev202-0      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00<br>10:02:23     dev253-0      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00<br>10:02:23     dev253-1      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00<br>10:02:23     dev253-2      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00</p>
<p>Average:          DEV       tps  rd_sec/s  wr_sec/s  avgrq-sz  avgqu-sz     await     svctm     %util<br>Average:     dev202-0      0.34      0.00      5.39     16.00      0.00      0.00      0.00      0.00<br>Average:     dev253-0      0.67      0.00      5.39      8.00      0.00      0.00      0.00      0.00<br>Average:     dev253-1      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00<br>Average:     dev253-2      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00<br>host-xxx-xxx-xxx-xxx:~ # sar -r 1 3<br>Linux 3.0.76-0.11-default (host-10-200-43-168)  10/19/16    _x86<em>64</em></p>
<p>10:02:32    kbmemfree kbmemused  %memused kbbuffers  kbcached  kbcommit   %commit<br>10:02:33     13106232   3189176     19.57   1981048    766804   1231016      6.01<br>10:02:34     13105448   3189960     19.58   1981048    766804   1231016      6.01<br>10:02:35     13105448   3189960     19.58   1981048    766804   1231016      6.01<br>Average:     13105709   3189699     19.57   1981048    766804   1231016      6.01<br>host-xxx-xxx-xxx-xxx:~ # sar -n DEV 1 1<br>Linux 3.0.76-0.11-default (host-10-200-43-168)  10/19/16    _x86<em>64</em></p>
<p>10:03:02        IFACE   rxpck/s   txpck/s    rxkB/s    txkB/s   rxcmp/s   txcmp/s  rxmcst/s<br>10:03:03           lo      0.00      0.00      0.00      0.00      0.00      0.00      0.00<br>10:03:03         eth0    371.72    363.64     58.21     28.63      0.00      0.00      0.00</p>
<p>Average:        IFACE   rxpck/s   txpck/s    rxkB/s    txkB/s   rxcmp/s   txcmp/s  rxmcst/s<br>Average:           lo      0.00      0.00      0.00      0.00      0.00      0.00      0.00<br>Average:         eth0    371.72    363.64     58.21     28.63      0.00      0.00      0.00<br>注意要点：<br>    -u表示查看系统cpu整体的使用状态，-P可以分开查询每个cpu的使用情况，其中对cpu的计数是从0开始的</p>
<h2 id="iostat"><a href="#iostat" class="headerlink" title="iostat"></a>iostat</h2><p>可以对系统磁盘的IO操作进行监控，同时也可以输出显示cpu的使用情况<br>语法：</p>
<p>iostat options [interval [count]]<br>options<br>说明<br>-c<br>显示cpu的使用情况<br>-d<br>显示磁盘的使用情况<br>-k<br>表示以KB为单位显示数据<br>-x device<br>指定要统计的磁盘设备</p>
<p>host-xxx-xxx-xxx-xxx:~ # iostat -d -k<br>Linux 3.0.76-0.11-default (host-10-200-43-168)  10/19/16    _x86<em>64</em></p>
<p>Device:            tps    kB_read/s    kB_wrtn/s    kB_read    kB_wrtn<br>xvda              3.65         0.08       144.00     556844  970083876<br>dm-0              0.69         0.07         2.75     494361   18546224<br>dm-1             35.31         0.00       141.22      13817  951330764<br>dm-2              0.01         0.00         0.03      24905     206736</p>
<p>KB_read/s:表示每秒读取的数据块数量<br>KB_wrtn/s:表示每秒写入的数据块数量<br>KB_read:表示总共读的数据块数量KB_wrtn:表示总共写的数据块数量</p>
<h2 id="netstat"><a href="#netstat" class="headerlink" title="netstat"></a>netstat</h2><pre><code>可以显示网络连接、端口状态和路由表等信息

</code></pre><p>语法：netstat [options]<br>常见option<br>说明<br>-a<br>显示所有的连接和监听的端口<br>-r<br>显示路由信息<br>-t<br>显示tcp连接<br>-u<br>显示udp连接<br>-l<br>显示连接状态为LISTEN的连接<br>-p<br>显示连接对应的PID<br>-n<br>以IP和端口的形式显示连接<br>比较常见用法：<br>netstat -plnt<br>netstat -puln<br>netstat -r</p>
<h2 id="free"><a href="#free" class="headerlink" title="free"></a>free</h2><pre><code>监控linux内存的使用情况

</code></pre><p>host-xxx-xxx-xxx-xxx:~ # free<br>             total       used       free     shared    buffers     cached<br>Mem:      16295408    3190164   13105244          0    1981048     766576<br>-/+ buffers/cache:     442540   15852868<br>Swap:      4193276          0    4193276</p>
<h2 id="uptime"><a href="#uptime" class="headerlink" title="uptime"></a>uptime</h2><pre><code>可以查看系统的启动时长和cpu的负载情况

</code></pre><p>host-xxx-xxx-xxx-xxx:~ # uptime<br> 10:12am  up 77 days 23:17,  3 users,  load average: 0.08, 0.12, 0.13<br>系统现在时间    启动时长   登录用户数量      1分钟内的平均负载  5分钟内的平均负载   15分钟内的平均负载<br>注意：load average的三个输出值如果大于系统逻辑cpu数量时，表示cpu繁忙，会影响系统性能</p>
]]></content>
      
        <categories>
            
            <category> Linux </category>
            
        </categories>
        
        
    </entry>
    
    <entry>
      <title><![CDATA[HBase架构详解]]></title>
      <url>/CDH/HBase/HBase%E6%9E%B6%E6%9E%84%E8%AF%A6%E8%A7%A3.html</url>
      <content type="html"><![CDATA[<h1 id="HBase架构详解"><a href="#HBase架构详解" class="headerlink" title="HBase架构详解"></a>HBase架构详解</h1><h2 id="Hbase架构"><a href="#Hbase架构" class="headerlink" title="Hbase架构"></a>Hbase架构</h2><h3 id="Hbase组件"><a href="#Hbase组件" class="headerlink" title="Hbase组件"></a>Hbase组件</h3><h4 id="client"><a href="#client" class="headerlink" title="client"></a>client</h4><ul>
<li>整个Hbase集群的入口</li>
<li>使用Hbase RPC机制与HMaster和HRegionserver通信</li>
<li>与HMaster通信进行管理类的操作</li>
<li>与HRegionserver通信进行读写类操作</li>
<li>包含访问HBase的接口,并维护cache来加快对HBase的访问,与HRegionserver交互</li>
</ul>
<h4 id="程序协调服务ZooKeeper"><a href="#程序协调服务ZooKeeper" class="headerlink" title="程序协调服务ZooKeeper"></a>程序协调服务ZooKeeper</h4><ul>
<li>保证任何时候,集群中只有一个Master(HA)</li>
<li>存贮所有Region的寻址入口</li>
<li>实时监控Region server的上线和下线信息.并实时通知给Master</li>
<li>存储Hbase的schema和table元数据</li>
</ul>
<h4 id="Hbase主节点Master"><a href="#Hbase主节点Master" class="headerlink" title="Hbase主节点Master"></a>Hbase主节点Master</h4><ul>
<li>管理用户对Table的增删改查操作</li>
<li>管理HRegionServer的负载均衡,调整Region分布</li>
<li>在Region Split后,负责新Region的分配</li>
<li>在HRegionServer停机后,负责失效HRegionServer上的Region迁移</li>
<li>HHMaster失效仅会导致所有元数据无法被修改,表的数据读写还是可以正常进行</li>
</ul>
<h4 id="HRegionServer节点"><a href="#HRegionServer节点" class="headerlink" title="HRegionServer节点"></a>HRegionServer节点</h4><ul>
<li>维护Hregion并往HDFS中写数据</li>
<li>当表的大小超过设置值的时候,Split Hregion</li>
<li>在HRegionServer停机后,负责失效HRegionServer上的Region迁移</li>
</ul>
<h3 id="Hbase架构-1"><a href="#Hbase架构-1" class="headerlink" title="Hbase架构"></a>Hbase架构</h3><h4 id="Hbase与ZooKeeper"><a href="#Hbase与ZooKeeper" class="headerlink" title="Hbase与ZooKeeper"></a>Hbase与ZooKeeper</h4><ul>
<li>Hbase元数据存储在ZooKeeper中</li>
<li>默认情况下,HBase管理ZooKeeper实例,比如,启动或者停止ZooKeeper</li>
<li>Zookeeper解决HBase单节点故障问题</li>
<li>HMaster与HRegionserver启动时会向Zookeeper注册</li>
</ul>
<h4 id="寻找RegionServer过程详解"><a href="#寻找RegionServer过程详解" class="headerlink" title="寻找RegionServer过程详解"></a>寻找RegionServer过程详解</h4><ul>
<li>ZooKeeper (读取Zookeeper找到-ROOT-表的位置)</li>
<li>-ROOT-(-ROOT-表包含.METAA.表所在的region列表,该表只会有一个Region;Zookeeper中记录了-ROOT-表的location)</li>
<li>.META.(.META.表包含所有的用户空间region列表,以及RegionServer的服务器地址)</li>
<li>用户表</li>
<li>Client第一次操作后,会将-ROOT-和.META.缓存到本地,不需要再访问Zookeeper</li>
</ul>
<h3 id="Hbase容错性"><a href="#Hbase容错性" class="headerlink" title="Hbase容错性"></a>Hbase容错性</h3><ul>
<li>Master容错: Zookeeper 重新选择一个新的Master<ul>
<li>无Master过程中,数据读取仍照正常进行</li>
<li>无Master过程中,region切分,负载均衡等无法进行</li>
</ul>
</li>
<li>RegionServer容错: 定时向Zookeeper汇报心跳,如果一段时间内出现心跳<ul>
<li>Master将该RegionServer上的Region重新分配到其他RegionServer上</li>
<li>失效服务器上”预写”日志由主服务器进行分割并派送给新的RegionServer</li>
</ul>
</li>
<li>Zookeeper容错<ul>
<li>Zookeeper高可靠的服务,不存在单点故障</li>
</ul>
</li>
</ul>
<h2 id="Hbase数据存储"><a href="#Hbase数据存储" class="headerlink" title="Hbase数据存储"></a>Hbase数据存储</h2><ul>
<li>HBase中的所有数据文件都存储在Hadoop HDFS文件系统上,格式主要有两种:<ul>
<li>HFile: HBase中KeyValue数据的存储格式,HFile是Hadoop的二进制格式文件,实际上StoreFile就是对HFile做了轻量级包装,即StoreFile底层就是HFile</li>
<li>HLog File: HBase 中WAL(Write Ahead Log)的存储格式,物理上是Hadoop的Sequence File带项目符号的内容 </li>
</ul>
</li>
</ul>
<h3 id="HRegionServer"><a href="#HRegionServer" class="headerlink" title="HRegionServer"></a>HRegionServer</h3><ul>
<li>HRegionServer管理一些列HRegion对象</li>
<li>每个HRegion对应Table中的一个Region,HRegion由多个HStore组成</li>
<li>每个HStore对应Table中一个Column Family的存储</li>
<li>Column Family就是一个集中的额存储单元,故将具有相同IO特性的Column放在一个Column Family会更高效</li>
</ul>
<h3 id="HStore-MemStore-和-StoreFile"><a href="#HStore-MemStore-和-StoreFile" class="headerlink" title="HStore (MemStore 和 StoreFile)"></a>HStore (MemStore 和 StoreFile)</h3><ul>
<li>Client写入 -&gt; 存储MemStore,一直到MemStore满 -&gt; Flush成一个StoreFile,直至增长到一定阈值 -&gt; 触发Compact合并操作 -&gt; 多个StoreFile合并成一个StoreFile,同时进行版本合并和数据删除 -&gt; 当StoreFiles Compact后,逐步形成越来越大的StoreFile -&gt; 单个StoreFile大小超过一定阈值后,触发Split操作,把当前Region Split成2个Region,被分割的Region会下线,新Split出2个子Region会被HMaster分配到相应的HRegionServer上,使得原先1个Region的压力得以分流到2个Region上</li>
<li>HBase只是增加数据,所有的更新和删除操作,都是在Compact阶段做的,所以,用户写操作只需要进入到内存即可立即返回,从而保证I/O高性能</li>
</ul>
<h4 id="StoreFile文件结构"><a href="#StoreFile文件结构" class="headerlink" title="StoreFile文件结构"></a>StoreFile文件结构</h4><ul>
<li>StoreFile以HFile格式保存在HDFS上</li>
<li>Data Block段:保存表中的额数据,这部分可以被压缩</li>
<li>Meta Block段(可选的):保存用户自定义的kv对,可以被压缩</li>
<li>File Info 段:Hfile的元信息,不压缩 ,用户也可以在这一部分添加自己的元信息</li>
<li>Data Block Index 段:Data Block的索引.每条索引的key是被索引的block的第一条记录的key</li>
<li>Meta Block Index段(可选的):Meta Block的索引</li>
<li>Trailer:这一段是定长的,保存的是每一段的偏移量</li>
</ul>
<h4 id="压缩"><a href="#压缩" class="headerlink" title="压缩"></a>压缩</h4><ul>
<li>HFile的Data Block,Meta Block通常采用压缩方式存储<ul>
<li>好处: 压缩之后可以大大减小网络IO和磁盘IO</li>
<li>坏处: 需要花费CPU进行压缩和解压缩</li>
</ul>
</li>
<li>Hfile支持的压缩格式: Gzip,Lzo,Snappy…</li>
</ul>
<h3 id="KeyValue-存储结构"><a href="#KeyValue-存储结构" class="headerlink" title="KeyValue 存储结构"></a>KeyValue 存储结构</h3><ul>
<li>HFile里面的每个KeyValue对就是一个简单的byte数组</li>
<li>KeyLength和ValueLength: 两个固定的长度,分别代表key和value的长度</li>
<li>Key部分: RowLength是固定长度的数值,表示RowKey的长度,Row就是RowKey</li>
<li>Column Family Length是固定长度的数值,表示Family的长度,接着就是Column Family,再接着就是Qualifier,然后是两个固定长度的数值,表示Time Stamp和Key Type(Put/Delete)</li>
<li>Value部分没有这么复杂的结构,就是纯粹的二进制数据</li>
</ul>
<h3 id="HLog文件结构"><a href="#HLog文件结构" class="headerlink" title="HLog文件结构"></a>HLog文件结构</h3><ul>
<li>HLog文件就是一个普通的Hadoop sequence File,Sequence File 的Key就是HLogKey对象,HLogkey中记录了写入数据的归属信息,除了table和region名字外,同时还包括sequence number和timestamp,timestamp是”写入时间”,sequence number的起始值为0,或者是最近一次存入文件系统中sequence number.</li>
<li>HLog Sequece File的Value是HBase的KeyValue对象,即对应HFile中的KeyValue</li>
</ul>
<h2 id="Hbase内部表"><a href="#Hbase内部表" class="headerlink" title="Hbase内部表"></a>Hbase内部表</h2><h3 id="Hbase内部表-1"><a href="#Hbase内部表-1" class="headerlink" title="Hbase内部表"></a>Hbase内部表</h3><ul>
<li>-ROOT-表<ul>
<li>存储.meta.表信息,-ROOT-表中仅有一行数据</li>
<li>Zookeeper中存储了-ROOT-表的位置信息</li>
</ul>
</li>
<li>.META.表<ul>
<li>主要存储HRegin的列表和HRegionServer的服务器地址</li>
</ul>
</li>
<li>Namespace表<ul>
<li>存储命名空间</li>
</ul>
</li>
</ul>
<h3 id="hbck修复错误表"><a href="#hbck修复错误表" class="headerlink" title="hbck修复错误表"></a>hbck修复错误表</h3><p>hbase hbck -fix</p>
<h2 id="Hbase管理命令"><a href="#Hbase管理命令" class="headerlink" title="Hbase管理命令"></a>Hbase管理命令</h2><h3 id="Flush"><a href="#Flush" class="headerlink" title="Flush"></a>Flush</h3><p>把内存中的数据写入硬盘</p>
<h3 id="Compact"><a href="#Compact" class="headerlink" title="Compact"></a>Compact</h3><h3 id="Region"><a href="#Region" class="headerlink" title="Region"></a>Region</h3>]]></content>
      
        <categories>
            
            <category> CDH </category>
            
            <category> HBase </category>
            
        </categories>
        
        
    </entry>
    
    <entry>
      <title><![CDATA[HBase入门]]></title>
      <url>/CDH/HBase/HBase%E5%85%A5%E9%97%A8.html</url>
      <content type="html"><![CDATA[<h1 id="HBase入门"><a href="#HBase入门" class="headerlink" title="HBase入门"></a>HBase入门</h1><h2 id="HBase概述–Hbase表特点"><a href="#HBase概述–Hbase表特点" class="headerlink" title="HBase概述–Hbase表特点"></a>HBase概述–Hbase表特点</h2><ul>
<li>大:一个表可以有数十亿行,上百万列</li>
<li>面向列:面向列(族)的存储和权限访问,列(族)独立索引</li>
<li>稀疏:对于为空(null)的列,并不占用存储空间,因此,表可以设计的非常稀疏</li>
<li>数据类型单一:HBase的数据类型都是字符串</li>
<li>无模式:每行都有一个可以排序的主键和任意多的列,列可以诶根据需要动态增加,同一张表中的不同的行可以有截然不同的列</li>
</ul>
<hr>
<h3 id="HBase-VS-RDBMS"><a href="#HBase-VS-RDBMS" class="headerlink" title="HBase VS RDBMS"></a>HBase VS RDBMS</h3><ul>
<li>HBase中的数据都是字符串类型</li>
<li>HBase只有普通的增删改查等操作,没有表之间的关联查询</li>
<li>HBase是基于列式存储的,而RDBMS是基于行式存储的</li>
<li>HBase适合存储大量数据,查询效果极高</li>
</ul>
<h3 id="术语说明"><a href="#术语说明" class="headerlink" title="术语说明"></a>术语说明</h3><h4 id="主键"><a href="#主键" class="headerlink" title="主键"></a>主键</h4><ul>
<li>用来检索记录的主键</li>
<li>访问HBasw表中的行,只有三种方式<ul>
<li>通过单个Row Key访问</li>
<li>通过Row Key的range</li>
<li>全表扫描</li>
</ul>
</li>
<li>主键为任意字符串,最大长度为64kb,按字典顺序存储,在HBase内部保存为字节数组</li>
</ul>
<h4 id="列族-Column-Family"><a href="#列族-Column-Family" class="headerlink" title="列族 (Column Family )"></a>列族 (Column Family )</h4><ul>
<li>列族在创建表的适合声明,一个列族可以包含多个列,列中的数据都是以二进制形式存在,没有数据类型</li>
<li>列族是一些列的集合</li>
<li>一个列族所有列成员是有着相同的前缀,比如,列courses:histor 和 courses:math都是 列族courses的成员.冒号(:)是列族的分隔符,用来区分前缀和列名</li>
</ul>
<h4 id="时间戳和存储单元"><a href="#时间戳和存储单元" class="headerlink" title="时间戳和存储单元"></a>时间戳和存储单元</h4><ul>
<li>HBase中通过row和columns确定的为一个存贮单元称为cell,每个cell都保存着同一份数据的多个版本</li>
<li>写入数据时,时间戳可以由HBase自动赋值(当前系统时间精确到毫秒),可以显示赋值</li>
<li>每个cell中,不同版本的数据按照时间的倒序排列</li>
<li>{row,column,version}元组就是一个HBase中的一个cell</li>
</ul>
<h2 id="HBase-HDFS目录分析"><a href="#HBase-HDFS目录分析" class="headerlink" title="HBase HDFS目录分析"></a>HBase HDFS目录分析</h2><h3 id="Region"><a href="#Region" class="headerlink" title="Region"></a>Region</h3><ul>
<li>WAL是RegionServer在处理数据插入和删除的过程中用来记录操作内容的一种日志,在0.94.x叫做.logs</li>
<li>向Hbase写入数据:<ul>
<li>存HLog文件,存储在HDFS上</li>
<li>存Region:<ul>
<li>内存</li>
<li>spill到磁盘,文件,hdfs文件</li>
</ul>
</li>
<li>公式:内存 * 0.4/60</li>
</ul>
</li>
</ul>
<h3 id="HDFS目录"><a href="#HDFS目录" class="headerlink" title="HDFS目录"></a>HDFS目录</h3><ul>
<li>/hbase/.tmp: 临时目录,当对表做创建和删除的适合,会将表move到该目录下,然后进行操作</li>
<li>/hbase/data: 核心目录,存储Hbase表的数据</li>
<li>默认情况下,目录下游两个子目录<ul>
<li>/hbase/data/default<ul>
<li>在用户创建表的时候,没有指定namespace时,表就创建在此目录下</li>
</ul>
</li>
<li>/hbase/data/hbase <ul>
<li>系统内部创建的表</li>
</ul>
</li>
</ul>
</li>
<li>/hbase/hbase.id<ul>
<li>存储的是集群的唯一的cluster id(uuid)</li>
</ul>
</li>
<li>/hbase/hbase.version: 集群版本号</li>
<li>/hbase/oldWALs<ul>
<li>对应的0.94.X版本中的.oldlogs目录</li>
<li>当/hbase/WALs目录中的logs没有用之后,会将这些logs移动到此目录下,HMaster会定期的进行清理</li>
</ul>
</li>
</ul>
<h2 id="HBase-Shell-操作"><a href="#HBase-Shell-操作" class="headerlink" title="HBase Shell 操作"></a>HBase Shell 操作</h2>]]></content>
      
        <categories>
            
            <category> CDH </category>
            
            <category> HBase </category>
            
        </categories>
        
        
    </entry>
    
    <entry>
      <title><![CDATA[MapReduce on HBase]]></title>
      <url>/CDH/HBase/MapReduce%20on%20HBase.html</url>
      <content type="html"><![CDATA[<h1 id="MapReduce-on-HBase"><a href="#MapReduce-on-HBase" class="headerlink" title="MapReduce on HBase"></a>MapReduce on HBase</h1><h2 id="HBase自带MapReduce工具使用"><a href="#HBase自带MapReduce工具使用" class="headerlink" title="HBase自带MapReduce工具使用"></a>HBase自带MapReduce工具使用</h2><p>导入环境变量 HADOOP_HOME HBASE_HOME</p>
<h3 id="HBase自带命令"><a href="#HBase自带命令" class="headerlink" title="HBase自带命令"></a>HBase自带命令</h3><h3 id="Tsv与BulkLoad"><a href="#Tsv与BulkLoad" class="headerlink" title="Tsv与BulkLoad"></a>Tsv与BulkLoad</h3><h3 id="Sqoop集成HBase"><a href="#Sqoop集成HBase" class="headerlink" title="Sqoop集成HBase"></a>Sqoop集成HBase</h3><h2 id="HBase表数据迁移"><a href="#HBase表数据迁移" class="headerlink" title="HBase表数据迁移"></a>HBase表数据迁移</h2><h2 id="HDFS数据导入HBase"><a href="#HDFS数据导入HBase" class="headerlink" title="HDFS数据导入HBase"></a>HDFS数据导入HBase</h2><h2 id="HBase-BulkLoad"><a href="#HBase-BulkLoad" class="headerlink" title="HBase BulkLoad"></a>HBase BulkLoad</h2><h2 id="BulkLoad源码分析"><a href="#BulkLoad源码分析" class="headerlink" title="BulkLoad源码分析"></a>BulkLoad源码分析</h2>]]></content>
      
        <categories>
            
            <category> CDH </category>
            
            <category> HBase </category>
            
        </categories>
        
        
    </entry>
    
    <entry>
      <title><![CDATA[linux系统安全日志分析]]></title>
      <url>/Linux/linux%E7%B3%BB%E7%BB%9F%E5%AE%89%E5%85%A8%E6%97%A5%E5%BF%97%E5%88%86%E6%9E%90.html</url>
      <content type="html"><![CDATA[<h1 id="linux系统安全日志分析"><a href="#linux系统安全日志分析" class="headerlink" title="linux系统安全日志分析"></a>linux系统安全日志分析</h1><p>我们主要讲一下Linux环境中的系统记帐和系统日志管理以及怎么用一些工具更加方便有效的管理日志信息。</p>
]]></content>
      
        <categories>
            
            <category> Linux </category>
            
        </categories>
        
        
    </entry>
    
    <entry>
      <title><![CDATA[HQL执行过程]]></title>
      <url>/CDH/Hadoop/HQL%E6%89%A7%E8%A1%8C%E8%BF%87%E7%A8%8B.html</url>
      <content type="html"><![CDATA[<p>MapReduce 的容错机制</p>
<ol>
<li>重复执行 重复4次后失败</li>
<li>推测执行 保证不会因为某一两个tasktracker失败而影响整个计算</li>
</ol>
<p>数据仓库是一个面向主题的,集成的,不可更新的,随时间不变化的数据集合,它用户支持企业或组织的决策分析处理</p>
<p>数据源:业务数据系统,文档系统,其他数据<br>数据存储及管理: 抽取(Extract),转换(Transform),装载(Load)<br>仓库引擎:服务器,<br>前端展示:数据查询,数据报表,数据分析,各类应用</p>
<p>OLTP应用 人机事物处理 银行转账<br>OLAP应用 人机分析处理 商品推荐系统</p>
<p>数据仓库中的数据模型:<br>星型模型</p>
<p><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">+----------------------+                                 +-----------------+</div><div class="line">|                      |                                 |                 |</div><div class="line">|                      |                                 |                 |</div><div class="line">|     客户信息             |                                 |      订单信息       |</div><div class="line">|                      |                                 |                 |</div><div class="line">|                      |                                 |                 |</div><div class="line">+----------------------+                         +-------+-----------------+</div><div class="line">                       |                         |</div><div class="line">                       |                         |</div><div class="line">                       |                         |</div><div class="line">                       +-----+-------------------+</div><div class="line">                             |                   |</div><div class="line">                             |                   |</div><div class="line">                             |  商品信息             |</div><div class="line">                             |                   |</div><div class="line">                             |                   |</div><div class="line">                 +-----------+--------------+----+</div><div class="line">                 |                          |    |</div><div class="line">                 |                          |    +--------+---------------------------+</div><div class="line">                 |                          |             |                           |</div><div class="line">+----------------+                          |             |    物流信息                   |</div><div class="line">|                |                          |             |                           |</div><div class="line">|   厂家信息         |                          |             +---------------------------+</div><div class="line">|                |                          |</div><div class="line">+----------------+                          |</div><div class="line">                                            |</div><div class="line">                                            |</div><div class="line">                                            |</div><div class="line">                                      +-----+----------+</div><div class="line">                                      |                |</div><div class="line">                                      |                |</div><div class="line">                                      |    促销信息        |</div><div class="line">                                      |                |</div><div class="line">                                      +----------------+</div></pre></td></tr></table></figure>
</p>
<ul>
<li>Hive 是建立在Hadoop HDFS上的数据仓库基础架构</li>
<li>Hive 可以用来进行数据提取转化加载(ETL)</li>
<li>Hive 定义了简单的类似SQL查询语言,称为HQL它允许数据SQL的用户查询数据</li>
<li>Hive 允许熟悉MapReduce开发者的开发自定义的mapper和reducer来处理内建的mapper和reducer无法完成的复杂的分析工作</li>
<li>Hive 是SQL解析引擎,他将SQL语句专一成M/R Job然后再Hadoop执行</li>
<li><p>Hive 的表其实就是HDFS的目录/文件</p>
</li>
<li><p>Hive 元数据</p>
<ul>
<li>Hive 将元数据存储在数据库中(metastore),支持mysql,derby等数据库</li>
<li>Hive 中的元数据包括表的名字,表的列和分区及其属性,标的属性(是否为外部表等),标的数据所在目录等</li>
</ul>
</li>
</ul>
<h3 id="HQL的执行过程"><a href="#HQL的执行过程" class="headerlink" title="HQL的执行过程"></a>HQL的执行过程</h3><ul>
<li>解析器,编译器,优化器完成HQL查询语句从词法分析,语法分析,编译,优化以及查询计划(Plan)的生成,生成的查询计划存储在HDFS中,并在随后又MapReduce调用执行</li>
</ul>
<h3 id="Hive-的安装"><a href="#Hive-的安装" class="headerlink" title="Hive 的安装"></a>Hive 的安装</h3><ul>
<li>嵌入模式<ul>
<li>元数据信息被存储在Hive自带的Derby数据库中</li>
<li>值允许创建一个连接</li>
<li>多用于Demo</li>
</ul>
</li>
<li>本地模式<ul>
<li>元数据信息被存储在MySQL数据库中</li>
<li>MySQL数据库与Hive运行在同一台物理机器上</li>
<li>多用户与开发和测试</li>
</ul>
</li>
<li>远程模式<ul>
<li>Hive和MySQL运行在不同的操作系统上</li>
</ul>
</li>
</ul>
]]></content>
      
        <categories>
            
            <category> CDH </category>
            
            <category> Hadoop </category>
            
        </categories>
        
        
    </entry>
    
    <entry>
      <title><![CDATA[Hadoop 性能调优与运维]]></title>
      <url>/CDH/Hadoop/Hadoop%E6%80%A7%E8%83%BD%E8%B0%83%E4%BC%98%E4%B8%8E%E8%BF%90%E7%BB%B4.html</url>
      <content type="html"><![CDATA[<h1 id="Hadoop-性能调优与运维"><a href="#Hadoop-性能调优与运维" class="headerlink" title="Hadoop 性能调优与运维"></a>Hadoop 性能调优与运维</h1>]]></content>
      
        <categories>
            
            <category> CDH </category>
            
            <category> Hadoop </category>
            
        </categories>
        
        
    </entry>
    
    <entry>
      <title><![CDATA[Hadoop 用户行为分析项目之应用概述]]></title>
      <url>/CDH/Hadoop/Hadoop%20%E7%94%A8%E6%88%B7%E8%A1%8C%E4%B8%BA%E5%88%86%E6%9E%90%E9%A1%B9%E7%9B%AE%E4%B9%8B%E5%BA%94%E7%94%A8%E6%A6%82%E8%BF%B0.html</url>
      <content type="html"><![CDATA[<h1 id="hadoop用户行为分析项目之应用概述"><a href="#hadoop用户行为分析项目之应用概述" class="headerlink" title="hadoop用户行为分析项目之应用概述"></a>hadoop用户行为分析项目之应用概述</h1><h2 id="应用概述"><a href="#应用概述" class="headerlink" title="应用概述"></a>应用概述</h2><h3 id="hadoop业务场景-应用场景"><a href="#hadoop业务场景-应用场景" class="headerlink" title="hadoop业务场景,应用场景"></a>hadoop业务场景,应用场景</h3><ul>
<li>业务场景:<ul>
<li>时延</li>
<li>吞吐量</li>
</ul>
</li>
<li>应用场景:<ul>
<li>MapReduce计算模型</li>
<li>海量数据的离线分析</li>
<li>静态数据源</li>
</ul>
</li>
</ul>
<h3 id="用户行为分析平台搭建注意事项"><a href="#用户行为分析平台搭建注意事项" class="headerlink" title="用户行为分析平台搭建注意事项:"></a>用户行为分析平台搭建注意事项:</h3><ul>
<li>高可用性</li>
<li>NNA和NNS节点配置注意事项<p><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;</div><div class="line">&lt;configuration&gt;</div><div class="line">    &lt;!-- 指定HDFS的NameService为cluster1,需要和core-site.xml中的保持一致 --&gt;</div><div class="line">    &lt;property&gt;</div><div class="line">        &lt;name&gt;dfs.nameservice&lt;/name&gt;</div><div class="line">        &lt;value&gt;cluster1&lt;/value&gt;</div><div class="line">    &lt;/property&gt;</div><div class="line">    &lt;!-- cluster1下面又两个NameNode,分别是nna,nns --&gt;</div><div class="line">    &lt;property&gt;</div><div class="line">        &lt;name&gt;dfs.ha.namenodes.cluster1&lt;/name&gt;</div><div class="line">        &lt;value&gt;nna,nns&lt;/value&gt;</div><div class="line">    &lt;/property&gt;</div><div class="line">    &lt;!-- nna的RPC通信地址 --&gt;</div><div class="line">    &lt;property&gt;</div><div class="line">        &lt;name&gt;dfs.namenode.rpc-address.cluster1.nna&lt;/name&gt;</div><div class="line">        &lt;value&gt;nna:9000&lt;/value&gt;</div><div class="line">    &lt;/property&gt;</div><div class="line">    &lt;!-- nns的RPC通信地址 --&gt;</div><div class="line">    &lt;property&gt;</div><div class="line">        &lt;name&gt;dfs.namenode.rpc.address.cluster.nns&lt;/name&gt;</div><div class="line">        &lt;value&gt;nns:9000&lt;/value&gt;</div><div class="line">    &lt;/property&gt;</div><div class="line">    &lt;!-- nna的http通信地址 --&gt;</div><div class="line">    &lt;property&gt;</div><div class="line">        &lt;name&gt;dfs.namenode.rpc.address.cluster.nna&lt;/name&gt;</div><div class="line">        &lt;value&gt;nna:50070&lt;/value&gt;</div><div class="line">    &lt;/property&gt;</div><div class="line">    &lt;!-- nns的http通信地址 --&gt;</div><div class="line">    &lt;property&gt;</div><div class="line">        &lt;name&gt;dfs.namenode.rpc.address.cluster.nns&lt;/name&gt;</div><div class="line">        &lt;value&gt;nns:50070&lt;/value&gt;</div><div class="line">    &lt;/property&gt;</div><div class="line">    &lt;!-- 共享存储目录地址,节点保证是奇数 --&gt;</div><div class="line">    &lt;property&gt;</div><div class="line">        &lt;name&gt;dfs.namenode.shared.edits.dir&lt;/name&gt;</div><div class="line">        &lt;value&gt;qjournal://dn1:8485;dn2:8485;dn3:8485/cluster1&lt;/value&gt;</div><div class="line">    &lt;/property&gt;</div><div class="line">    &lt;!-- 配置失败自动切换实现方式 --&gt;</div><div class="line">    &lt;property&gt;</div><div class="line">        &lt;name&gt;dfs.client.failover.proxy.provider.cluster1&lt;/name&gt;</div><div class="line">        &lt;value&gt;org.apache.hadoop.hdfs.server.namenode.ha.ConfigureFailoverProxyProvider&lt;/value&gt;</div><div class="line">    &lt;/property&gt;</div><div class="line">    &lt;!-- 配置隔离机制 --&gt;</div><div class="line">    &lt;property&gt;</div><div class="line">        &lt;name&gt;dfs.ha.fencing.methods&lt;/name&gt;</div><div class="line">        &lt;value&gt;sshfence&lt;/value&gt;</div><div class="line">    &lt;/property&gt;</div><div class="line">    &lt;!-- 使用隔离机制是需要ssh免密码登录 --&gt;</div><div class="line">    &lt;property&gt;</div><div class="line">        &lt;name&gt;dfs.ha.fencing.ssh.private-key-files&lt;/name&gt;</div><div class="line">        &lt;value&gt;/home/hadoop/.ssh/id_rsa&lt;/value&gt;</div><div class="line">    &lt;/property&gt;</div><div class="line">    &lt;!-- 指定NameNode的元数据在JournalNode上的存放位置 --&gt;</div><div class="line">    &lt;property&gt;</div><div class="line">        &lt;name&gt;dfs.journalnode.edits.dir&lt;/name&gt;</div><div class="line">        &lt;value&gt;/home/hadoop/tmp/journal&lt;/value&gt;</div><div class="line">    &lt;/property&gt;</div><div class="line">    &lt;!-- 指定高可用自动转换机制 --&gt;</div><div class="line">    &lt;property&gt;</div><div class="line">        &lt;name&gt;dfs.ha.automatic-failover.enabled&lt;/name&gt;</div><div class="line">        &lt;value&gt;true&lt;/value&gt;</div><div class="line">    &lt;/property&gt;</div><div class="line">    &lt;!-- 指定 --&gt;</div><div class="line">    &lt;!-- 指定DataNode数据存储地址 --&gt;</div><div class="line">    &lt;property&gt;</div><div class="line">        &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt;</div><div class="line">        &lt;value&gt;/home/hadoop/dfs/data&lt;/value&gt;</div><div class="line">    &lt;/property&gt;</div><div class="line">    &lt;!-- 指定数据冗余份数 --&gt;</div><div class="line">    &lt;property&gt;</div><div class="line">        &lt;name&gt;dfs.replication&lt;/name&gt;</div><div class="line">        &lt;value&gt;3&lt;/value&gt;</div><div class="line">    &lt;/property&gt;</div><div class="line">    &lt;!-- 指定可以通过web访问HDFS目录 --&gt;</div><div class="line">    &lt;property&gt;</div><div class="line">        &lt;name&gt;dfs.webhdfs.enabled&lt;/name&gt;</div><div class="line">        &lt;value&gt;true&lt;/value&gt;</div><div class="line">    &lt;/property&gt;</div><div class="line">    &lt;!-- 保证数据恢复 --&gt;</div><div class="line">    &lt;property&gt;</div><div class="line">        &lt;name&gt;dfs.journalnode.http-address&lt;/name&gt;</div><div class="line">        &lt;value&gt;0.0.0.0:8480&lt;/value&gt;</div><div class="line">    &lt;/property&gt;</div><div class="line">    &lt;property&gt;</div><div class="line">        &lt;name&gt;dfs.journalnode.rpc-address&lt;/name&gt;</div><div class="line">        &lt;value&gt;0.0.0.0:8485&lt;/value&gt;</div><div class="line">    &lt;/property&gt;</div><div class="line">    &lt;!-- ZooKeeper集群的地址和端口,注意,数量一定是奇数,且不少于三个节点 --&gt;</div><div class="line">    &lt;property&gt;</div><div class="line">        &lt;name&gt;ha.zookeeper.quorum&lt;/name&gt;</div><div class="line">        &lt;value&gt;dn1:2181.dn2:2181,dn3:2181&lt;/value&gt;</div><div class="line">    &lt;/property&gt;</div><div class="line">&lt;/configuration&gt;</div></pre></td></tr></table></figure>
</p>
</li>
</ul>
<p><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">&lt;configuration&gt;</div><div class="line">    &lt;!-- 指定hdfs的nameservice为cluster1,是NameNode的URI --&gt;</div><div class="line">    &lt;property&gt;</div><div class="line">        &lt;name&gt;dfs.defaultFS&lt;/name&gt;</div><div class="line">        &lt;value&gt;hdfs://cluster1&lt;/value&gt;</div><div class="line">    &lt;/property&gt;</div><div class="line">    &lt;property&gt;</div><div class="line">        &lt;name&gt;io.file.buffer.size&lt;/name&gt;</div><div class="line">        &lt;value&gt;131072&lt;/value&gt;</div><div class="line">    &lt;/property&gt;</div><div class="line">    &lt;!-- 指定hadoop临时目录 --&gt;</div><div class="line">    &lt;property&gt;</div><div class="line">        &lt;name&gt;dhadoop.tmp.dir&lt;/name&gt;</div><div class="line">        &lt;value&gt;/home/hadoop/tmp&lt;/value&gt;</div><div class="line">    &lt;/property&gt;</div><div class="line">    &lt;!-- 指定可以在任何IP访问 --&gt;</div><div class="line">    &lt;property&gt;</div><div class="line">        &lt;name&gt;hadoop.proxyuser.hduser.hosts&lt;/name&gt;</div><div class="line">        &lt;value&gt;*&lt;/value&gt;</div><div class="line">    &lt;/property&gt;</div><div class="line">    &lt;!-- 指定所有用户可以访问 --&gt;</div><div class="line">    &lt;property&gt;</div><div class="line">        &lt;name&gt;hadoop.proxyuser.hduser.groups&lt;/name&gt;</div><div class="line">        &lt;value&gt;*&lt;/value&gt;</div><div class="line">    &lt;/property&gt;</div><div class="line">    &lt;!-- 指定zookeeper地址 --&gt;</div><div class="line">    &lt;property&gt;</div><div class="line">        &lt;name&gt;ha.zookeeper.quorum&lt;/name&gt;</div><div class="line">        &lt;value&gt;dn1:2181.dn2:2181,dn3:2181&lt;/value&gt;</div><div class="line">    &lt;/property&gt;</div><div class="line">&lt;/configuration&gt;</div></pre></td></tr></table></figure>
</p>
]]></content>
      
        <categories>
            
            <category> CDH </category>
            
            <category> Hadoop </category>
            
        </categories>
        
        
    </entry>
    
    <entry>
      <title><![CDATA[Hadoop构造]]></title>
      <url>/CDH/Hadoop/Hadoop%E6%9E%84%E9%80%A0.html</url>
      <content type="html"><![CDATA[<h2 id="Hadoop的组成"><a href="#Hadoop的组成" class="headerlink" title="Hadoop的组成"></a>Hadoop的组成</h2><ul>
<li>HDFS:分布式文件系统,存储海量的数据</li>
<li>MapReduce:并行处理框架,实现任务分解和调度</li>
<li>搭建大学数据仓库,PB级数据的存储,处理,分析,统计的业务</li>
</ul>
<hr>
<p>HIVE 只需编写SQL语句转化为Hadoop任务执行</p>
<p>HBASE 放弃事务特性 追求更高的扩展</p>
<ul>
<li>提供数据的随机读写和实时访问,实现对表数据的读写功能</li>
</ul>
<p>Zookeeper</p>
]]></content>
      
        <categories>
            
            <category> CDH </category>
            
            <category> Hadoop </category>
            
        </categories>
        
        
    </entry>
    
    <entry>
      <title><![CDATA[Hive 基本原理]]></title>
      <url>/CDH/Hadoop/Hive%20%E5%9F%BA%E6%9C%AC%E5%8E%9F%E7%90%86.html</url>
      <content type="html"><![CDATA[<h1 id="Hive基本原理"><a href="#Hive基本原理" class="headerlink" title="Hive基本原理"></a>Hive基本原理</h1><h2 id="Hive的体系结构"><a href="#Hive的体系结构" class="headerlink" title="Hive的体系结构"></a>Hive的体系结构</h2><ul>
<li>用户接口</li>
<li>Thrift服务器</li>
<li>元数据存储 </li>
</ul>
]]></content>
      
        <categories>
            
            <category> CDH </category>
            
            <category> Hadoop </category>
            
        </categories>
        
        
    </entry>
    
    <entry>
      <title><![CDATA[YARN原理]]></title>
      <url>/CDH/Hadoop/YARN%E5%8E%9F%E7%90%86.html</url>
      <content type="html"><![CDATA[<h1 id="YARN"><a href="#YARN" class="headerlink" title="YARN"></a>YARN</h1><p>Yet Another Resource Negotiator<br>YARN的基本思想：将JobTracker两个主要得功能分离成单独的组件，一个全局的ResourceManager和每个应用对应的ApplicationMaster</p>
<h2 id="YARN的组件"><a href="#YARN的组件" class="headerlink" title="YARN的组件"></a>YARN的组件</h2><ul>
<li>ResourceManager<ul>
<li>一个纯粹的调度器</li>
<li>根据应用程序的资源请求严格限制系统的可用资源</li>
<li>在保证容量，公平性以及服务等级的情况下，优化集群支援利用率，让所有资源都得到充分利用</li>
<li>有可插拔的调度器来应用不同的调度算法，如注重容量调度还是中医公平调度</li>
</ul>
</li>
<li>ApplicationMaster<ul>
<li>负责与ResourceManager协商资源，并和NodeManager协同工作来执行和监控Container以及他们的资源消耗</li>
<li>有责任与ResourceManager协商并获取合适的资源Container,跟踪它们的状态，以及监控其进展</li>
<li>在真实环境中，每一个应用都是有自己的ApplicationMaster实例，但是也可为一组应用提供一个ApplicationMaster,比如Pig或者Hive的ApplicationMaster</li>
</ul>
</li>
<li>资源模型<ul>
<li>YARN提供了通用的应用资源模型</li>
<li>一个应用可以通过ApplicationMaster请求非常具体的资源<ul>
<li>资源名称（包括主机名，机架名以及复制的网络拓扑）</li>
<li>内存量</li>
<li>CPU</li>
<li>其他资源：磁盘和网络IO等</li>
</ul>
</li>
</ul>
</li>
<li>ResourceRequest和Container<ul>
<li>一个应用程序通过Application请求特定的ResourceRequest来满足资源需求</li>
<li>Scheduler会分配一个Container来响应资源请求</li>
<li>Container是一种资源分配形式，为应用程序授予在特定主机上使用资源（如内存，CPU）的权利</li>
<li>ApplicationMaster取走Container，并且交给NodeManager,NodeManager会利用相应的资源来启动container的任务进程</li>
</ul>
</li>
</ul>
]]></content>
      
        <categories>
            
            <category> CDH </category>
            
            <category> Hadoop </category>
            
        </categories>
        
        
    </entry>
    
    <entry>
      <title><![CDATA[安装 hadoop YARN]]></title>
      <url>/CDH/Hadoop/%E5%AE%89%E8%A3%85hadoop%20YARN.html</url>
      <content type="html"><![CDATA[<h1 id="安装-hadoop-YARN"><a href="#安装-hadoop-YARN" class="headerlink" title="安装 hadoop YARN"></a>安装 hadoop YARN</h1><h2 id="YARN安装简介"><a href="#YARN安装简介" class="headerlink" title="YARN安装简介"></a>YARN安装简介</h2><h3 id="hadoop的安装简介"><a href="#hadoop的安装简介" class="headerlink" title="hadoop的安装简介"></a>hadoop的安装简介</h3><ul>
<li>hadoop的各个组件均采用XML文件进行配置，这些配置文件都在etc/hadoop子目录中</li>
<li>core-site.xml文件用于配置通用属性</li>
<li>hdfs-site.xml文件用于配置HDFS属性</li>
<li>mapred-site.xml文件用于配置MapReduce属性</li>
<li>yarn-site.xml用于配置YARN的属性</li>
</ul>
<h3 id="hadoop的三种运行模式"><a href="#hadoop的三种运行模式" class="headerlink" title="hadoop的三种运行模式"></a>hadoop的三种运行模式</h3><ul>
<li>单机模式<ul>
<li>单机模式是hadoop的默认模式</li>
<li>因为不需要与其他节点交互，单机模式就不使用HDFS，也不加载任何hadoop的守护进程</li>
<li>所有程序都在同一个JVM上执行</li>
<li>该模式主要用于开发调试MapReduce程序的应用逻辑</li>
</ul>
</li>
<li>伪分布式<ul>
<li>hadoop守护进程运行在本地机器上，模拟一个小规模集群</li>
<li>hadoop的每个守护进程都运行在单独的Java进程中</li>
</ul>
</li>
<li>全分布式<ul>
<li>hadoop守护进程运行在一个集群上</li>
<li>例如：集群环境中，NameNode和ResourceManager各运行于一台单独的主机为Master，而其他的主机为DataNode和NodeManager，为slave</li>
</ul>
</li>
</ul>
<h3 id="hadoop与Java版本"><a href="#hadoop与Java版本" class="headerlink" title="hadoop与Java版本"></a>hadoop与Java版本</h3><ul>
<li>hadoop的开发和测试是在OpenJDK和OracleJDK上进行的</li>
<li>hadoop支持Java7和比较新的Java6版本</li>
</ul>
<h2 id="安装YARN前的准备"><a href="#安装YARN前的准备" class="headerlink" title="安装YARN前的准备"></a>安装YARN前的准备</h2><h3 id="需要的软件"><a href="#需要的软件" class="headerlink" title="需要的软件"></a>需要的软件</h3><ul>
<li>操作系统</li>
<li>hadoop 2.6 stable</li>
<li>OpenJDK 1.7</li>
<li>openssh</li>
</ul>
<h3 id="安装JDK及SSH"><a href="#安装JDK及SSH" class="headerlink" title="安装JDK及SSH"></a>安装JDK及SSH</h3><h3 id="创建用户和组"><a href="#创建用户和组" class="headerlink" title="创建用户和组"></a>创建用户和组</h3><ul>
<li>创建hadoop组 <code>addgroup hadoop</code></li>
<li>创建用户hduser并添加到hadoop组中 <code>adduser --ingroup hadoop hduser</code><h3 id="配置SSH无密码登录"><a href="#配置SSH无密码登录" class="headerlink" title="配置SSH无密码登录"></a>配置SSH无密码登录</h3></li>
<li>检查hduser是否可以以无密码的方式登录localhost <code>ssh localhost</code></li>
<li>如果需要输入密码<p><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">ssh-keygen -t dsa</div><div class="line">cat ~/.ssh/id_dsa.pub &gt;&gt; ~/.ssh/authorized_keys</div></pre></td></tr></table></figure>
</p>
</li>
</ul>
<h3 id="解压缩以及设置hadoop环境变量"><a href="#解压缩以及设置hadoop环境变量" class="headerlink" title="解压缩以及设置hadoop环境变量"></a>解压缩以及设置hadoop环境变量</h3><ul>
<li><p>解压缩hadoop</p>
<p><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">gunzip hadoop-2.6.0.tar.gz</div><div class="line">tar -xvf hadoop-2.6.0.tar</div></pre></td></tr></table></figure>
</p>
</li>
<li><p>假设解压后的文件路径是在/hadoop/下 在解压后的文件中,找到并编辑hadoop环境变量文件 ./hadoop-2.6.0/etc/hadoop/hadoop-env.sh</p>
</li>
<li>设置Java Home: <code>export JAVA_HOME=/usr/lib/jvm/java-7-openjdk-amd64</code></li>
<li>设置hadoop prefix: <code>HADOOP_PREFIX=/hadoop/hadoop-2.6.0</code></li>
</ul>
<h3 id="测试hadoop"><a href="#测试hadoop" class="headerlink" title="测试hadoop"></a>测试hadoop</h3><p><code>bin/hadoop</code></p>
<ul>
<li><p>测试hadoop:</p>
<h2 id="伪分布式安装"><a href="#伪分布式安装" class="headerlink" title="伪分布式安装"></a>伪分布式安装</h2><h3 id="修改core-site-xml文件"><a href="#修改core-site-xml文件" class="headerlink" title="修改core-site.xml文件"></a>修改core-site.xml文件</h3><p><code>etc/hadoop/core-site.xml</code></p>
</li>
<li><p>fs.defaultFS配置NameNode的URI</p>
<p><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">&lt;configuration&gt;</div><div class="line">    &lt;property&gt;</div><div class="line">        &lt;name&gt;fs.defaultFS&lt;/name&gt;</div><div class="line">        &lt;value&gt;hdfs://localhost:9000&lt;/value&gt;</div><div class="line">    &lt;/property&gt;</div><div class="line">&lt;/configuration&gt;</div></pre></td></tr></table></figure>
</p>
</li>
</ul>
<h3 id="修改hdfs-site-xml文件"><a href="#修改hdfs-site-xml文件" class="headerlink" title="修改hdfs-site.xml文件"></a>修改hdfs-site.xml文件</h3><p><code>etc/hadoop/hdfs-site.xml</code></p>
<ul>
<li>dfs.replication设置块的复制数量<p><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">&lt;configuration&gt;</div><div class="line">    &lt;property&gt;</div><div class="line">        &lt;name&gt;dfs.replication&lt;/name&gt;</div><div class="line">        &lt;value&gt;1&lt;/value&gt;</div><div class="line">    &lt;/property&gt;</div><div class="line">&lt;/configuration&gt;</div></pre></td></tr></table></figure>
</p>
</li>
</ul>
<h3 id="修改mapred-site-xml文件"><a href="#修改mapred-site-xml文件" class="headerlink" title="修改mapred-site.xml文件"></a>修改mapred-site.xml文件</h3><p><code>etc/hadoop/mapred-site.xml</code></p>
<ul>
<li>mapreduce.framework.name配置MapReduce应用使用YARN框架<p><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">&lt;configuration&gt;</div><div class="line">    &lt;property&gt;</div><div class="line">        &lt;name&gt;mapreduce.framework.name&lt;/name&gt;</div><div class="line">        &lt;value&gt;yarn&lt;/value&gt;</div><div class="line">    &lt;/property&gt;</div><div class="line">&lt;/configuration&gt;</div></pre></td></tr></table></figure>
</p>
</li>
</ul>
<h3 id="修改yarn-site-xml文件"><a href="#修改yarn-site-xml文件" class="headerlink" title="修改yarn-site.xml文件"></a>修改yarn-site.xml文件</h3><p><code>etc/hadoop/yarn-site.xml</code></p>
<ul>
<li>yarn.nodemanager.aux-services:为NodeManager配置MapReduce应用的Shuffle服务<p><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">&lt;configuration&gt;</div><div class="line">    &lt;property&gt;</div><div class="line">        &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;</div><div class="line">        &lt;value&gt;mapreduce_shuffle&lt;/value&gt;</div><div class="line">    &lt;/property&gt;</div><div class="line">&lt;/configuration&gt;</div></pre></td></tr></table></figure>
</p>
</li>
</ul>
<h3 id="检查yarn是否安装成功"><a href="#检查yarn是否安装成功" class="headerlink" title="检查yarn是否安装成功"></a>检查yarn是否安装成功</h3><ul>
<li>启动资源管理器(ResourceManager)和节点管理器(NodeManager)守护进程<br><code>sbin/start-yarn.sh</code></li>
<li>访问资源管理器的web接口<br><code>http://localhost:8088/</code></li>
</ul>
]]></content>
      
        <categories>
            
            <category> CDH </category>
            
            <category> Hadoop </category>
            
        </categories>
        
        
    </entry>
    
    <entry>
      <title><![CDATA[影响linux系统的硬件因素]]></title>
      <url>/Linux/%E5%BD%B1%E5%93%8Dlinux%E7%B3%BB%E7%BB%9F%E7%9A%84%E7%A1%AC%E4%BB%B6%E5%9B%A0%E7%B4%A0.html</url>
      <content type="html"><![CDATA[<h1 id="影响linux系统的硬件因素"><a href="#影响linux系统的硬件因素" class="headerlink" title="影响linux系统的硬件因素"></a>影响linux系统的硬件因素</h1><pre><code>1、cpu
    ①CPU的速度与性能很大程度决定了系统整体的性能，cpu数量越多，主频越高，服务
     器性能越好
    ②利用超线程的cpu，在同一时间可运行多个线程，提高系统性能
    ③在linux中，只有运行SMP内核才能支持超线程
    ④linux会把多核的处理器当成多个单独的cpu来识别，如2个4核的cpu会被识别成8个
     单核cpu，两者的性能不等价
    ⑤安装的cpu数量越多，超线程对性能的提高效果越少
    ⑥邮件服务器、动态web服务器比较可能出现cpu瓶颈
2、内存
    ①内存太小，进程容易被堵塞；内存太大，会导致资源浪费
    ②虚拟内存性能比不上物理内存，占用过多的虚拟内存，应用程序的性能会明显下降
    ③在32位linux系统上，最多只能支持8G的物理内存
    ④由于处理器寻址范围的限制，在32位系统上，应用程序单个进程最多只能使用2G内
     存
    ⑤打印服务器、数据库服务器、静态web服务器容易出现内存瓶颈
3、磁盘IO性能
    使用磁盘RAID技术，选用合适的RAID级别，提高磁盘性能
4、网络带宽

</code></pre><h2 id="linux操作系统优化方向"><a href="#linux操作系统优化方向" class="headerlink" title="linux操作系统优化方向"></a>linux操作系统优化方向</h2><pre><code>1、系统安装优化
    ①磁盘划分：
        读写操作频繁而对数据安全性要求不高的可以将磁盘做成RAID0
        对数据安全性要求较高而对读写没有特殊要求的可以做成RAID1
        对读要求和数据安全性较高，而写要求不高的可以做成RAID5
        对读写要求和数据安全性要求都高的可以做成RAID0+1
    ②虚拟内存的设置
        物理内存小于4G，设置SWAP为内存的2倍
        物理内存大于4G小于16G，设置SWAP大小与物理内存相等
        物理内存大于16G的，可以不设置SWAP，但设置一定大小的SWAP还是有一定作用的
2、内核参数的优化
3、文件系统优化
    linux系统下可选的文件系统有ext2、ext3、ext4、xfs等
    ext2与ext3的区别在于ext3增加了日志文件功能
    xfs是一种高级日志文件系统，具有优秀的日志记录功能，可扩展性强、快速写入性能
    等优点
4、应用程序资源的优化

</code></pre>]]></content>
      
        <categories>
            
            <category> Linux </category>
            
        </categories>
        
        
    </entry>
    
    <entry>
      <title><![CDATA[SQL基础]]></title>
      <url>/MySQL/SQL%E5%9F%BA%E7%A1%80.html</url>
      <content type="html"><![CDATA[<h1 id="SQL基础"><a href="#SQL基础" class="headerlink" title="SQL基础"></a>SQL基础</h1><ul>
<li>DDL (Data Definition Languages):数据定义语音,这些语句定义了不同的数据段,数据库,表,列,索引等数据库对象.常用的语句关键字主要包括create,drop,alter等;</li>
<li>DML (Data Manipulation Languages): 数据操纵语句,用于添加,删除,更新和查询数据库记录,并检查数据完整性,常用的语句关键字主要包括 insert,delete,update,select等;</li>
<li>DCL (Data Control Language): 数据控制语句,用于控制不同数据段直接的许可和访问级别的语句.这些语句定义了数据库,表,字段,用户的访问权限和安全级别.主要的语句关键字包括grant,revoke<h2 id="DDL"><a href="#DDL" class="headerlink" title="DDL"></a>DDL</h2>对数据库内部的对象进行创建.删除,修改等操作的语音.和DML最大的差别是DML只是对表内部数据操作,而不涉及表的定义,结构的修改,更不涉及其他对象,DDL语句更多地由数据库管理员(DBA)使用<h3 id="创建数据库"><a href="#创建数据库" class="headerlink" title="创建数据库"></a>创建数据库</h3><code>create database test1;</code><h3 id="删除数据库"><a href="#删除数据库" class="headerlink" title="删除数据库"></a>删除数据库</h3><code>drop database test1;</code><br>drop操作语句的结果都显示’0 rows affected’<h3 id="创建表"><a href="#创建表" class="headerlink" title="创建表"></a>创建表</h3><code>create table emp(ename varchar(10),hiredate date,sal decimal(10,2),deptno int(2));</code><h4 id="查看表"><a href="#查看表" class="headerlink" title="查看表"></a>查看表</h4><code>desc emp;</code><h4 id="查看创建表的SQL语句"><a href="#查看创建表的SQL语句" class="headerlink" title="查看创建表的SQL语句"></a>查看创建表的SQL语句</h4><code>show create table emp \G;</code><p><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">MariaDB [test1]&gt; show create table emp \G;</div><div class="line">*************************** 1. row ***************************</div><div class="line">       Table: emp</div><div class="line">Create Table: CREATE TABLE `emp` (</div><div class="line">  `ename` varchar(20) DEFAULT NULL,</div><div class="line">  `hiredate` date DEFAULT NULL,</div><div class="line">  `sal` decimal(10,2) DEFAULT NULL,</div><div class="line">  `deptno` int(2) DEFAULT NULL</div><div class="line">) ENGINE=InnoDB DEFAULT CHARSET=latin1</div><div class="line">1 row in set (0.00 sec)</div><div class="line"></div><div class="line">ERROR: No query specified</div></pre></td></tr></table></figure>
</p>
</li>
</ul>
<hr>
<p>除了可以看到表定义以外,还可以看到表的engine(存储引擎)和charset(字符集)等信息.”\G”选项的含义是使得记录能够按照字段竖向排列,以便更好地显示内容较长的记录.</p>
<h3 id="删除表"><a href="#删除表" class="headerlink" title="删除表"></a>删除表</h3><p><code>drop table emp;</code></p>
<h3 id="修改表"><a href="#修改表" class="headerlink" title="修改表"></a>修改表</h3><h4 id="1-修改表类型"><a href="#1-修改表类型" class="headerlink" title="1. 修改表类型:"></a>1. 修改表类型:</h4><p><code>ALTER TABLE tablename MODIFY [COLUMN] column_definition [FIRST|AFTER col_name]</code></p>
<p><code>alter table emp modify ename varchar(20);</code></p>
<h4 id="2-增加表字段"><a href="#2-增加表字段" class="headerlink" title="2. 增加表字段:"></a>2. 增加表字段:</h4><p><code>ALTER TABLE tablename ADD [COLUMN] column_definition [FIRST|AFTER col_name]</code></p>
<p><code>alter table emp add [column] age int(3);</code></p>
<h4 id="3-删除表字段"><a href="#3-删除表字段" class="headerlink" title="3. 删除表字段"></a>3. 删除表字段</h4><p><code>ALTER TABLE tablename DROP[COLUMN] column_name</code></p>
<p><code>alter table emp drop [column] age;</code></p>
<h4 id="4-字段改名"><a href="#4-字段改名" class="headerlink" title="4. 字段改名"></a>4. 字段改名</h4><p><code>ALTER TABLE tablename CHANGE [COLUMN] old_col_name column_definition  [FIRST|AFTER col_name]</code></p>
<p><code>alter table emp change age age1 int(4);</code></p>
<p>change和modify都可以修改表的定义,不同的是change后面需要两次列名,不方便,但是change的优点是可以修改列名称,modify则不能.</p>
<h4 id="5-修改字段排列顺序"><a href="#5-修改字段排列顺序" class="headerlink" title="5. 修改字段排列顺序"></a>5. 修改字段排列顺序</h4><p>  前面介绍的极端增加和修改语法(ADD/CHANGE/MODIFY)中,都有一个可选项first|after column name,这个选项可以用来修改字段在表中的位置,ADD增加的新字段默认是加载标的最后位置,而CHANGE/MODIFY 默认不会改变字段的位置.</p>
<p>将新增的字段birth date 加在ename之后:</p>
<p><code>alter table emp add birth date after ename;</code></p>
<p>修改字段age,将他放在最前面:</p>
<p><code>alter table emp modify age int(3) first;</code></p>
<h4 id="6-更改表名"><a href="#6-更改表名" class="headerlink" title="6. 更改表名"></a>6. 更改表名</h4><p><code>ALTER TABLE tablename RENAME [TO] new_tablename</code><br>将<br><code>alter table emp rename emp1</code></p>
<h2 id="DML"><a href="#DML" class="headerlink" title="DML"></a>DML</h2><p>DML操作是指对数据库中表记录的操作,主要包括表记录的插入(insert),更新(update),删除(delete)和查询(select),是开发人员日常使用最为频繁的操作.</p>
<h3 id="插入记录"><a href="#插入记录" class="headerlink" title="插入记录"></a>插入记录</h3><p><code>INSERT INTO tablename(field1,field2,...fieldn) VALUES(value1,value2,...,valuen);</code></p>
<p><code>insert into emp(ename,hiredate,sal,deptno) values(&#39;zzzx1&#39;,&#39;2000-01-01&#39;,&#39;2000&#39;,1);</code></p>
<p><code>insert into dept values(5,&#39;dept5&#39;),(6,&#39;dept6&#39;);</code></p>
<h3 id="更新记录"><a href="#更新记录" class="headerlink" title="更新记录"></a>更新记录</h3><p><code>UPDATE tablename SET field1=value1,field2=value2,...,fieldn=valuen [WHERE CONDITION]</code></p>
<p>将wmp表中ename 为’lisa’的薪水(sal)从3000改为4000:</p>
<p><code>update emp set sal=4000 where ename=&#39;lisa&#39;;</code></p>
<h3 id="删除记录"><a href="#删除记录" class="headerlink" title="删除记录"></a>删除记录</h3><p><code>DELETE FROM tablename [WHERE CONDITION]</code></p>
<p>在MySQL中可以一次删除多个表的数据,</p>
<p><code>DELETE t1,t2,...,tn FROM t1,t2,...tn [WHERE CONDITION]</code></p>
<p>同时删除表emp和dept中deptno为3的记录:</p>
<p><code>delete a,b from emp a,dept b where a.deptno=b.deptno and a.deptno=3;</code></p>
<h3 id="查询记录"><a href="#查询记录" class="headerlink" title="查询记录"></a>查询记录</h3><p><code>SELECT * FROM tablename [WHERE CONDITION]</code></p>
<h4 id="1-查询不重复的记录"><a href="#1-查询不重复的记录" class="headerlink" title="1 查询不重复的记录"></a>1 查询不重复的记录</h4><p><code>select distinct deptno from emp;</code></p>
<h4 id="2-条件查询"><a href="#2-条件查询" class="headerlink" title="2 条件查询"></a>2 条件查询</h4><p><code>select * from emp where deptno=1;</code></p>
<p>上面例子中,where后面的条件是一个字段的=比较,除了=之外,还可以使用&lt;,&gt;,&gt;=,&lt;=,!=等比较运算符;多个条件之间还可以使用<em>or,and</em>等逻辑运算符进行多条件联合查询.</p>
<pre><code>MariaDB [test1]&gt; select * from emp where deptno=1 and sal&lt;3000;
+-------+------------+---------+--------+
| ename | hiredate   | sal     | deptno |
+-------+------------+---------+--------+
| zzx   | 2000-01-01 | 2000.00 |      1 |
+-------+------------+---------+--------+
1 row in set (0.00 sec)


</code></pre><h4 id="排序和限制"><a href="#排序和限制" class="headerlink" title="排序和限制"></a>排序和限制</h4><p>取出按照某个字段进行排序后的记录结果集,用到了数据库的排序操作,用关键字<em>ORDER BY</em>来实现</p>
<p><code>SELECT * FROM tablename [WHERE CONDITION] [ORDER BY field1 [DESC|ASC],field2 [DESC|ASC],...,fieldn[DESC|ASC]]</code></p>
<p>其中DESC和ASC是排序顺序关键字,DESV表示按照字段进行降序排列,ASC则表示升序序列，如果不写此关键字默认是升序排列．ORDER BY 后面可以跟多个不同的排序字段，并且每个排序字段可以有不同的排序顺序．</p>
<p>例如，把emp表中的记录按照工资高低进行显示:</p>
<pre><code>MariaDB [test1]&gt; select * from emp order by sal;
+--------+------------+---------+--------+
| ename  | hiredate   | sal     | deptno |
+--------+------------+---------+--------+
| zzx    | 2000-01-01 | 2000.00 |      1 |
| dony   | 2005-02-05 | 2000.00 |      4 |
| lisa   | 2003-02-01 | 4000.00 |      2 |
| bjguan | 2004-04-02 | 5000.00 |      1 |
+--------+------------+---------+--------+

</code></pre><p>如果排序字段的值一样,则值相同的字段按照第二个排序字段进行排序,依次类推.如果只有一个排序字段,则这些字段相同的记录将会无序排列</p>
<pre><code>MariaDB [test1]&gt; select * from emp order by deptno;
+--------+------------+---------+--------+
| ename  | hiredate   | sal     | deptno |
+--------+------------+---------+--------+
| zzx    | 2000-01-01 | 2000.00 |      1 |
| bjguan | 2004-04-02 | 5000.00 |      1 |
| lisa   | 2003-02-01 | 4000.00 |      2 |
| dony   | 2005-02-05 | 2000.00 |      4 |
+--------+------------+---------+--------+

</code></pre><p>对于deptno相同的前两条记录,如果要按照工资由高到低排序,可以使用以下命令:</p>
<pre><code>MariaDB [test1]&gt; select * from emp order by deptno,sal desc;
+--------+------------+---------+--------+
| ename  | hiredate   | sal     | deptno |
+--------+------------+---------+--------+
| bjguan | 2004-04-02 | 5000.00 |      1 |
| zzx    | 2000-01-01 | 2000.00 |      1 |
| lisa   | 2003-02-01 | 4000.00 |      2 |
| dony   | 2005-02-05 | 2000.00 |      4 |
+--------+------------+---------+--------+

</code></pre><p>对于排序后的记录,如果希望只显示一部分,而不是全部,这时,就可以使用LIMIT关键字来实现</p>
<p><code>SELECT ...[LIMIT offset_start,roe_count]</code></p>
<p>其中<em>offset_start</em>表示记录的起始偏移量,<em>row_count</em>表示显示的行数.在默认情况下,起始偏移量为0,只需要写记录行数就可以,这时实际显示的是前n条记录.例如显示emp表中按照sal排序后的前3条记录:</p>
<pre><code>MariaDB [test1]&gt; select * from emp order by sal limit 3;
+-------+------------+---------+--------+
| ename | hiredate   | sal     | deptno |
+-------+------------+---------+--------+
| zzx   | 2000-01-01 | 2000.00 |      1 |
| dony  | 2005-02-05 | 2000.00 |      4 |
| lisa  | 2003-02-01 | 4000.00 |      2 |
+-------+------------+---------+--------+

</code></pre><p>如果要显示emp表中按照sal排序后从第二条记录开始的3条记录</p>
<pre><code>MariaDB [test1]&gt; select * from emp order by sal limit 1,3;
+--------+------------+---------+--------+
| ename  | hiredate   | sal     | deptno |
+--------+------------+---------+--------+
| dony   | 2005-02-05 | 2000.00 |      4 |
| lisa   | 2003-02-01 | 4000.00 |      2 |
| bjguan | 2004-04-02 | 5000.00 |      1 |
+--------+------------+---------+--------+

</code></pre><h4 id="聚合"><a href="#聚合" class="headerlink" title="聚合"></a>聚合</h4><p><code>SELECT [field1,field2,...,fieldn] fun_name FROM tablename [WHERE where_contition] [GROUP BY field1,field2,...,fieldn [WITH ROLLUP]][HAVING where_contition]</code></p>
<ul>
<li>fun_name 表示要做的聚合操作,也就是聚合函数,常用的有sum(求和),count(*)(记录数),max(最大值),min(最小值).</li>
<li>GROUP BY 关键字表示要进行分类聚合的字段,比如要按照部门分类统计员工数量,部门就应该写在group by后面.</li>
<li>WITH ROLLUP [可选语法] 表明是否分类聚合后的结果进行在汇总.</li>
<li>HAVING 关键字表示对分类后的结果再进行条件的过滤.</li>
</ul>
<p>having 和 where 的区别在于,having是对聚合后的结果进行条件的过滤,而where实在聚合前就对记录进行过滤,如果逻辑允许,我们尽可能用where先过滤记录,这样因为结果集减小,将对聚合的效率大大提高,最后在根据逻辑看是否用having进行再过滤.</p>
<p>在emp表中统计公司总人数:</p>
<pre><code>MariaDB [test1]&gt; select count(1) from emp;
+----------+
| count(1) |
+----------+
|        4 |
+----------+

</code></pre><p>在此基础上要统计各个部门人数:</p>
<pre><code>MariaDB [test1]&gt; select deptno,count(1) from emp group by deptno;
+--------+----------+
| deptno | count(1) |
+--------+----------+
|      1 |        2 |
|      2 |        1 |
|      4 |        1 |
+--------+----------+

</code></pre><p>既要统计各部门人数,又要统计总人数:</p>
<pre><code>MariaDB [test1]&gt; select deptno,count(1) from emp group by deptno with rollup;
+--------+----------+
| deptno | count(1) |
+--------+----------+
|      1 |        2 |
|      2 |        1 |
|      4 |        1 |
|   NULL |        4 |
+--------+----------+


</code></pre><p>统计人数大于1的部门:</p>
<pre><code>MariaDB [test1]&gt; select deptno,count(1) from  emp group by deptno having count(1)&gt;1;
+--------+----------+
| deptno | count(1) |
+--------+----------+
|      1 |        2 |
+--------+----------+

</code></pre><p>统计公司所有员工的薪水总额,最高和最低薪水:</p>
<pre><code>MariaDB [test1]&gt; select sum(sal),max(sal),min(sal) from emp;
+----------+----------+----------+
| sum(sal) | max(sal) | min(sal) |
+----------+----------+----------+
| 13000.00 |  5000.00 |  2000.00 |
+----------+----------+----------+

</code></pre><h3 id="表连接"><a href="#表连接" class="headerlink" title="表连接"></a>表连接</h3><p>当需要同时显示多个表中的字段时,就可以用表连接来实现这样的功能.从大类上分,表连接分为内连接和外连接,他们之间的最主要区别是,内连接仅选出两张表中互相匹配的记录,而外连接胡选出其他不匹配的记录.我们最常用的是内连接.</p>
<p>查询出所有雇员的名字和所在部门名称,因为雇员名称和部门分别存放在表emp和dept中,因此,需要使用表连接来查询:</p>
<pre><code>MariaDB [test1]&gt; select ename,deptname from emp,dept where emp.deptno=dept.deptno;
+--------+----------+
| ename  | deptname |
+--------+----------+
| zzx    | tech     |
| lisa   | sale     |
| bjguan | tech     |
| dony   | hr       |
+--------+----------+


</code></pre><p>外连接又分为<strong>左连接</strong>和<strong>右连接</strong></p>
<ul>
<li>左连接:包含所有的左边表中的记录甚至是右边表中没有和它匹配的记录;</li>
<li><p>右连接:包含所有的右边表中的记录甚至是左边表中没有和它匹配的记录;<br>查询emp中所有用户名和所在部门名称:</p>
<pre><code>MariaDB [test1]&gt; select ename,deptname from emp left join dept on emp.deptno=dept.deptno;
+--------+----------+
| ename  | deptname |
+--------+----------+
| zzx    | tech     |
| bjguan | tech     |
| lisa   | sale     |
| dzshen | hr       |
| dony   | NULL     |
+--------+----------+

MariaDB [test1]&gt; select ename,deptname from dept right join emp on dept.deptno=emp.deptno;
+--------+----------+
| ename  | deptname |
+--------+----------+
| zzx    | tech     |
| bjguan | tech     |
| lisa   | sale     |
| dzshen | hr       |
| dony   | NULL     |
+--------+----------+

</code></pre><p>比较这个查询和上例中的查询,都是查询用户名和部门名,两者的区别在于本例中列出来所有的用户名,即使有的用户名(dony)并不存在合法的部门名称;而上例中仅仅列出了存在合法部门的用户名和部门名称.</p>
<h4 id="子查询"><a href="#子查询" class="headerlink" title="子查询"></a>子查询</h4><p>某些情况下,当进行查询的时候,需要的条件是另外一个select语句的结果,这个时候就要用到子查询.用于子查询的关键字主要包括in,not in,=,!=,exitst,not exists等.</p>
<pre><code>MariaDB [test1]&gt; select * from emp where deptno in(select deptno from dept);
+--------+------------+---------+--------+
| ename  | hiredate   | sal     | deptno |
+--------+------------+---------+--------+
| zzx    | 2000-01-01 | 2000.00 |      1 |
| lisa   | 2003-02-01 | 4000.00 |      2 |
| bjguan | 2004-04-02 | 5000.00 |      1 |
| dzshen | 2005-04-01 | 4000.00 |      3 |
+--------+------------+---------+--------+

</code></pre><p>如果子查询记录数唯一,还可以用=代替in:</p>
<pre><code>MariaDB [test1]&gt; select * from emp where deptno = (select deptno from dept limit 1);
+--------+------------+---------+--------+
| ename  | hiredate   | sal     | deptno |
+--------+------------+---------+--------+
| zzx    | 2000-01-01 | 2000.00 |      1 |
| bjguan | 2004-04-02 | 5000.00 |      1 |
+--------+------------+---------+--------+

</code></pre><p>某些情况下子查询可以转化(等效)为表连接,</p>
<pre><code>MariaDB [test1]&gt; select * from emp where deptno in(select deptno from dept);
+--------+------------+---------+--------+
| ename  | hiredate   | sal     | deptno |
+--------+------------+---------+--------+
| zzx    | 2000-01-01 | 2000.00 |      1 |
| lisa   | 2003-02-01 | 4000.00 |      2 |
| bjguan | 2004-04-02 | 5000.00 |      1 |
| dzshen | 2005-04-01 | 4000.00 |      3 |
+--------+------------+---------+--------+

</code></pre><p>转化为表连接后:</p>
<pre><code>MariaDB [test1]&gt; select emp.* from emp,dept where emp.deptno=dept.deptno;
+--------+------------+---------+--------+
| ename  | hiredate   | sal     | deptno |
+--------+------------+---------+--------+
| zzx    | 2000-01-01 | 2000.00 |      1 |
| lisa   | 2003-02-01 | 4000.00 |      2 |
| bjguan | 2004-04-02 | 5000.00 |      1 |
| dzshen | 2005-04-01 | 4000.00 |      3 |
+--------+------------+---------+--------+

</code></pre><p>子查询和表连接之间的转换主要应用在两个方面</p>
</li>
<li>MySQL 4.1 以前的版本不支持子查询,需要用表连接来实现子查询的功能</li>
<li>表连接在很多情况下用于优化子查询</li>
</ul>
<h4 id="记录联合"><a href="#记录联合" class="headerlink" title="记录联合"></a>记录联合</h4><p>将两个表的数据按照一定的查询条件查询出来后,将结果合并到一起显示出来,这个时候就需要union和union all 关键字来实现这样的功能</p>
<p><code>SELECT * FROM t1  UNION|UNION ALL  SELECT * FROM t2 ...UNION|UNION ALL SELECT * FROM tn;</code></p>
<pre><code>MariaDB [test1]&gt; select deptno from emp
    -&gt; union all
    -&gt; select deptno from dept;
+--------+
| deptno |
+--------+
|      1 |
|      2 |
|      1 |
|      4 |
|      3 |
|      1 |
|      2 |
|      3 |
|      5 |
+--------+

</code></pre><p>将结果去掉重复记录后显示如下:</p>
<pre><code>MariaDB [test1]&gt; select deptno from emp union  select deptno from dept;
+--------+
| deptno |
+--------+
|      1 |
|      2 |
|      4 |
|      3 |
|      5 |
+--------+


</code></pre><h2 id="DCL"><a href="#DCL" class="headerlink" title="DCL"></a>DCL</h2><p>DCL主要是DBA用来管理系统中的对象权限时使用</p>
<p>创建一个数据库用户z1,具有对sakila数据库中所有表的SELECT/INSERT权限:</p>
<pre><code>MariaDB [test1]&gt; grant select,insert on sakila.* to &apos;z1&apos;@&apos;localhost&apos; identified by &apos;123&apos;;
Query OK, 0 rows affected (0.00 sec)

MariaDB [test1]&gt; exit
Bye
[root@CentOS3 bin]# mysql -uz1 -p


</code></pre><p>收回z1,INSERT,只能对数据进行SELECT操作:</p>
<pre><code>MariaDB [(none)]&gt; revoke insert on sakila.* from &apos;z1&apos;@&apos;localhost&apos;;
Query OK, 0 rows affected (0.00 sec)



</code></pre>]]></content>
      
        <categories>
            
            <category> MySQL </category>
            
        </categories>
        
        
    </entry>
    
    <entry>
      <title><![CDATA[选择和配置数据压缩]]></title>
      <url>/CDH/%E9%80%89%E6%8B%A9%E5%92%8C%E9%85%8D%E7%BD%AE%E6%95%B0%E6%8D%AE%E5%8E%8B%E7%BC%A9.html</url>
      <content type="html"><![CDATA[<h1 id="选择和配置数据压缩"><a href="#选择和配置数据压缩" class="headerlink" title="选择和配置数据压缩"></a>选择和配置数据压缩</h1><h2 id="选择压缩类型的指导"><a href="#选择压缩类型的指导" class="headerlink" title="选择压缩类型的指导"></a>选择压缩类型的指导</h2><ul>
<li>GZIP压缩比Snappy或LZO使用更多的CPU资源，但提供了更高的压缩比。GZip通常是冷数据的不错选择，不经常访问。 Snappy或LZO是热门数据的更好选择，频繁访问。</li>
<li>对于某些类型的文件，BZip2还可以产生比GZip更多的压缩，压缩和解压缩时需要一定的速度。 HBase不支持BZip2压缩。</li>
<li>Snappy经常表现比LZO好。</li>
<li>对于MapReduce，如果您需要压缩数据是可拆分的，则可以拆分BZip2和LZO格式。 Snappy和GZip块不是可拆分的，但是可以拆分诸如SequenceFile或Avro之类的容器文件格式的Snappy块的文件</li>
<li>Snappy旨在与容器格式（如SequenceFiles或Avro数据文件）一起使用，而不是直接用于纯文本，例如，由于后者不可分割，并且不能使用MapReduce并行处理。可分离性与HBase数据无关。</li>
<li></li>
</ul>
<h2 id="配置数据压缩"><a href="#配置数据压缩" class="headerlink" title="配置数据压缩"></a>配置数据压缩</h2><h3 id="使用Cloudera-Manager配置数据压缩"><a href="#使用Cloudera-Manager配置数据压缩" class="headerlink" title="使用Cloudera Manager配置数据压缩"></a>使用Cloudera Manager配置数据压缩</h3><p>要使用Cloudera Manager配置对LZO的支持，您必须安装GPL Extras宗地，然后配置服务才能使用。请参阅<br><a href="https://www.cloudera.com/documentation/enterprise/latest/topics/cm_ig_install_gpl_extras.html#xd_583c10bfdbd326ba-3ca24a24-13d80143249--7ec6" target="_blank" rel="external">安装GPL附加包</a></p>
<p><a href="https://www.cloudera.com/documentation/enterprise/latest/topics/cm_mc_gpl_extras.html#xd_583c10bfdbd326ba--6eed2fb8-14349d04bee--7c3e" target="_blank" rel="external">配置服务以使用GPL附加包裹</a></p>
<h3 id="Configuring-Data-Compression-Using-the-Command-Line"><a href="#Configuring-Data-Compression-Using-the-Command-Line" class="headerlink" title="Configuring Data Compression Using the Command Line"></a>Configuring Data Compression Using the Command Line</h3>]]></content>
      
        <categories>
            
            <category> CDH </category>
            
        </categories>
        
        
    </entry>
    
    <entry>
      <title><![CDATA[手动安装zypper源和rpm包]]></title>
      <url>/CDH/Upgrade/%E6%89%8B%E5%8A%A8%E5%AE%89%E8%A3%85zypper%E6%BA%90%E5%92%8Crpm%E5%8C%85.html</url>
      <content type="html"><![CDATA[<p>因大数据cloudera manager 平台升级,请将所有大数据服务器安装python-psycopg2-2.6.2-3.3.x86_64.rpm这个包,大数据服务器列表见附件</p>
<p>CM前端管理方式安装CDH失败:</p>
<ol>
<li>主机密码不对</li>
<li>先手工卸载旧rpm包,手工配zypper源安装</li>
<li>zypper源指向不对,删除无效的zypper源</li>
<li>用户目录下的host影响,或者/etc/hosts和CM节点不一致</li>
</ol>
<p>Usage: 本地Zypper源使用 [适用SUSE]</p>
<ol>
<li>Backup repository.<br>find /etc/zypp/repos.d -name “*.repo” -exec mv {} {}.bak \;</li>
<li>Install<br>zypper ar <a href="http://10.200.58.43/" target="_blank" rel="external">http://10.200.58.43/</a> local.main</li>
<li>Update cache<br>zypper ref</li>
</ol>
<p>zyppzy</p>
<p>ps aux|grep  supervisord |grep -v grep | awk ‘{print “kill -9 “$2}’|sh</p>
<p>zypper in cloudera-manager-daemons-5.11.1-1.cm5111.p0.9.sles11<br>zypper in cloudera-manager-agent-5.11.1-1.cm5111.p0.9.sles11<br>下载地址:<a href="https://software.opensuse.org/download.html?project=server%3Adatabase%3Apostgresql&amp;package=python-psycopg2" target="_blank" rel="external">https://software.opensuse.org/download.html?project=server%3Adatabase%3Apostgresql&amp;package=python-psycopg2</a></p>
<p>如果安装时报出缺少   libpq.so.5()(64bit) 依赖,需要先安装系统ISO镜像自带的postgresql91-9.1.9-0.3.1.x86_64.rpm（需要用到此包中的libpq库）,之后再安装python-psycopg2-2.6.2-3.3.x86_64.rpm</p>
<p><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">zypper in postgresql91</div><div class="line">rpm -ivh python-psycopg2-2.6.2-3.2.x86_64.rpm</div></pre></td></tr></table></figure>
</p>
<hr>
<p><a href="http://stackoverflow.com/questions/35544155/how-to-access-a-zookeeper-ensemble-as-a-super-user-via-zookeeper-shell/35654757#35654757" target="_blank" rel="external">Zookeeper 绕过密码注册超级管理员</a></p>
<p>SERVER_JVMFLAGS=-Dzookeeper.DigestAuthenticationProvider.superDigest=super:DyNYQEQvajljsxlhf5uS4PJ9R28=</p>
<p><a href="http://www.tuicool.com/articles/neY7Vrq" target="_blank" rel="external">http://www.tuicool.com/articles/neY7Vrq</a></p>
<p>suse http zypper源 http<br>apache2<br>分发服务器<br>/srv/www/<br>HA namenode snamenode<br>zypper 源配置 添加http源 ar 指定源名字</p>
<p>/mnt/suse/x86_64/postgresql91-9.1.24-1.1.x86_64.rpm</p>
<p><a href="http://ftp.rrzn.uni-hannover.de/opensuse/repositories/server:/database:/postgresql/SLE_11_SP3/x86_64/?winzoom=1" target="_blank" rel="external">http://ftp.rrzn.uni-hannover.de/opensuse/repositories/server:/database:/postgresql/SLE_11_SP3/x86_64/?winzoom=1</a></p>
<p>sudo /usr/sbin/groupadd manage</p>
<p>manage@ddp-dn-101:/home&gt; sudo chown manage:manage manage/</p>
<p><code>zypper ar http://10.200.58.43/ local.main</code></p>
<p><code>sudo zypper mr -d &quot;SUSE-Linux-Enterprise-Server-11-SP3 11.3.3-1.138&quot;</code><br>Repository ‘SUSE-Linux-Enterprise-Server-11-SP3 11.3.3-1.138’ has been successfully disabled.<br><code>sudo zypper ref</code></p>
<p>sudo zypper in postgresql91<br>sudo rpm -ivh python-psycopg2-2.6.2-3.2.x86_64.rpm</p>
<p>密码不对</p>
<p>先手工卸载旧rpm包</p>
<p>host mv usr</p>
<p>/etc/hosts 没配</p>
]]></content>
      
        <categories>
            
            <category> CDH </category>
            
            <category> Upgrade </category>
            
        </categories>
        
        
    </entry>
    
    <entry>
      <title><![CDATA[Ruby入门]]></title>
      <url>/Ruby/Ruby%E5%85%A5%E9%97%A8.html</url>
      <content type="html"><![CDATA[<ol>
<li>方法调用的最外层括号可以省略</li>
<li>函数最后一行默认有return</li>
<li>hash处于一个函数最后一个参数的时候大括号可以省略<br>Apple.create :name =&gt; ‘apple’,:color =&gt;’red’<br>Apple.create({:name =&gt; ‘aplle’,:color =&gt;’red’})<br>Apple.create name:’apple’,color:’red’</li>
<li><p>调用block<br>Apple.all.map { |apple| apple.name }<br>Apple.all.map (&amp;:name)</p>
</li>
<li><p>Module </p>
<ul>
<li>不能别new</li>
<li>不能被include</li>
<li>module 中的 self.xx 方法可以被直接调用(不建议)</li>
<li>module 中的普通方法,需要被某个class include 之后,由对应的class调用</li>
</ul>
<p><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">[&apos;jim&apos;,&apos;li_lei&apos;,&apos;han_mei_mei&apos;].each do |name|</div><div class="line">    define_method &quot;say_hi_to_#&#123;name&#125;&quot; do</div><div class="line">        puts &quot;hi,#&#123;name&#125;&quot;</div><div class="line">    end</div><div class="line">end</div></pre></td></tr></table></figure>
</p>
</li>
</ol>
<p>想知道 第一行中 |name| 这种写法是定义遍历前面数组的参数的意思么?我猜的</p>
<p><code>:: 开头的是类方法 可以直接调用</code><br><code># 开头的是实例方法 需要new</code></p>
]]></content>
      
        <categories>
            
            <category> Ruby </category>
            
        </categories>
        
        
    </entry>
    
    <entry>
      <title><![CDATA[Installation]]></title>
      <url>/CDH/Kerberos%20MIT/Installation.html</url>
      <content type="html"><![CDATA[<h2 id="初始化"><a href="#初始化" class="headerlink" title="初始化"></a>初始化</h2><p><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">➜  ~ kdb5_util create -s</div><div class="line">Loading random data</div><div class="line">Initializing database &apos;/var/kerberos/krb5kdc/principal&apos; for realm &apos;EXAMPLE.COM&apos;,</div><div class="line">master key name &apos;K/M@EXAMPLE.COM&apos;</div><div class="line">You will be prompted for the database Master Password.</div><div class="line">It is important that you NOT FORGET this password.</div><div class="line">Enter KDC database master key: </div><div class="line">Re-enter KDC database master key to verify:</div></pre></td></tr></table></figure>
</p>
<p>This will create five files in LOCALSTATEDIR/krb5kdc (or at the locations specified in kdc.conf):</p>
<ul>
<li>two Kerberos database files, principal, and principal.ok</li>
<li>the Kerberos administrative database file, principal.kadm5</li>
<li>the administrative database lock file, principal.kadm5.lock</li>
<li>the stash file, in this example .k5.ATHENA.MIT.EDU. If you do not want a stash file, run the above command without the -s option.</li>
</ul>
<h2 id="krb5-conf"><a href="#krb5-conf" class="headerlink" title="krb5.conf"></a>krb5.conf</h2><p>cat /etc/krb5.conf<br><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">[libdefaults]</div><div class="line">#       default_realm = EXAMPLE.COM </div><div class="line">default_realm = CMDMP.COM</div><div class="line">dns_lookup_realm = false</div><div class="line">dns_lookup_kdc = false</div><div class="line">ticket_lifetime = 24h</div><div class="line">renew_lifetime = 7d</div><div class="line">forwardable = true</div><div class="line">default_tkt_enctypes = arcfour-hmac-md5</div><div class="line">default_tgs_enctypes = arcfour-hmac-md5</div><div class="line"></div><div class="line">[realms]</div><div class="line">#       EXAMPLE.COM = &#123;</div><div class="line">#                kdc = kerberos.example.com</div><div class="line">#               admin_server = kerberos.example.com</div><div class="line">#       &#125;</div><div class="line">CMDMP.COM = &#123;</div><div class="line">  kdc = ddp-nn-02.cmdmp.com</div><div class="line">  admin_server = ddp-nn-02.cmdmp.com</div><div class="line">&#125;</div><div class="line"></div><div class="line">[domain_realm]</div><div class="line">.cmdmp.com = CMDMP.COM</div><div class="line">cmdmp.com = CMDMP.COM</div><div class="line"></div><div class="line">[logging]</div><div class="line">    kdc = FILE:/var/log/krb5/krb5kdc.log</div><div class="line">    admin_server = FILE:/var/log/krb5/kadmind.log</div><div class="line">    default = SYSLOG:NOTICE:DAEMON</div></pre></td></tr></table></figure></p>
<h2 id="kdc-conf"><a href="#kdc-conf" class="headerlink" title="kdc.conf"></a>kdc.conf</h2><p>cat /var/lib/kerberos/krb5kdc/kdc.conf<br><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">[kdcdefaults]</div><div class="line">        kdc_ports = 750,88</div><div class="line">        kdc_tcp_ports = 750,88</div><div class="line"></div><div class="line">[realms]</div><div class="line">#       EXAMPLE.COM = &#123;</div><div class="line">#               database_name = /var/lib/kerberos/krb5kdc/principal</div><div class="line">#               admin_keytab = FILE:/var/lib/kerberos/krb5kdc/kadm5.keytab</div><div class="line">#               acl_file = /var/lib/kerberos/krb5kdc/kadm5.acl</div><div class="line">#               dict_file = /var/lib/kerberos/krb5kdc/kadm5.dict</div><div class="line">#               key_stash_file = /var/lib/kerberos/krb5kdc/.k5.EXAMPLE.COM</div><div class="line">#               kdc_ports = 750,88</div><div class="line">#               max_life = 10h 0m 0s</div><div class="line">#               max_renewable_life = 7d 0h 0m 0s</div><div class="line">#       &#125;</div><div class="line"> CMDMP.COM = &#123;</div><div class="line">  max_renewable_life = 7d 0h 0m 0s</div><div class="line">  database_name = /var/lib/kerberos/krb5kdc/principal</div><div class="line">  acl_file = /var/lib/kerberos/krb5kdc/kadm5.acl</div><div class="line">  dict_file = /var/lib/kerberos/krb5kdc/kadm5.dict</div><div class="line">  admin_keytab = /var/lib/kerberos/krb5kdc/kadm5.keytab</div><div class="line">  supported_enctypes = aes256-cts:normal aes128-cts:normal des3-hmac-sha1:normal arcfour-hmac:normal des-hmac-sha1:normal dex-cbc-md5:normal des-cbc-crc:normal</div><div class="line">&#125;</div><div class="line"></div><div class="line">[logging]</div><div class="line">    kdc = FILE:/var/log/krb5/krb5kdc.log</div><div class="line">    admin_server = FILE:/var/log/krb5/kadmind.log</div></pre></td></tr></table></figure></p>
]]></content>
      
        <categories>
            
            <category> CDH </category>
            
            <category> Kerberos MIT </category>
            
        </categories>
        
        
    </entry>
    
    <entry>
      <title><![CDATA[MIT Kerberos Documentation]]></title>
      <url>/CDH/Kerberos%20MIT/MIT%20Kerberos%20Documentation.html</url>
      <content type="html"><![CDATA[<h1 id="MIT-Kerberos-Documentation"><a href="#MIT-Kerberos-Documentation" class="headerlink" title="MIT Kerberos Documentation"></a>MIT Kerberos Documentation</h1><h2 id="For-user"><a href="#For-user" class="headerlink" title="For user"></a>For user</h2><h3 id="密码管理"><a href="#密码管理" class="headerlink" title="密码管理"></a>密码管理</h3><p>kpasswd</p>
<p>### </p>
]]></content>
      
        <categories>
            
            <category> CDH </category>
            
            <category> Kerberos MIT </category>
            
        </categories>
        
        
    </entry>
    
    <entry>
      <title><![CDATA[kerberos配置]]></title>
      <url>/CDH/Kerberos%20MIT/kerberos%E9%85%8D%E7%BD%AE.html</url>
      <content type="html"><![CDATA[<h2 id="虚拟机上配置"><a href="#虚拟机上配置" class="headerlink" title="虚拟机上配置"></a>虚拟机上配置</h2><p><figure class="highlight bash"><table><tr><td class="code"><pre><div class="line"><span class="built_in">cd</span> /etc/sysconfig/network-scripts </div><div class="line">cp ifcfg-eth0 ifcfg-eth0.bak </div><div class="line">vi ifcfg-eth0 </div><div class="line">service network restart</div><div class="line">vi /etc/hosts </div><div class="line">vi /etc/sysconfig/network</div><div class="line"></div><div class="line">NETWORKING=yes</div><div class="line">HOSTNAME=elephant.migu.cn</div><div class="line">NETWORKING_IPV6=no</div><div class="line"></div><div class="line">hostname elephant.migu.cn</div><div class="line">hostname -s elephant</div></pre></td></tr></table></figure>
</p>
<hr>
<h2 id="测试环境上kerberos"><a href="#测试环境上kerberos" class="headerlink" title="测试环境上kerberos"></a>测试环境上kerberos</h2><p>改了默认位置<br>vi /var/lib/kerberos/krb5kdc/kdc.conf </p>
<p><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">1  2017-04-26 15:12:47 vi /var/lib/kerberos/krb5kdc/kdc.conf </div><div class="line">    2  2016-12-26 18:53:59 kadmin.local</div><div class="line">    3  2016-12-26 18:54:28 chkconfig krb5kdc on</div><div class="line">    4  2016-12-26 18:54:39 chkconfig kadmin on</div><div class="line">    5  2016-12-26 18:54:44 service krb5kdc status</div><div class="line">    6  2016-12-26 18:54:47 service krb5kdc start</div><div class="line">    7  2016-12-26 18:54:54 kadmin</div><div class="line">    8  2016-12-26 18:55:16 scp /etc/krb5.conf root@10.200.65.50:/etc/</div><div class="line">    9  2016-12-26 18:55:21 scp /etc/krb5.conf root@10.200.65.51:/etc/</div><div class="line">   10  2016-12-26 18:55:31 scp /etc/krb5.conf root@10.200.65.53:/etc/</div><div class="line">   11  2016-12-26 18:55:39 scp /etc/krb5.conf root@10.200.65.54:/etc/</div><div class="line">   12  2016-12-26 18:55:45 scp /etc/krb5.conf root@10.200.65.55:/etc/</div><div class="line">   13  2016-12-26 18:55:51 scp /etc/krb5.conf root@10.200.65.56:/etc/</div><div class="line">   14  2016-12-26 18:55:56 scp /etc/krb5.conf root@10.200.65.57:/etc/</div><div class="line">   15  2016-12-26 18:56:01 scp /etc/krb5.conf root@10.200.65.58:/etc/</div><div class="line">   16  2016-12-26 18:56:06 scp /etc/krb5.conf root@10.200.65.59:/etc/</div><div class="line">   17  2016-12-26 18:56:12 scp /etc/krb5.conf root@10.200.65.60:/etc/</div><div class="line">   18  2016-12-26 18:56:18 scp /etc/krb5.conf root@10.200.65.61:/etc/</div><div class="line">   19  2016-12-26 18:56:24 scp /etc/krb5.conf root@10.200.65.62:/etc/</div><div class="line">   20  2016-12-26 18:56:29 scp /etc/krb5.conf root@10.200.65.63:/etc/</div><div class="line">   21  2016-12-26 18:56:38 scp /etc/krb5.conf root@10.200.65.64:/etc/</div><div class="line">   22  2016-12-26 18:56:45 scp /etc/krb5.conf root@10.200.65.65:/etc/</div><div class="line">   23  2016-12-26 18:56:58 scp /etc/krb5.conf root@10.200.65.71:/etc/</div></pre></td></tr></table></figure>
</p>
<hr>
<p>查看Linux某用户属于哪个组<br>id  user<br>groups user</p>
]]></content>
      
        <categories>
            
            <category> CDH </category>
            
            <category> Kerberos MIT </category>
            
        </categories>
        
        
    </entry>
    
    <entry>
      <title><![CDATA[kafka的相关概念]]></title>
      <url>/Kafka/kafka%E7%9A%84%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5.html</url>
      <content type="html"><![CDATA[<h2 id="kafka的相关概念"><a href="#kafka的相关概念" class="headerlink" title="kafka的相关概念"></a>kafka的相关概念</h2><h3 id="AMPQ协议"><a href="#AMPQ协议" class="headerlink" title="AMPQ协议"></a>AMPQ协议</h3><p>一些基本的概念:</p>
<ul>
<li>消费者(Consumer):从消息队列中请求消息的客户端应用程序</li>
<li>生产者(Producer):向broker发布消息的客户端应用程序</li>
<li>AMQP服务器端(broker):用来接收生产者发送的消息并将这些消息路由给服务器中的队列</li>
</ul>
<h3 id="kafka支持的客户端语言"><a href="#kafka支持的客户端语言" class="headerlink" title="kafka支持的客户端语言"></a>kafka支持的客户端语言</h3><p>支持当前大部分主流语言(变形自己的consumer和producer程序)</p>
<h3 id="kafka架构"><a href="#kafka架构" class="headerlink" title="kafka架构"></a>kafka架构</h3><p>一些基本的概念:</p>
<ul>
<li>主题(Topic):一个主题类似新闻中的体育,娱乐,教育等分类概念,在实际工程中通常一个业务一个主题</li>
<li>分区(Partition):一个topic钟的消息数据按照多个分区组织,分区是kafka消息队列组织的最小单位,一个分区可以看做是一个FIFO的队列</li>
</ul>
<hr>
<h2 id="Zookeeper集群搭建"><a href="#Zookeeper集群搭建" class="headerlink" title="Zookeeper集群搭建"></a>Zookeeper集群搭建</h2><ul>
<li>集群搭建</li>
<li>集群配置参数介绍<ul>
<li>myid文件 和server.myid</li>
<li>zoo.cfg文件</li>
<li>log4j.properties文件</li>
<li>zkEnv.sh和zkServer.sh</li>
</ul>
</li>
</ul>
]]></content>
      
        <categories>
            
            <category> Kafka </category>
            
        </categories>
        
        
    </entry>
    
    <entry>
      <title><![CDATA[java加密解密]]></title>
      <url>/java/java%E5%8A%A0%E5%AF%86%E8%A7%A3%E5%AF%86.html</url>
      <content type="html"><![CDATA[<p># </p>
<ul>
<li>安全和密码</li>
<li>常用安全体系介绍</li>
<li>密码分类及java的安全组成</li>
<li>JDK相关包及第三方扩展</li>
<li>Base64算法</li>
</ul>
<hr>
<h2 id="密码常用术语"><a href="#密码常用术语" class="headerlink" title="密码常用术语"></a>密码常用术语</h2><ul>
<li>明文:待加密信息</li>
<li>密文:经过加密后的明文</li>
<li>加密:明文转为密文的过程</li>
<li>加密算法:明文转为密文的转换算法</li>
<li>解密:将密文转为明文的过程</li>
<li>解密算法:密文转为明文的算法</li>
<li>解密密钥:通过解密算法进行解密操作用的密钥</li>
<li>密码分析:截获密文者试图通过分析截获的密文从而推断出原来的明文或者密钥的过程</li>
<li>主动攻击:攻击者非法入侵密码系统,采用伪造,修改,删除等手段向系统注入假消息进行欺骗(对密文具有破坏作用)</li>
<li>被动攻击:对一个保密系统采取截获密文并对其进行分析和攻击(对密文没有破坏作用)</li>
<li>密码体制:由明文空间,密文空间,密钥空间,加密算法和解密算法五部分构成</li>
<li>密码协议:也称为安全协议,指以密码学为基础的消息交换的通信协议,目的是在网络环境中提供安全的服务</li>
<li>密码系统:指用于加密,解密的系统</li>
<li>科克霍夫原则:数据的安全基于密钥而不是算法的保密,即系统的安全取决于密钥,对密钥保密,对算法公开–现代密码学设计的基本原则</li>
</ul>
<p>##加解密基础</p>
<ul>
<li><p>密码分类</p>
<ul>
<li>时间<ul>
<li>古典密码:以字符为基本加密单元</li>
<li>现代密码:以信息块为基本加密单元</li>
</ul>
</li>
<li><p>保密内容算法<br>|名称|详细说明|应用领域|类别|<br>|:–|:–|:–|:–|<br>|受限制算法|算法的保密性基于保持算法的秘密|军师领域|古典密码|<br>|基于密钥算法|算法的保密性基于对密钥的保密||现代密码|</p>
</li>
<li><p>密码体制<br>|名称|别名|详细说明|<br>|:–|:–|:–|<br>|对称密码|单钥秘密或私钥密码|指加密密钥与解密密钥相同|<br>|非对称密码|双钥密码或公钥密码|指加密密钥与解密密钥不同,密钥分公钥,私钥|<br>|对称密码算法|单钥密码算法或私钥密码算法|指应用于对称密码的加密,解密算法|<br>|非对称密码算法|双钥密码算法或公钥密码|只对应非对称密码的加密,解密算法|</p>
</li>
<li><p>明文处理方法</p>
<ul>
<li>分组密码:指加密时将名为分成固定长度的组,用同一密钥和算法对每一块加密,输出也是固定长度的密文,多用于网络加密.</li>
<li>流密码:也称为序列密码.指加密时每次加密一位或者一个字节明文</li>
</ul>
</li>
<li>散列函数<ul>
<li>用来验证数据的完整性</li>
<li>长度不受限制</li>
<li>哈希值容易计算</li>
<li>散列运算过程不可逆</li>
<li>算法:<ul>
<li>消息摘要算法MD5</li>
<li>SHA–安全散列算法</li>
<li>MAC–消息认证码算法(MAC–苹果操作系统)</li>
</ul>
</li>
</ul>
</li>
<li>数字签名<ul>
<li>主要是针对以数字的形式存储的消息进行的处理</li>
</ul>
</li>
</ul>
</li>
</ul>
<hr>
<h2 id="OSI安全体系"><a href="#OSI安全体系" class="headerlink" title="OSI安全体系"></a>OSI安全体系</h2><p>OSI(Open System Interconnection)</p>
<p>网络通信<br>    7.应用层<br>    6.表示层<br>    5.会话层<br>    4.传输层<br>    3.网络层<br>    2.数据链路层<br>    1.物理层<br>安全机制<br>    加密机制<br>    数字签名机制<br>    访问控制机制<br>    数据完整性机制<br>    认证机制<br>    业务流填充机制<br>    路由控制机制<br>    公证机制<br>安全服务<br>    认证(鉴别)<br>    访问控制服务<br>    数据保密性服务<br>    数据完整性服务<br>    抗否认性服务</p>
<p>TCP/IP</p>
<hr>
<h2 id="java安全组成"><a href="#java安全组成" class="headerlink" title="java安全组成"></a>java安全组成</h2><p>JCA（Java Cryptography Architectrue）JAVA加密体系。<br>JCE（Java Cryptography Extension）JAVA加密拓展：DES 、AES 、RSA算法通过JCE提供。<br>JSSE（Java Secure Socket Extesion）JAVA套接字安全拓展：提供基于SSL的加密功能，主要用于网络传输。<br>JAAS（Java Authentication and Authentication Service）JAVA鉴别与安全服务。</p>
<hr>
<ul>
<li>相关java包,类<br>java.security<ul>
<li>消息摘要<br>javax.crypto</li>
</ul>
</li>
<li>安全消息摘要,消息认证(鉴别)码<br>java.net.ssl</li>
<li>安全套接字</li>
</ul>
<hr>
<ul>
<li>第三方java扩展<br>Bouncy Castle</li>
<li>两种支持方案:1)配置;2)调用</li>
</ul>
<p>Commons Codec</p>
<ul>
<li>Apache</li>
<li>Base64,二进制,十六进制,字符集编码</li>
<li>Url编码/解码 </li>
</ul>
]]></content>
      
        <categories>
            
            <category> java </category>
            
        </categories>
        
        
    </entry>
    
    <entry>
      <title><![CDATA[轮询 心跳]]></title>
      <url>/java/%E8%BD%AE%E8%AF%A2%20%E5%BF%83%E8%B7%B3.html</url>
      <content type="html"><![CDATA[<p>轮询：概括来说是服务端定时主动的去与要监控状态的客户端（或者叫其他系统）通信，询问当前的某种状态，客户端返回状态信息，客户端没有返回或返回错误、失效信息、则认为客户端已经宕机，然后服务端自己内部把这个客户端的状态保存下来（宕机或者其他），如果客户端正常，那么返回正常状态，如果客户端宕机或者返回的是定义的失效状态那么当前的客户端状态是能够及时的监控到的，如果客户端宕机之后重启了那么当服务端定时来轮询的时候，还是可以正常的获取返回信息，把其状态重新更新。</p>
<p>轮询：客户端定时向服务器发送Ajax请求，服务器接到请求后马上返回响应信息并关闭连接。<br>优点：后端程序编写比较容易。<br>缺点：请求中有大半是无用，浪费带宽和服务器资源。<br>实例：适于小型应用。</p>
<p>长轮询：客户端向服务器发送Ajax请求，服务器接到请求后hold住连接，直到有新消息才返回响应信息并关闭连接，客户端处理完响应信息后再向服务器发送新的请求。<br>优点：在无消息的情况下不会频繁的请求，耗费资源小。<br>缺点：服务器hold连接会消耗资源，返回数据顺序无保证，难于管理维护。<br>实例：WebQQ、Hi网页版、Facebook IM。</p>
<p>长连接：在页面里嵌入一个隐蔵iframe，将这个隐蔵iframe的src属性设为对一个长连接的请求或是采用xhr请求，服务器端就能源源不断地往客户端输入数据。<br>优点：消息即时到达，不发无用请求；管理起来也相对方便。<br>缺点：服务器维护一个长连接会增加开销。<br>实例：Gmail聊天</p>
<p>Flash Socket：在页面中内嵌入一个使用了Socket类的 Flash 程序JavaScript通过调用此Flash程序提供的Socket接口与服务器端的Socket接口进行通信，JavaScript在收到服务器端传送的信息后控制页面的显示。<br>优点：实现真正的即时通信，而不是伪即时。<br>缺点：客户端必须安装Flash插件；非HTTP协议，无法自动穿越防火墙。<br>实例：网络互动游戏。</p>
<p>心跳：最终得到的结果是与轮询一样的但是实现的方式有差别，心跳不是服务端主动去发信息检测客户端状态，而是在服务端保存下来所有客户端的状态信息，然后等待客户端定时来访问服务端，更新自己的当前状态，如果客户端超过指定的时间没有来更新状态，则认为客户端已经宕机或者其状态异常。</p>
]]></content>
      
        <categories>
            
            <category> java </category>
            
        </categories>
        
        
    </entry>
    
    <entry>
      <title><![CDATA[java笔记]]></title>
      <url>/java/java%E7%AC%94%E8%AE%B0.html</url>
      <content type="html"><![CDATA[<p>java 是区分单双引号的<br>new 数组是要定义数组个数的 个数值 等于数组最后一个值-1</p>
<p> 如果方法的返回类型为 void ，则方法中不能使用 return 返回值！<br> 方法的返回值最多只能有一个，不能返回多个值; return 需要带参数!<br>  方法返回值的类型必须兼容，例如，如果返回值类型为 int ，则不能返回 String 型值</p>
<p>1、 静态方法中可以直接调用同类中的静态成员，但不能直接调用非静态成员。</p>
<p>如果希望在静态方法中调用非静态变量，可以通过创建类的对象，然后通过对象来访问非静态变量。</p>
<p>2、 在普通成员方法中，则可以直接访问同类的非静态变量和静态变量</p>
<p>3、 静态方法中不能直接调用非静态方法，需要通过对象来访问非静态方法。</p>
<p>Java 中可以通过初始化块进行数据赋值<br>在类的声明中，可以包含多个初始化块，当创建类的实例时，就会依次执行这些代码块。如果使用 static 修饰初始化块，就称为静态初始化块。</p>
<p>需要特别注意：静态初始化块只在类加载时执行，且只会执行一次，同时静态初始化块只能给静态变量赋值，不能初始化普通的成员变量。</p>
<p>程序运行时静态初始化块最先被执行，然后执行普通初始化块，最后才执行构造方法。由于静态初始化块只在类加载时执行一次，所以当再次创建对象 hello2 时并未执行静态初始化块。</p>
<p>1、 静态内部类不能直接访问外部类的非静态成员，但可以通过 new 外部类().成员 的方式访问 </p>
<p>2、 如果外部类的静态成员与内部类的成员名称相同，可通过“类名.静态成员”访问外部类的静态成员；如果外部类的静态成员与内部类的成员名称不相同，则可通过“成员名”直接调用外部类的静态成员</p>
<p>3、 创建静态内部类的对象时，不需要外部类的对象，可以直接创建 内部类 对象名= new 内部类();</p>
<p>方法内部类就是内部类定义在外部类的方法中，方法内部类只在该方法的内部可见，即只在该方法内可以使用。<br>一定要注意哦：由于方法内部类不能在外部类的方法以外的地方使用，因此方法内部类不能使用访问控制符和 static 修饰符。</p>
<p>String 对象创建后则不能被修改，是不可变的，所谓的修改其实是创建了新的对象，所指向的内存空间不同。</p>
<p>局部变量不会自动赋初始值</p>
<p>两类变量同名时局部变量具有更高的优先级</p>
<h3 id="构造方法"><a href="#构造方法" class="headerlink" title="构造方法"></a>构造方法</h3><ol>
<li>使用new+构造方法 创建一个新的对象</li>
<li>构造方法是定义在java类中的一个用来初始化对象的方法</li>
<li>构造方法与类同名且没有返回值</li>
</ol>
<hr>
]]></content>
      
        <categories>
            
            <category> java </category>
            
        </categories>
        
        
    </entry>
    
    <entry>
      <title><![CDATA[slapd配置文件]]></title>
      <url>/CDH/OpenLDAP/slapd%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6.html</url>
      <content type="html"><![CDATA[<p><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">#</div><div class="line"># See slapd.conf(5) for details on configuration options.</div><div class="line"># This file should NOT be world readable.</div><div class="line">#</div><div class="line"></div><div class="line">include		/etc/openldap/schema/corba.schema</div><div class="line">include		/etc/openldap/schema/core.schema</div><div class="line">include		/etc/openldap/schema/cosine.schema</div><div class="line">include		/etc/openldap/schema/duaconf.schema</div><div class="line">include		/etc/openldap/schema/dyngroup.schema</div><div class="line">include		/etc/openldap/schema/inetorgperson.schema</div><div class="line">include		/etc/openldap/schema/java.schema</div><div class="line">include		/etc/openldap/schema/misc.schema</div><div class="line">include		/etc/openldap/schema/nis.schema</div><div class="line">include		/etc/openldap/schema/openldap.schema</div><div class="line">include		/etc/openldap/schema/ppolicy.schema</div><div class="line">include		/etc/openldap/schema/collective.schema</div><div class="line"></div><div class="line"># Allow LDAPv2 client connections.  This is NOT the default.</div><div class="line">allow bind_v2</div><div class="line"></div><div class="line"># Do not enable referrals until AFTER you have a working directory</div><div class="line"># service AND an understanding of referrals.</div><div class="line">#referral	ldap://root.openldap.org</div><div class="line"></div><div class="line">pidfile		/var/run/openldap/slapd.pid</div><div class="line">argsfile	/var/run/openldap/slapd.args</div><div class="line"></div><div class="line"># Load dynamic backend modules</div><div class="line"># - modulepath is architecture dependent value (32/64-bit system)</div><div class="line"># - back_sql.la overlay requires openldap-server-sql package</div><div class="line"># - dyngroup.la and dynlist.la cannot be used at the same time</div><div class="line"></div><div class="line"># modulepath /usr/lib/openldap</div><div class="line"># modulepath /usr/lib64/openldap</div><div class="line"></div><div class="line"># moduleload accesslog.la</div><div class="line"># moduleload auditlog.la</div><div class="line"># moduleload back_sql.la</div><div class="line"># moduleload chain.la</div><div class="line"># moduleload collect.la</div><div class="line"># moduleload constraint.la</div><div class="line"># moduleload dds.la</div><div class="line"># moduleload deref.la</div><div class="line"># moduleload dyngroup.la</div><div class="line"># moduleload dynlist.la</div><div class="line"># moduleload memberof.la</div><div class="line"># moduleload pbind.la</div><div class="line"># moduleload pcache.la</div><div class="line"># moduleload ppolicy.la</div><div class="line"># moduleload refint.la</div><div class="line"># moduleload retcode.la</div><div class="line"># moduleload rwm.la</div><div class="line"># moduleload seqmod.la</div><div class="line"># moduleload smbk5pwd.la</div><div class="line"># moduleload sssvlv.la</div><div class="line"># moduleload syncprov.la</div><div class="line"># moduleload translucent.la</div><div class="line"># moduleload unique.la</div><div class="line"># moduleload valsort.la</div><div class="line"></div><div class="line"># The next three lines allow use of TLS for encrypting connections using a</div><div class="line"># dummy test certificate which you can generate by running</div><div class="line"># /usr/libexec/openldap/generate-server-cert.sh. Your client software may balk</div><div class="line"># at self-signed certificates, however.</div><div class="line">TLSCACertificatePath /etc/openldap/certs</div><div class="line">TLSCertificateFile &quot;\&quot;OpenLDAP Server\&quot;&quot;</div><div class="line">TLSCertificateKeyFile /etc/openldap/certs/password</div><div class="line"></div><div class="line"># Sample security restrictions</div><div class="line">#	Require integrity protection (prevent hijacking)</div><div class="line">#	Require 112-bit (3DES or better) encryption for updates</div><div class="line">#	Require 63-bit encryption for simple bind</div><div class="line"># security ssf=1 update_ssf=112 simple_bind=64</div><div class="line"></div><div class="line"># Sample access control policy:</div><div class="line">#	Root DSE: allow anyone to read it</div><div class="line">#	Subschema (sub)entry DSE: allow anyone to read it</div><div class="line">#	Other DSEs:</div><div class="line">#		Allow self write access</div><div class="line">#		Allow authenticated users read access</div><div class="line">#		Allow anonymous users to authenticate</div><div class="line">#	Directives needed to implement policy:</div><div class="line"># access to dn.base=&quot;&quot; by * read</div><div class="line"># access to dn.base=&quot;cn=Subschema&quot; by * read</div><div class="line"># access to *</div><div class="line">#	by self write</div><div class="line">#	by users read</div><div class="line">#	by anonymous auth</div><div class="line">#</div><div class="line"># if no access controls are present, the default policy</div><div class="line"># allows anyone and everyone to read anything but restricts</div><div class="line"># updates to rootdn.  (e.g., &quot;access to * by * read&quot;)</div><div class="line">#</div><div class="line"># rootdn can always read and write EVERYTHING!</div><div class="line"></div><div class="line"># enable on-the-fly configuration (cn=config)</div><div class="line">database config</div><div class="line">access to *</div><div class="line">	by dn.exact=&quot;gidNumber=0+uidNumber=0,cn=peercred,cn=external,cn=auth&quot; manage</div><div class="line">	by * none</div><div class="line"></div><div class="line"># enable server status monitoring (cn=monitor)</div><div class="line">database monitor</div><div class="line">access to *</div><div class="line">	by dn.exact=&quot;gidNumber=0+uidNumber=0,cn=peercred,cn=external,cn=auth&quot; read</div><div class="line">        by dn.exact=&quot;cn=Manager,dc=my-domain,dc=com&quot; read</div><div class="line">        by * none</div><div class="line"></div><div class="line">#######################################################################</div><div class="line"># database definitions</div><div class="line">#######################################################################</div><div class="line"></div><div class="line">database	bdb</div><div class="line">suffix		&quot;dc=my-domain,dc=com&quot;</div><div class="line">checkpoint	1024 15</div><div class="line">rootdn		&quot;cn=Manager,dc=my-domain,dc=com&quot;</div><div class="line"># Cleartext passwords, especially for the rootdn, should</div><div class="line"># be avoided.  See slappasswd(8) and slapd.conf(5) for details.</div><div class="line"># Use of strong authentication encouraged.</div><div class="line"># rootpw		secret</div><div class="line"># rootpw		&#123;crypt&#125;ijFYNcSNctBYg</div><div class="line"></div><div class="line"># The database directory MUST exist prior to running slapd AND </div><div class="line"># should only be accessible by the slapd and slap tools.</div><div class="line"># Mode 700 recommended.</div><div class="line">directory	/var/lib/ldap</div><div class="line"></div><div class="line"># Indices to maintain for this database</div><div class="line">index objectClass                       eq,pres</div><div class="line">index ou,cn,mail,surname,givenname      eq,pres,sub</div><div class="line">index uidNumber,gidNumber,loginShell    eq,pres</div><div class="line">index uid,memberUid                     eq,pres,sub</div><div class="line">index nisMapName,nisMapEntry            eq,pres,sub</div><div class="line"></div><div class="line"># Replicas of this database</div><div class="line">#replogfile /var/lib/ldap/openldap-master-replog</div><div class="line">#replica host=ldap-1.example.com:389 starttls=critical</div><div class="line">#     bindmethod=sasl saslmech=GSSAPI</div><div class="line">#     authcId=host/ldap-master.example.com@EXAMPLE.COM</div></pre></td></tr></table></figure></p>
]]></content>
      
        <categories>
            
            <category> CDH </category>
            
            <category> OpenLDAP </category>
            
        </categories>
        
        
    </entry>
    
    <entry>
      <title><![CDATA[hive Sentry 权限配置]]></title>
      <url>/CDH/Sentry/hive%20Sentry%20%E6%9D%83%E9%99%90%E9%85%8D%E7%BD%AE.html</url>
      <content type="html"><![CDATA[<h1 id="hive-Sentry用户授权配置"><a href="#hive-Sentry用户授权配置" class="headerlink" title="hive Sentry用户授权配置"></a>hive Sentry用户授权配置</h1><p><a href="www.cloudera.com/documentation/enterprise/latest/topics/sg_hive_sql.html#concept_c2q_4qx_p4__section_t5x_pg4_rp">Hive SQL Syntax for Use with Sentry</a></p>
<h2 id="为Hive用户添加全局的权限。"><a href="#为Hive用户添加全局的权限。" class="headerlink" title="为Hive用户添加全局的权限。"></a>为Hive用户添加全局的权限。</h2><p>目前的管理员为Hive，用户授权的操作都需要由Hive完成。这里使用Hive用户的Kerberos账户进入Beeline<br>为hive用户获取principal </p>
<p><code>kinit -kt hive.keytab  hive</code></p>
<p>列出了当前缓存中保存的Kerberos主体和Kerberos principal</p>
<p><code>klist</code></p>
<p>由于我们使用的是sentry基于数据库的存储方式,我们直接建在了hive的元数据库中</p>
<p><code>beeline -u &#39;jdbc:hive2://xx.xxx.xx.xx:xxxx/rptdata;principal=hive/xxxx@XXXX&#39;</code></p>
<p>在beeline中，添加管理员角色，然后将Sentry服务器上的所有权限付给hive用户，然后将admin的角色赋给hive组，让hive组能够自由的建立数据库。</p>
<h3 id="创建角色声明"><a href="#创建角色声明" class="headerlink" title="创建角色声明"></a>创建角色声明</h3><p>语句创建一个可以授予权限的角色。权限可以授予角色，然后可以将其分配给用户。已分配角色的用户只能行使该角色的权限。<br>只有具有管理权限的用户才能创建/删除角色。默认情况下hive, impala 和 hue用户在Sentry中拥有管理员权限。(然而并没有)</p>
<p><code>CREATE ROLE [role_name];</code></p>
<h3 id="删除角色声明"><a href="#删除角色声明" class="headerlink" title="删除角色声明"></a>删除角色声明</h3><p>一旦删除，该角色将被撤销给以前分配给它的所有用户。已经执行的查询不会受到影响。但是，由于Hive在执行每个查询之前检查用户权限，因此角色已启用的活动用户会话将受到影响。</p>
<p><code>DROP ROLE [role_name];</code></p>
<h3 id="授权角色声明"><a href="#授权角色声明" class="headerlink" title="授权角色声明"></a>授权角色声明</h3><p>只有Sentry管理员用户可以将角色授予组。</p>
<p><code>GRANT ROLE role_name [, role_name] TO GROUP &lt;groupName&gt; [,GROUP &lt;groupName&gt;]</code></p>
<h3 id="撤销角色声明"><a href="#撤销角色声明" class="headerlink" title="撤销角色声明"></a>撤销角色声明</h3><p>可用于从组中撤销角色。只有Sentry管理员用户可以从组中撤销该角色。</p>
<p><code>REVOKE ROLE role_name [, role_name] FROM GROUP &lt;groupName&gt; [,GROUP &lt;groupName&gt;]</code></p>
<h3 id="赋予角色权限声明"><a href="#赋予角色权限声明" class="headerlink" title="赋予角色权限声明"></a>赋予角色权限声明</h3><p>用户必须是Sentry admin用户。</p>
<p><code>GRANT &lt;PRIVILEGE&gt; [, &lt;PRIVILEGE&gt; ] ON &lt;OBJECT&gt; &lt;object_name&gt; TO ROLE &lt;roleName&gt; [,ROLE &lt;roleName&gt;]</code></p>
<h3 id="赋予角色外部表权限"><a href="#赋予角色外部表权限" class="headerlink" title="赋予角色外部表权限"></a>赋予角色外部表权限</h3><p><code>GRANT ALL ON URI &#39;hdfs://namenode:XXX/path/to/table&#39;</code></p>
<p><code>grant all on URI &#39;hdfs://nameservice1/user/thadoop/ods&#39; to role thadoop</code></p>
<h3 id="撤销角色权限"><a href="#撤销角色权限" class="headerlink" title="撤销角色权限"></a>撤销角色权限</h3><p>只有Sentry管理员用户可以从组中撤销权限</p>
<p><code>REVOKE &lt;PRIVILEGE&gt; [, &lt;PRIVILEGE&gt; ] ON &lt;OBJECT&gt; &lt;object_name&gt; FROM ROLE &lt;roleName&gt; [,ROLE &lt;roleName&gt;]</code></p>
<h3 id="指定角色声明"><a href="#指定角色声明" class="headerlink" title="指定角色声明"></a>指定角色声明</h3><p>用于指定当前会话启用的角色。用户只能启用已授予他们的角色。任何未列出，尚未启用的角色均将禁用当前会话。如果没有指定启用任何角色，用户将具有由他所属的所有角色授予的权限。</p>
<ul>
<li>启用特定角色</li>
</ul>
<p><code>SET ROLE &lt;roleName&gt;;</code></p>
<ul>
<li>启用所有角色：</li>
</ul>
<p><code>SET ROLE ALL;</code></p>
<ul>
<li>未启用角色：</li>
</ul>
<p><code>SET ROLE NONE;</code></p>
<h3 id="show声明"><a href="#show声明" class="headerlink" title="show声明"></a>show声明</h3><ul>
<li>列出当前用户具有数据库，表或列级访问的数据库：</li>
</ul>
<p><code>SHOW DATABASES;</code></p>
<ul>
<li>列出当前用户具有表或列级访问权限的表：</li>
</ul>
<p><code>SHOW TABLES;</code></p>
<ul>
<li>列出当前用户拥有的列的访问：</li>
</ul>
<p><code>SHOW COLUMNS;</code></p>
<ul>
<li>列出系统中的所有角色（仅适用于哨兵管理员用户）：</li>
</ul>
<p><code>SHOW ROLES;</code></p>
<ul>
<li>列出当前用户会话有效的所有角色：</li>
</ul>
<p><code>SHOW CURRENT ROLES;</code></p>
<ul>
<li>列出分配给给定的所有角色 &lt;groupName&gt; （仅允许Sentry管理员用户和属于指定的组的其他用户 &lt;groupName&gt;）：</li>
</ul>
<p><code>SHOW ROLE GRANT GROUP &lt;groupName&gt;;</code></p>
<ul>
<li>SHOW语句也可以用于列出授予角色的特权或给予特定对象的角色的所有特权</li>
</ul>
<p><code>SHOW GRANT ROLE &lt;roleName&gt;;</code></p>
<ul>
<li>列出给定的角色的所有授权</li>
</ul>
<p><code>SHOW GRANT ROLE &lt;roleName&gt; on OBJECT &lt;objectName&gt;;</code></p>
<h2 id="操作样例"><a href="#操作样例" class="headerlink" title="操作样例:"></a>操作样例:</h2><p><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">&gt; create role test_role;</div><div class="line">&gt; grant all on server hive to role test_role;</div><div class="line">&gt; show roles;</div><div class="line">+---------------+--+</div><div class="line">|     role      |</div><div class="line">+---------------+--+</div><div class="line">| test_role     |</div><div class="line">| thadoop       |</div><div class="line">| admin_role    |</div><div class="line">| thadoop_role  |</div><div class="line">+---------------+--+</div><div class="line">4 rows selected (0.179 seconds)</div><div class="line">&gt; show grant role test_role;</div><div class="line">+-----------+--------+------------+---------+-----------------+-----------------+------------+---------------+-------------------+----------+--+</div><div class="line">| database  | table  | partition  | column  | principal_name  | principal_type  | privilege  | grant_option  |    grant_time     | grantor  |</div><div class="line">+-----------+--------+------------+---------+-----------------+-----------------+------------+---------------+-------------------+----------+--+</div><div class="line">| *         |        |            |         | test_role       | ROLE            | *          | false         | 1493690779745000  | --       |</div><div class="line">+-----------+--------+------------+---------+-----------------+-----------------+------------+---------------+-------------------+----------+--+</div><div class="line">1 row selected (0.146 seconds)</div><div class="line">&gt; show grant role admin_role;</div><div class="line"></div><div class="line">+-----------+--------+------------+---------+-----------------+-----------------+------------+---------------+-------------------+----------+--+</div><div class="line">| database  | table  | partition  | column  | principal_name  | principal_type  | privilege  | grant_option  |    grant_time     | grantor  |</div><div class="line">+-----------+--------+------------+---------+-----------------+-----------------+------------+---------------+-------------------+----------+--+</div><div class="line">| *         |        |            |         | admin_role      | ROLE            | *          | false         | 1490532327514000  | --       |</div><div class="line">+-----------+--------+------------+---------+-----------------+-----------------+------------+---------------+-------------------+----------+--+</div><div class="line">1 row selected (0.167 seconds)</div><div class="line"></div><div class="line">&gt; drop role test_role;</div><div class="line">&gt; show roles;</div><div class="line">+---------------+--+</div><div class="line">|     role      |</div><div class="line">+---------------+--+</div><div class="line">| thadoop       |</div><div class="line">| admin_role    |</div><div class="line">| thadoop_role  |</div><div class="line">+---------------+--+</div><div class="line">&gt; grant select on database rpt to role thadoop;</div><div class="line">&gt; grant insert on database rpt to role thadoop;</div><div class="line">&gt; grant all on database rptdata to role thadoop;</div><div class="line"></div><div class="line">#对于建表权限 必须先把要要建的表的权限赋给相应的role</div><div class="line">&gt; grant all on database ods to role thadoop;</div><div class="line">&gt; show grant role thadoop;</div><div class="line">+---------------------------------------+-----------------------------------+------------+---------+-----------------+-----------------+------------+---------------+-------------------+----------+--+</div><div class="line">|               database                |               table               | partition  | column  | principal_name  | principal_type  | privilege  | grant_option  |    grant_time     | grantor  |</div><div class="line">+---------------------------------------+-----------------------------------+------------+---------+-----------------+-----------------+------------+---------------+-------------------+----------+--+</div><div class="line">| hdfs://nameservice1/user/thadoop/ods  |                                   |            |         | thadoop         | ROLE            | *          | false         | 1493369887314000  | --       |</div><div class="line">| ods                                   | pub_50118_visit_log_voms_tour_ex  |            |         | thadoop         | ROLE            | select     | false         | 1492593741232000  | --       |</div><div class="line">| rptdata                               |                                   |            |         | thadoop         | ROLE            | *          | false         | 1492619387707000  | --       |</div><div class="line">| rpt                                   |                                   |            |         | thadoop         | ROLE            | *          | false         | 1493023378368000  | --       |</div><div class="line">| ods                                   |                                   |            |         | thadoop         | ROLE            | *          | false         | 1493369642620000  | --       |</div><div class="line">+---------------------------------------+-----------------------------------+------------+---------+-----------------+-----------------+------------+---------------+-------------------+----------+--+</div><div class="line">5 rows selected (0.159 seconds)</div><div class="line">&gt; use ods;</div><div class="line">&gt; revoke select on table pub_50118_visit_log_voms_tour_ex from role thadoop;</div><div class="line">&gt; show grant role thadoop;</div><div class="line">+---------------------------------------+--------+------------+---------+-----------------+-----------------+------------+---------------+-------------------+----------+--+</div><div class="line">|               database                | table  | partition  | column  | principal_name  | principal_type  | privilege  | grant_option  |    grant_time     | grantor  |</div><div class="line">+---------------------------------------+--------+------------+---------+-----------------+-----------------+------------+---------------+-------------------+----------+--+</div><div class="line">| ods                                   |        |            |         | thadoop         | ROLE            | *          | false         | 1493369642620000  | --       |</div><div class="line">| hdfs://nameservice1/user/thadoop/ods  |        |            |         | thadoop         | ROLE            | *          | false         | 1493369887314000  | --       |</div><div class="line">| rptdata                               |        |            |         | thadoop         | ROLE            | *          | false         | 1492619387707000  | --       |</div><div class="line">| rpt                                   |        |            |         | thadoop         | ROLE            | *          | false         | 1493023378368000  | --       |</div><div class="line">+---------------------------------------+--------+------------+---------+-----------------+-----------------+------------+---------------+-------------------+----------+--+</div><div class="line">4 rows selected (0.142 seconds)</div></pre></td></tr></table></figure>
</p>
<h4 id="此外发现hive开启sentry后-hue中对应的用户也得是sentry最终授权的用户-否则将无法访问元数据"><a href="#此外发现hive开启sentry后-hue中对应的用户也得是sentry最终授权的用户-否则将无法访问元数据" class="headerlink" title="此外发现hive开启sentry后 hue中对应的用户也得是sentry最终授权的用户,否则将无法访问元数据"></a>此外发现hive开启sentry后 hue中对应的用户也得是sentry最终授权的用户,否则将无法访问元数据</h4>]]></content>
      
        <categories>
            
            <category> CDH </category>
            
            <category> Sentry </category>
            
        </categories>
        
        
    </entry>
    
    <entry>
      <title><![CDATA[nginx]]></title>
      <url>/nginx/nginx.html</url>
      <content type="html"><![CDATA[<ul>
<li>nginx采用基于事件的模型和OS依赖的机制在工作进程之间有效地分配请求</li>
<li></li>
</ul>
]]></content>
      
        <categories>
            
            <category> nginx </category>
            
        </categories>
        
        
    </entry>
    
    <entry>
      <title><![CDATA[SUSE开机启动脚本配置文件]]></title>
      <url>/CDH/Shell/SUSE%E5%BC%80%E6%9C%BA%E5%90%AF%E5%8A%A8%E8%84%9A%E6%9C%AC%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6.html</url>
      <content type="html"><![CDATA[<h1 id="SUSE开机启动脚本配置文件"><a href="#SUSE开机启动脚本配置文件" class="headerlink" title="SUSE开机启动脚本配置文件"></a>SUSE开机启动脚本配置文件</h1><p>关于开机启动脚本配置文件  SUSE与其它Linux设置不一样</p>
<p>那就是 /etc/init.d 下的几个档案</p>
<ol>
<li>boot.local –&gt; 这个开机启动档案会在 rc5.d 前就有动作</li>
<li>halt.local –&gt; 这个关机启动档案会在最后有动作</li>
<li>before.local –&gt; 这个档案比较用不到所以不需多做解释</li>
<li>after.local –&gt; 这个档案会在 rc5.d 之后有动作 , 就是最重要的开机启动档 ， 没有的话 新建一个</li>
</ol>
<p>上面第三及第四个档案预设是不存在的喔!!<br>当你看过 /etc/init.d/rc 这个档案就知道为什幺了<br>所以当你要使用第三或第四个档案时请自己建立, 就像你写个shell一样很简单</p>
<p>例:<br><code>vi /etc/init.d/after.local</code><br><figure class="highlight bash"><table><tr><td class="code"><pre><div class="line"><span class="meta">#! /bin/sh</span></div><div class="line">/usr/tem/run.sh</div><div class="line"> </div><div class="line">:wq</div></pre></td></tr></table></figure></p>
]]></content>
      
        <categories>
            
            <category> CDH </category>
            
            <category> Shell </category>
            
        </categories>
        
        
    </entry>
    
    <entry>
      <title><![CDATA[执行数据节点的磁盘热插拔]]></title>
      <url>/CDH/Error/%E6%89%A7%E8%A1%8C%E6%95%B0%E6%8D%AE%E8%8A%82%E7%82%B9%E7%9A%84%E7%A3%81%E7%9B%98%E7%83%AD%E6%8F%92%E6%8B%94.html</url>
      <content type="html"><![CDATA[<h1 id="执行数据节点的磁盘热插拔"><a href="#执行数据节点的磁盘热插拔" class="headerlink" title="执行数据节点的磁盘热插拔"></a>执行数据节点的磁盘热插拔</h1><blockquote>
<p><strong>v1.0</strong> updated:2017-04-01 何常通</p>
</blockquote>
<p><a href="https://www.cloudera.com/documentation/enterprise/latest/topics/admin_dn_swap.html" target="_blank" rel="external">在不关闭DataNode的情况下替换HDFS磁盘,这被称为热插拔</a></p>
<h4 id="警告-要求和限制"><a href="#警告-要求和限制" class="headerlink" title="警告:要求和限制"></a>警告:要求和限制</h4><p><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">CDH 5.4及更高版本支持热插拔。</div><div class="line">热插拔只能添加带有空数据目录的磁盘。</div><div class="line">删除磁盘不会将数据移出磁盘，这可能会导致数据丢失。</div><div class="line">不要同时在多台主机上执行热插拔。</div></pre></td></tr></table></figure>
</p>
<h2 id="使用Cloudera-Manager执行数据节点的磁盘热插拔"><a href="#使用Cloudera-Manager执行数据节点的磁盘热插拔" class="headerlink" title="使用Cloudera Manager执行数据节点的磁盘热插拔"></a>使用Cloudera Manager执行数据节点的磁盘热插拔</h2><p><strong>角色最低要求</strong>：  Cluster Administrator </p>
<ol>
<li><p>配置数据目录以删除要交换的磁盘：</p>
<ol>
<li>选择导航栏上Cluster转到HDFS服务。</li>
<li>单击<strong>Instances</strong>选项卡。</li>
<li>单击受影响的DataNode。</li>
<li>单击<strong>Configuration</strong>选项卡。</li>
<li>选择左侧筛选器中的<strong>Scope</strong> &gt; <strong>DataNode</strong>。</li>
<li>选择左侧筛选器中的<strong>Category</strong> &gt; <strong>Main</strong>。</li>
<li>更改<strong>DataNode Data Directory</strong>属性的值，以删除要删除的磁盘的挂载点的目录。有哪些角色的都要删掉(hdfs,impala,yarn)</li>
</ol>
<p><strong>警告：仅对要计划热插拔磁盘的特定DataNode实例更改此属性的值。不要编辑此属性的</strong>角色组<strong>值。否则会导致数据丢失和角色变更。</strong></p>
</li>
<li><p>单击<strong>Save Changes</strong>以提交更改。</p>
</li>
<li>刷新受影响的DataNode。选择<strong>Actions</strong> &gt; 刷新数据目录。</li>
<li>删除旧磁盘并添加替换磁盘。</li>
<li>更改DataNode Data Directory属性的值以添加作为您添加的磁盘的挂接点的目录。</li>
<li>单击<strong>Save Changes</strong>以提交更改。</li>
<li>刷新受影响的DataNode。选择 <strong>Actions</strong> &gt; <strong>Refresh Data Directories</strong>。</li>
<li>运行HDFS fsck 验证health of HDFS。</li>
</ol>
<h4 id="如果使用Cloudera-Manager-就不要使用命令行来操作磁盘热插拔"><a href="#如果使用Cloudera-Manager-就不要使用命令行来操作磁盘热插拔" class="headerlink" title="如果使用Cloudera Manager,就不要使用命令行来操作磁盘热插拔"></a>如果使用Cloudera Manager,就不要使用命令行来操作磁盘热插拔</h4><hr>
<p>##换盘步骤</p>
<p><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">df -h</div><div class="line">fdisk -l|more</div><div class="line">parted /dev/sdi</div><div class="line"></div><div class="line">parted -s $DEVICE mklabel gpt mkpart primary ext3</div><div class="line">mkfs.ext3 /dev/sdi1</div></pre></td></tr></table></figure>
</p>
<p><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">ddp-dn-121:~ # mkfs.ext3 /dev/sdi</div><div class="line">mke2fs 1.41.9 (22-Aug-2009)</div><div class="line">/dev/sdi is entire device, not just one partition!</div><div class="line">Proceed anyway? (y,n) y</div><div class="line">Filesystem label=</div><div class="line">OS type: Linux</div><div class="line">Block size=4096 (log=2)</div><div class="line">Fragment size=4096 (log=2)</div><div class="line">183107584 inodes, 732421632 blocks</div><div class="line">36621081 blocks (5.00%) reserved for the super user</div><div class="line">First data block=0</div><div class="line">Maximum filesystem blocks=4294967296</div><div class="line">22352 block groups</div><div class="line">32768 blocks per group, 32768 fragments per group</div><div class="line">8192 inodes per group</div><div class="line">Superblock backups stored on blocks: </div><div class="line">  32768, 98304, 163840, 229376, 294912, 819200, 884736, 1605632, 2654208, </div><div class="line">  4096000, 7962624, 11239424, 20480000, 23887872, 71663616, 78675968, </div><div class="line">  102400000, 214990848, 512000000, 550731776, 644972544</div><div class="line"></div><div class="line">Writing inode tables: done                            </div><div class="line">Creating journal (32768 blocks): done</div><div class="line">Writing superblocks and filesystem accounting information: </div><div class="line">mount /dev/sdi1 /mnt/sdi1/</div><div class="line">df -h</div></pre></td></tr></table></figure>
</p>
<hr>
<h2 id="根目录数据爆满"><a href="#根目录数据爆满" class="headerlink" title="根目录数据爆满"></a>根目录数据爆满</h2><blockquote>
<p>原因:因为不正确的换盘操作导致,cdh将数据写入系统盘</p>
</blockquote>
<ol>
<li>通过Cm停止该节点所有角色</li>
<li>然后umount掉所有盘 停止角色到能umount之间有时长 需要等待</li>
<li>然后计算每个目录大小  <code>du -hs /mnt/*</code></li>
<li>将数据备份</li>
<li>删除有数据的那个目录</li>
<li><p>接着mount所有盘</p>
<p><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">mount /dev/sdb1 /mnt/sdb1/</div><div class="line">mount /dev/sdc1 /mnt/sdc1/</div><div class="line">mount /dev/sdd1 /mnt/sdd1/</div><div class="line">mount /dev/sde1 /mnt/sde1/</div><div class="line">mount /dev/sdf1 /mnt/sdf1/</div><div class="line">mount /dev/sdg1 /mnt/sdg1/</div><div class="line">mount /dev/sdh1 /mnt/sdh1/</div><div class="line">mount /dev/sdi1 /mnt/sdi1/</div><div class="line">mount /dev/sdj1 /mnt/sdj1/</div><div class="line">mount /dev/sdk1 /mnt/sdk1/</div><div class="line">mount /dev/sdl1 /mnt/sdl1/</div><div class="line">mount /dev/sdm1 /mnt/sdm1/</div></pre></td></tr></table></figure>
</p>
</li>
<li><p>然后重启agent</p>
</li>
</ol>
<hr>
<h2 id="机房换盘步骤"><a href="#机房换盘步骤" class="headerlink" title="机房换盘步骤"></a>机房换盘步骤</h2><ol>
<li>机房打电话提醒换盘后,因为要重启机器,所以要求他们一台台操作</li>
<li>通过CM停止该节点所有角色,后通知机房</li>
<li>机房更换好后,因对新硬盘做分区和格式化 </li>
<li>之后mount对应磁盘</li>
<li>重启cloudera-scm-agent</li>
<li>CM上恢复对应角色</li>
<li>恢复后可能有一段时间连不上namenode节点,过一段时间后告警就会自动消失</li>
</ol>
<hr>
<h2 id="Linux-umount的device-is-busy问题"><a href="#Linux-umount的device-is-busy问题" class="headerlink" title="Linux umount的device is busy问题"></a>Linux umount的device is busy问题</h2><blockquote>
<p>原因：其他进程可能在使用/mnt/vdb目录。</p>
</blockquote>
<p><figure class="highlight bash"><table><tr><td class="code"><pre><div class="line">[root@dbserver ~]<span class="comment"># df -h</span></div><div class="line">文件系统 容量 已用 可用 已用%% 挂载点</div><div class="line">/dev/vda1 9.9G 3.9G 5.6G 41% /</div><div class="line">tmpfs 3.9G 100K 3.9G 1% /dev/shm</div><div class="line">/dev/sr0 368K 368K 0 100% /media/CDROM</div><div class="line">/dev/vdb 197G 5.9G 181G 4% /mnt</div><div class="line"></div><div class="line">[root@dbserver /]<span class="comment"># umount /mnt/vdb</span></div><div class="line">umount: /mnt: device is busy.</div><div class="line">(In some cases useful info about processes that use</div><div class="line">the device is found by lsof(8) or fuser(1))</div></pre></td></tr></table></figure>
</p>
<p>解决办法：</p>
<p>关闭使用该目录的进程，然后再umount。</p>
<p><figure class="highlight bash"><table><tr><td class="code"><pre><div class="line">[root@dbserver /]<span class="comment"># fuser -m /dev/vdb</span></div><div class="line">/dev/vdb: 3052c</div><div class="line"></div><div class="line">[root@dbserver /]<span class="comment"># ps aux |grep 3052</span></div><div class="line">root 2218 0.0 0.0 163052 1648 ? Sl 10:30 0:00 gnome-keyring-daemon --start</div><div class="line">root 3052 0.0 0.0 108328 1732 pts/1 Ss+ 11:12 0:00 -bash</div><div class="line"></div><div class="line">root 3448 0.0 0.0 6384 708 pts/5 S+ 12:59 0:00 grep 3052</div><div class="line"></div><div class="line">[root@dbserver /]<span class="comment"># kill -9 3052</span></div><div class="line">[root@dbserver /]<span class="comment"># ps aux |grep 3052</span></div><div class="line">root 2218 0.0 0.0 163052 1648 ? Sl 10:30 0:00 gnome-keyring-daemon --start</div><div class="line">root 3453 0.0 0.0 6384 704 pts/5 S+ 13:00 0:00 grep 3052</div><div class="line">[root@dbserver /]<span class="comment">#</span></div><div class="line"></div><div class="line">[root@dbserver /]<span class="comment"># umount /dev/vdb</span></div><div class="line">[root@dbserver /]<span class="comment"># df -h</span></div><div class="line">Filesystem Size Used Avail Use% Mounted on</div><div class="line">/dev/vda1 9.9G 1.4G 8.0G 15% /</div><div class="line">tmpfs 3.9G 100K 3.9G 1% /dev/shm</div><div class="line">/dev/sr0 368K 368K 0 100% /media/CDROM</div></pre></td></tr></table></figure>
</p>
]]></content>
      
        <categories>
            
            <category> CDH </category>
            
            <category> Error </category>
            
        </categories>
        
        
    </entry>
    
    <entry>
      <title><![CDATA[Impala复杂查询报错]]></title>
      <url>/CDH/Error/Impala%E5%A4%8D%E6%9D%82%E6%9F%A5%E8%AF%A2%E6%8A%A5%E9%94%99.html</url>
      <content type="html"><![CDATA[<h1 id="Impala-Error-Couldn’t-open-transport-未解决"><a href="#Impala-Error-Couldn’t-open-transport-未解决" class="headerlink" title="Impala Error: Couldn’t open transport(未解决)"></a>Impala Error: Couldn’t open transport(未解决)</h1><blockquote>
<p><strong>v1.0</strong> updated:2017-03-30 何常通<br><a href="https://community.cloudera.com/t5/Interactive-Short-cycle-SQL/Impala-Error-Couldn-t-open-transport/m-p/15916#M366" target="_blank" rel="external">原地址</a></p>
</blockquote>
<h2 id="当运行复杂查询的时候会报出这种错误"><a href="#当运行复杂查询的时候会报出这种错误" class="headerlink" title="当运行复杂查询的时候会报出这种错误"></a>当运行复杂查询的时候会报出这种错误</h2><p>When we try to run more complex Impala queries, we often run into the following error:<br>Couldn’t open transport for worker29.ourdomain.com:22000(connect() failed: Connection timed out)</p>
<p>Sometimes there’s only one node with that error message, sometimes there are 2-5.<br>There doesn’t seem to be a network related problem - ping works, telnet to that port works, Impala debug ui works.<br>We tried setting vm.swappiness on the nodes from 60 to 0 - no positive effect. Same with switching vm.overcommit from 0 to 1.</p>
<p>Our setup:</p>
<ul>
<li>around 40 nodes, i7 quad core, 2-3TB, 1Gbit NIC, located in 5 different racks</li>
<li>nodes have around 16-48GB ram, same amount of swap, which they alsmost never use</li>
<li>OS: Ubuntu Linux 12.04</li>
<li>CDH 5.1.0</li>
<li>impalad version 1.4.0-cdh5-INTERNAL RELEASE (build e801bd8c0d134e783c2313c7dd422a5ad06591af)</li>
<li>~100TB HDFS storage</li>
<li>we are using a HA proxy which points to the nodes with &gt;32GB ram</li>
<li>“workerlogs”-table is around 6-7TB big, partitioned by year &gt; month &gt; day and contains apache log-data</li>
<li>almost 100% short circuit reads</li>
</ul>
<hr>
<h3 id="Solution"><a href="#Solution" class="headerlink" title="Solution"></a>Solution</h3><p>We found cause for the error - our firewall settings were to restrictive. Interestingly smaller queries without many query fragments worked even with these restrictive settings.</p>
<h3 id="另一种解决方法"><a href="#另一种解决方法" class="headerlink" title="另一种解决方法"></a>另一种解决方法</h3><p>发现impala daemon 进程因为端口不能正常释放导致进程僵死,而且cloudera manage 并不能很好的反应这个问题,只能在重启impala集群的时候,才能统一发现问题节点<br>登入上出问题的节点后,手动kill -9 结束impala daemon 进程等所有端口22000 释放后 重启impala角色</p>
<h3 id="之后怀疑是catalog中java堆栈不足导致"><a href="#之后怀疑是catalog中java堆栈不足导致" class="headerlink" title="之后怀疑是catalog中java堆栈不足导致"></a>之后怀疑是catalog中java堆栈不足导致</h3><p>因为其会存放所有的hive表,调大后问题依旧</p>
]]></content>
      
        <categories>
            
            <category> CDH </category>
            
            <category> Error </category>
            
        </categories>
        
        
    </entry>
    
    <entry>
      <title><![CDATA[使用Packages升级到CDH 5.10]]></title>
      <url>/CDH/Upgrade/%E4%BD%BF%E7%94%A8Packages%E5%8D%87%E7%BA%A7%E5%88%B0CDH%205.10.html</url>
      <content type="html"><![CDATA[<h1 id="使用Packages升级到CDH-5-10"><a href="#使用Packages升级到CDH-5-10" class="headerlink" title="使用Packages升级到CDH 5.10"></a>使用Packages升级到CDH 5.10</h1><blockquote>
<p><strong>v1.0</strong> updated:2017-03-22 Nameless13<br>角色最低要求：Cluster Administrator </p>
</blockquote>
<h2 id="步骤1：收集升级信息"><a href="#步骤1：收集升级信息" class="headerlink" title="步骤1：收集升级信息"></a>步骤1：收集升级信息</h2><p>开始升级之前，请收集以下信息：</p>
<ol>
<li>主机凭据。您必须具有SSH访问权限，并且能够使用root帐户或具有无密码sudo权限的帐户登录。</li>
<li>集群中使用的Cloudera Manager版本。转到支持 &gt; 关于。( Cloudera Enterprise 5.4.7)</li>
<li>在集群中部署的JDK的版本。转到支持 &gt; 关于。(Java 版本: 1.7.0_67)</li>
<li>CDH的版本。CDH版本号显示在主页上的集群名称旁边。(CDH 5.10.0, Parcel)</li>
<li>群集是使用Parcels或packages安装的。此信息显示在Cloudera Manager 主页上的CDH版本旁边。</li>
<li>集群中启用的服务。转到 Clusters &gt; Cluster name。</li>
<li>操作系统类型和版本。转到主机并单击列表中的主机名。操作系统类型和版本显示在“ 详细信息”部分的“ 分发”行中。</li>
<li>Sqoop，Oozie，Hue，Hive Metastore和Sentry Server使用的数据库的数据库信息（仅在集群中启用这些服务时才需要信息）。</li>
<li>收集以下信息：<ul>
<li>数据库类型（PostgreSQL，嵌入式PostgreSQL，MySQL，MariaDB或Oracle）</li>
<li>数据库的主机名</li>
<li>数据库的Credentials </li>
</ul>
</li>
</ol>
<p>如何查找数据库信息：</p>
<ul>
<li>Sqoop，Oozie和Hue - 转到群集名称 &gt; 配置 &gt; 数据库设置。</li>
<li>Hive Metastore - 转到Hive服务，选择配置，然后选择Hive Metastore Database类别。</li>
<li>Sentry - 转到Sentry服务，选择配置，然后选择Sentry服务器数据库类别。</li>
</ul>
<h2 id="步骤2：完成升级前步骤"><a href="#步骤2：完成升级前步骤" class="headerlink" title="步骤2：完成升级前步骤"></a>步骤2：完成升级前步骤</h2><ol>
<li>查看要升级到的新版本的CDH 5和Cloudera Manager 5要求和支持的版本。</li>
<li>阅读CDH 5发行说明。</li>
<li>阅读Cloudera安全公告。</li>
<li>确保跨集群安装了Java 1.7或1.8。有关安装说明和建议，请参阅在Cloudera Manager部署中升级到Oracle JDK 1.7或升级到Oracle JDK 1.8，并确保在继续升级之前已阅读Cloudera Manager 5中的已知问题和解决方法。</li>
<li><p>确保Cloudera Manager次要版本等于或大于 CDH次要版本。例如：</p>
<ul>
<li><p>目标CDH版本： 5.0.5<br>最低Cloudera Manager版本： 5.0.x</p>
</li>
<li><p>目标CDH版本： 5.8.2<br>最低Cloudera Manager版本： 5.8.x</p>
</li>
</ul>
</li>
<li><p>如果要从CDH 5.1或更低版本升级，并使用 Hive Date partition columns，则可能需要更新日期格式。请参阅日期分区列。</p>
</li>
<li>如果集群使用Impala，请检查您的SQL与&lt;a href=&quot;https://www.cloudera.com/documentation/enterprise/latest/topics/impala_incompatible_changes.html#incompatible_changes&quot;&gt;不兼容更改文档&lt;/a&gt;中列出的最新保留字冲突。如果在多个版本之间升级或出现任何问题，请检查Impala关键字的完整列表。</li>
<li>运行主机检查器并解决每个问题。转到Cluster &gt; Inspect Hosts。</li>
<li>运行安全检查器，修复错误报告。（转到  Administration &gt; Security，然后单击Security Inspector。）</li>
<li>以hdfs用户身份登录到任何集群节点，运行以下命令，并更正所有的错误报告<br><code>hdfs fsck /</code><br><strong>注意：在fsck 命令可能需要10分钟或更长时间才能完成，具体取决于集群中的文件数。</strong><br><code>hdfs dfsadmin -report</code></li>
<li>以hdfs用户身份登录到任何DataNode，运行以下命令，并更正任何报告的错误<br><code>hbase hbck</code></li>
<li>查看升级过程并保留分配足够时间以执行所有步骤的维护窗口。对于生产集群，Cloudera建议根据主机数量，使用Hadoop和Linux的经验以及您使用的特定硬件，分配一整天的维护时段来执行升级。</li>
<li>为了避免在升级过程中出现不必要的警报，请在开始升级之前在集群上启用维护模式。这会停止发送电子邮件警报和SNMP陷阱，但不会停止检查和配置验证。完成升级后，务必退出维护模式以重新启用Cloudera Manager警报。</li>
<li>如果已将Hue配置为使用TLS / SSL，并且正在从CDH 5.2或更低版本升级到CDH 5.3或更高版本，则Hue会验证CA证书并需要信任库。要创建信任库，请按照Hue中的说明作为TLS / SSL客户端。</li>
<li>如果群集使用Flume Kafka客户端，并且要升级到CDH 5.8.0或CDH 5.8.1，请在使用Flume Kafka Client时执行升级到CDH 5.8.0或CDH 5.8.1中描述的其他步骤，然后继续与本主题中的程序。</li>
<li><p>如果集群使用Impala和Llama，则该角色从CDH 5.9起已被弃用，并且必须在开始升级之前从Impala服务中删除该角色。如果不删除此角色，升级向导将暂停升级。</p>
<p>要确定Impala是否使用Llama：</p>
<ol>
<li>转到Impala服务。</li>
<li>选择实例选项卡。</li>
<li>检查“ 角色类型”列中的角色列表。如果出现Llama，Impala服务正在使用Llama。</li>
</ol>
<p>要删除Llama角色：</p>
<ol>
<li><p>转到Impala服务，选择操作 &gt; 禁用YARN和Impala集成资源管理。</p>
<p>将显示“ 禁用YARN和Impala集成资源管理”向导。</p>
</li>
<li><p>单击继续。</p>
<p>该禁止纱线和Impala的综合资源管理命令页面显示命令的进度禁用的作用。</p>
</li>
<li><p>命令完成后，单击完成。</p>
</li>
</ol>
</li>
</ol>
<h2 id="步骤3：停止群集服务"><a href="#步骤3：停止群集服务" class="headerlink" title="步骤3：停止群集服务"></a>步骤3：停止群集服务</h2><p>本节中的步骤仅需进行以下升级：</p>
<pre><code>CDH 5.0或5.1至5.2或更高
CDH 5.2或5.3至5.4或更高


</code></pre><ol>
<li>在主页 &gt; 状态选项卡上，单击集群名称的右侧，然后选择停止。</li>
<li>在确认屏幕中单击停止。“ 命令详细信息 ”窗口显示停止服务的进度。当所有服务成功停止时，任务完成，您可以关闭“ 命令详细信息 ”窗口。</li>
</ol>
<h2 id="步骤4：备份NameNode上的HDFS元数据"><a href="#步骤4：备份NameNode上的HDFS元数据" class="headerlink" title="步骤4：备份NameNode上的HDFS元数据"></a>步骤4：备份NameNode上的HDFS元数据</h2><p>本节中的步骤仅需进行以下升级：</p>
<pre><code>CDH 5.0或5.1至5.2或更高
CDH 5.2或5.3至5.4或更高


</code></pre><ol>
<li>转到HDFS服务。</li>
<li>单击配置选项卡。</li>
<li>在搜索字段中，搜索“NameNode Data Directories”并记下该值。</li>
<li>在活动的NameNode主机上，备份NameNode Data Directories属性中列出的目录。如果列出了多个目录，请备份一个目录，因为每个目录是完整的副本。例如，如果NameNode数据目录是/ data / dfs / nn，请以root身份执行以下操作：<p><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line"># cd /data/dfs/nn</div><div class="line"># tar -cvf /root/nn_backup_data.tar .</div></pre></td></tr></table></figure>
</p>
</li>
</ol>
<p>你应该看到这样的输出：<br><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">./</div><div class="line">./current/</div><div class="line">./current/fsimage</div><div class="line">./current/fstime</div><div class="line">./current/VERSION</div><div class="line">./current/edits</div><div class="line">./image/</div><div class="line">./image/fsimage</div></pre></td></tr></table></figure></p>
<p>如果NameNode数据目录中存在具有 extension <em>lock</em>的文件，则NameNode很有可能仍在运行。重复这些步骤，从关闭NameNode角色开始。</p>
<h2 id="步骤5：备份数据库"><a href="#步骤5：备份数据库" class="headerlink" title="步骤5：备份数据库"></a>步骤5：备份数据库</h2><p><strong>注意：备份数据库需要停止某些服务，这可能会使其在备份期间不可用</strong><br>备份部署在群集中的以下任何服务的数据库：</p>
<table>
<thead>
<tr>
<th>服务</th>
<th>在哪里可以找到数据库信息</th>
</tr>
</thead>
<tbody>
<tr>
<td>Sqoop</td>
<td>转到集群 &gt; 集群名称 &gt; Sqoop服务 &gt; 配置并选择数据库类别。</td>
</tr>
<tr>
<td>HUE</td>
<td>转到集群 &gt; 集群名称 &gt; Hue服务 &gt; 配置并选择数据库类别。</td>
</tr>
<tr>
<td>Oozie</td>
<td>转到群集 &gt; 群集名称 &gt; Oozie服务 &gt; 配置并选择数据库类别。</td>
</tr>
<tr>
<td>Cloudera Navigator Audit Server</td>
<td>转到集群 &gt; Cloudera管理服务 &gt; 配置并选择数据库类别。</td>
</tr>
<tr>
<td>Cloudera Navigator Metadata Server</td>
<td>转到集群 &gt; Cloudera管理服务 &gt; 配置并选择数据库类别。</td>
</tr>
<tr>
<td>Activity Monitor</td>
<td>转到集群 &gt; Cloudera管理服务 &gt; 配置并选择数据库类别。</td>
</tr>
<tr>
<td>Reports Manager</td>
<td>转到集群 &gt; Cloudera管理服务 &gt; 配置并选择数据库类别。</td>
</tr>
<tr>
<td>Sentry Server</td>
<td>转到群集 &gt; 群集名称 &gt; Sentry服务 &gt; 配置并选择Sentry服务器数据库类别。</td>
</tr>
<tr>
<td>Hive Metastore</td>
<td>转到集群 &gt; 集群名称 &gt; Hive服务 &gt; 配置并选择Hive Metastore数据库类别。</td>
</tr>
</tbody>
</table>
<p>备份数据库：</p>
<ol>
<li>如果尚未停止，请停止服务：<ol>
<li>在主页 &gt; 状态选项卡上，单击服务名称的右侧，然后选择停止。</li>
<li>在下一个屏幕中单击停止以确认。当您看到已完成状态时，服务已停止。</li>
</ol>
</li>
<li>备份数据库。有关每种支持的数据库类型的详细说明，请参阅备份数据库。</li>
<li>重新启动服务：<ol>
<li>在主页 &gt; 状态选项卡上，单击服务名称的右侧，然后选择开始。</li>
<li>单击出现在下一屏幕中的开始以确认。当您看到已完成状态时，服务已启动。</li>
</ol>
</li>
</ol>
<h2 id="步骤6：运行升级向导"><a href="#步骤6：运行升级向导" class="headerlink" title="步骤6：运行升级向导"></a>步骤6：运行升级向导</h2><ol>
<li>从主页 &gt; 状态选项卡，单击集群名称旁边的并选择升级集群。<br>此时将显示升级向导的“ 入门”页面。</li>
<li>如果显示在packages and parcels 之间选择的选项，请选择 <strong>Use Parcels</strong>。</li>
<li>在“ 选择CDH版本（Parcels）”字段中，选择CDH版本。如果未列出Parcels，或者要升级到其他版本，请单击“ 修改远程Parcels存储库URL”链接，转到远程Parcels存储库URL的配置页面，并将相应的URL添加到配置。有关输入Parcels存储库的正确URL的信息，请参阅&lt;a href=&quot;https://www.cloudera.com/documentation/enterprise/latest/topics/cm_ig_parcels.html#cmug_topic_7_11_5&quot;&gt;Parcel配置设置&lt;/a&gt;。单击继续。</li>
<li>阅读升级前必须完成的步骤的注意事项，完成步骤后单击 <strong>Yes, I …</strong>复选框，然后单击继续。Cloudera Manager验证主机是否安装了正确的软件。</li>
<li>单击继续。     Host Inspector运行并在主机上显示CDH版本。</li>
<li><p>单击继续。    “ Choose Upgrade Procedure”屏幕显示可用的升级类型：</p>
<ul>
<li>Full Cluster Restart - Cloudera Manager执行所有服务升级并重新启动集群。</li>
<li>Manual upgrade Cloudera Manager将群集配置为指定的CDH版本，但不执行任何升级或服务重新启动。手动升级很困难，只适用于高级用户。要执行手动升级：<ol>
<li>选中”Let me upgrade the cluster”复选框。</li>
<li>单击继续。</li>
<li>有关所需的步骤，请参阅手动执行升级向导操作。</li>
</ol>
</li>
</ul>
</li>
<li><p>选择<strong>Full Cluster Restart</strong>。</p>
</li>
<li><p>单击继续。<br>“Upgrade Cluster Command”屏幕显示向导在关闭所有服务，激活新Parcels，升级服务，部署客户端配置文件和重新启动服务时运行的命令的结果。如果任何步骤失败，请更正所有错误报告，然后单击重试按钮。如果单击“ 中止 ”按钮，则将启用右上方的“ 重试 ”按钮。</p>
<p>单击重试以重试该步骤并继续向导，或单击Cloudera Manager徽标返回<strong>Home</strong> &gt; <strong>Status</strong>选项卡，然后手动执行失败的步骤和所有后续步骤。</p>
</li>
<li><p>单击继续。</p>
<p>向导将报告升级的结果。</p>
<p>如果先前使用packages安装或升级了集群，向导可能会指示某些服务无法启动，因为其Parcels不可用。要下载所需的Parcels：</p>
<pre><code>1. 在另一个浏览器选项卡中，打开Cloudera Manager Admin Console。
1. 选择 Hosts  &gt; Parcels。
1. 找到包含缺少的Parcels的行，然后单击按钮以下载，Distribute，然后激活Parcels。
1. 返回升级向导并单击重试按钮。
1. 升级向导继续升级集群。

</code></pre></li>
<li><p>单击完成返回主页。</p>
</li>
</ol>
<h2 id="步骤7：从失败的步骤恢复或执行手动升级"><a href="#步骤7：从失败的步骤恢复或执行手动升级" class="headerlink" title="步骤7：从失败的步骤恢复或执行手动升级"></a>步骤7：从失败的步骤恢复或执行手动升级</h2><p>“手动执行升级向导操作”中列出了升级向导执行的操作。如果“ 升级群集命令”屏幕中的任何步骤失败，请在继续之前完成该部分中所述的步骤。</p>
<h2 id="步骤8：删除以前的CDH版本包和刷新符号链接"><a href="#步骤8：删除以前的CDH版本包和刷新符号链接" class="headerlink" title="步骤8：删除以前的CDH版本包和刷新符号链接"></a>步骤8：删除以前的CDH版本包和刷新符号链接</h2><p>如果先前的安装或升级使用Parcels，请跳过此步骤。</p>
<p>如果先前安装的CDH是使用packages完成的，请在安装了packages的所有主机上删除这些packages，并刷新符号链接，以便客户端运行新的软件版本。</p>
<ol>
<li>如果您的Hue服务使用嵌入式SQLite数据库，请备份 /var/lib/hue/desktop.db 到 /var/lib/hue以外的位置因为在删除packages时会删除此目录。</li>
<li><p>卸载每个主机上的CDHpackages：</p>
<ul>
<li>不包括Impala和Search<br><code>sudo zypper remove bigtop-utils bigtop-jsvc bigtop-tomcat hue-common sqoop2-client</code></li>
<li>包括Impala和Search<br><code>sudo zypper remove&#39;bigtop- *&#39;hue-common impala-shell solr-server sqoop2-client hbase-solr-doc avro-libs crunch-doc avro-doc solr-doc</code></li>
</ul>
</li>
<li><p>重新启动所有Cloudera Manager Agent以强制更新符号链接，以指向每个主机上新安装的组件：<br><code>sudo service cloudera-scm-agent restart</code></p>
</li>
<li>如果您的Hue服务使用嵌入式SQLite数据库，请还原您备份的数据库：<ol>
<li>停止Hue服务。</li>
<li>将备份从临时位置复制到新创建的Hue数据库目录中，/var/lib/hue。</li>
<li>启动Hue服务。</li>
</ol>
</li>
</ol>
<h2 id="步骤9：完成HDFS元数据升级"><a href="#步骤9：完成HDFS元数据升级" class="headerlink" title="步骤9：完成HDFS元数据升级"></a>步骤9：完成HDFS元数据升级</h2><p>本节中的步骤仅需进行以下升级：</p>
<pre><code>CDH 5.0或5.1至5.2或更高
CDH 5.2或5.3至5.4或更高


</code></pre><p>要确定是否可以完成，请以important workloads运行以确保它们能成功。一旦升级完成后，将无法使用备份即无法回滚到先前版本的HDFS。验证您是否已完成升级可能需要很长时间。</p>
<p>请确保您有足够的可用磁盘空间，请记住以下行为，直到升级完成：</p>
<ul>
<li>删除文件不会释放磁盘空间。</li>
<li>使用balancer会导致所有移动的副本被复制。</li>
<li>表示NameNodes元数据的所有磁盘数据都被保留，这可以将NameNode和JournalNode磁盘所需的空间大小增加一倍。</li>
</ul>
<p>要完成元数据升级：</p>
<ul>
<li>转到HDFS服务。</li>
<li>单击Instances选项卡。</li>
<li>单击NameNode实例。</li>
<li>选择Actions&gt;Finalize Metadata Upgrade，然后单击 Finalize Metadata Upgrade以确认。<h2 id="步骤10：退出维护模式"><a href="#步骤10：退出维护模式" class="headerlink" title="步骤10：退出维护模式"></a>步骤10：退出维护模式</h2>如果在此升级期间进入维护模式，请退出维护模式。</li>
</ul>
]]></content>
      
        <categories>
            
            <category> CDH </category>
            
            <category> Upgrade </category>
            
        </categories>
        
        
    </entry>
    
    <entry>
      <title><![CDATA[12-20 微信商学院-开发者培训班]]></title>
      <url>/Document/12-20%20%E5%BE%AE%E4%BF%A1%E5%95%86%E5%AD%A6%E9%99%A2-%E5%BC%80%E5%8F%91%E8%80%85%E5%9F%B9%E8%AE%AD%E7%8F%AD.html</url>
      <content type="html"><![CDATA[<h1 id="12-20-微信商学院-开发者培训班"><a href="#12-20-微信商学院-开发者培训班" class="headerlink" title="12-20 微信商学院-开发者培训班"></a>12-20 微信商学院-开发者培训班</h1><h3 id="小程序是什么"><a href="#小程序是什么" class="headerlink" title="小程序是什么"></a>小程序是什么</h3><ul>
<li><a href="https://mp.weixin.qq.com/" target="_blank" rel="external">https://mp.weixin.qq.com/</a></li>
<li>开发者社区 <a href="http://developers.weixin.qq.com/" target="_blank" rel="external">http://developers.weixin.qq.com/</a></li>
<li>一种连接用户和服务的方式</li>
<li>一个全新的生态</li>
<li>一套解决方案</li>
<li>AppService 应用逻辑层,实现交互和业务逻辑处理,生命周期管理</li>
<li>JSBrigdge</li>
</ul>
<hr>
<h3 id="小程序的优点"><a href="#小程序的优点" class="headerlink" title="小程序的优点"></a>小程序的优点</h3><ul>
<li>不是H5(算是类web应用,基于H5开发但是可以调用微信native API,缺点是不能对API进行定制)</li>
<li>基于微信</li>
<li>原生体验</li>
<li>解决方案</li>
<li>即用即走,随手可得</li>
<li>拥有离线能力</li>
<li>一次开发,多端兼容</li>
<li>优秀的操作系统</li>
<li>在安卓上的适配是最优的</li>
</ul>
<h3 id="小程序的接入流程"><a href="#小程序的接入流程" class="headerlink" title="小程序的接入流程"></a>小程序的接入流程</h3><p>注册-&gt;开发-&gt;提交审核-&gt;发布</p>
<p><em>邮箱不能注册过其他组件(公众号,企业号等)</em></p>
<ul>
<li>只有管理员才能上传代码</li>
<li>公测期间不支持发布</li>
<li>不能做营销和游戏</li>
<li>不做收集用户隐私的接口</li>
<li>便捷:尽量减少输入,避免误操作</li>
<li>统一:视觉统一,WeUI(可以根据它来调整,由自己的品牌来定制)</li>
<li>审核规范:判断原则<ul>
<li>真实性,一致性(信息一一对应),稳定性,合法性</li>
<li>常见拒绝原因:demo版本或trial版本,或仍存在bug<ul>
<li>提供的服务属于暂未开放的服务范围</li>
<li>名称,简介,logo,服务范围没有对应关系</li>
</ul>
</li>
</ul>
</li>
</ul>
<hr>
<ul>
<li>单页面应用 Single Page Application</li>
<li>动态加载</li>
<li>Framework Overview</li>
<li>10M稳定存储能力,</li>
<li>jssdk -&gt;小程序的API</li>
<li>API 拥有调取Native 系统层能力</li>
<li>Page Frame<ul>
<li>预先加载一个web view</li>
<li>当打开指定页面时,无需请求额外资源,直接渲染</li>
<li>init–&gt;ready–&gt;nav–&gt;ready-&gt;(从右边)push(100-200ms)(开发者最好自己也做一些点击态)</li>
</ul>
</li>
</ul>
<hr>
<ul>
<li>App Service -Manager<ul>
<li>App() 小程序入口</li>
<li>Page() 页面入口</li>
</ul>
</li>
<li>App Service -Life Cycle<ul>
<li>Native加载两个线程 (view, App Service)</li>
<li>redirect (越过5层页面层级限制)</li>
</ul>
</li>
<li>App Service -API<ul>
<li>通过JSbridge同客户端通信</li>
<li>用户信息,微信支付,模版消息,网络通信,本地存储等等</li>
</ul>
</li>
<li>View - Hybrid 混合应用<ul>
<li>凌驾于web view</li>
<li>web-like 开发方式</li>
<li>App-like 用户体验</li>
</ul>
</li>
<li>View -WXML<ul>
<li>支持逻辑,算数计算</li>
<li>支持模版,引用</li>
<li>(WXML)compiler–&gt;(JS:generateFunc)date–&gt;(Virtual Tree)Virtual DOM–&gt;(View DOM)</li>
</ul>
</li>
<li>View -WXSS<ul>
<li>自适应单位RPX- Responsive Pixel</li>
<li>无级联(但是其实还是可以用的用)-避免被组件内结构破坏(最好别用)</li>
<li>(WXSS)Compiler–&gt;(JS)Width,DPR–&gt;(CSS)</li>
</ul>
</li>
<li>View -Render<ul>
<li>数据M 和视图V 完全分离</li>
<li>逻辑层运行在独立的环境中,无法直接操作DOM</li>
<li>使用WXML模版语言减少维护成本</li>
<li>单项数据绑定</li>
</ul>
</li>
</ul>
<h3 id="Virtual-DOM"><a href="#Virtual-DOM" class="headerlink" title="Virtual-DOM"></a>Virtual-DOM</h3><ul>
<li>生成和DOM tree – 对应的Virtual Tree</li>
<li>对比新Tree和旧Tree的不同</li>
<li>将这些不同的点应用到DOM Tree中</li>
<li>动态改变的节点需要加上WX:key 来做节点的唯一标示</li>
<li>wx:if,wx:for,template增加一个&lt;virtual/&gt;的节点</li>
<li>diff</li>
<li>native 凌驾于webview 有些webview层中的一些事件(比如滑动)不能在原生环境中监听</li>
</ul>
<hr>
<h2 id="技术人员分享"><a href="#技术人员分享" class="headerlink" title="技术人员分享"></a>技术人员分享</h2><ul>
<li>单向数据流 </li>
<li>局部化组件</li>
<li>不携带cookie</li>
<li>globalData事件订阅</li>
<li>页面传值</li>
<li>server层</li>
<li>1M限制(用图片替代代码来越过限制,不推荐)</li>
<li>react与小程序语法替换关系<ul>
<li>state ==&gt; data</li>
<li>setState ==&gt; setData</li>
<li>array.map ==&gt; war.map</li>
</ul>
</li>
</ul>
]]></content>
      
        <categories>
            
            <category> Document </category>
            
        </categories>
        
        
    </entry>
    
    <entry>
      <title><![CDATA[weixin]]></title>
      <url>/Document/weixin.html</url>
      <content type="html"><![CDATA[<p>微信小程序中的每一个页面的【路径+页面名】都需要写在 app.json 的 pages 中，且 pages 中的第一个页面是小程序的首页。</p>
<p>通过全局函数 getApp() 可以获取全局的应用实例，如果需要全局的数据可以在 App() 中设置</p>
<p><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">对象</div><div class="line">&lt;template is=&quot;objectCombine&quot; data=&quot;&#123;&#123;for: a, bar: b&#125;&#125;&quot;&gt;&lt;/template&gt;</div><div class="line">Page(&#123;</div><div class="line">  data: &#123;</div><div class="line">    a: 1,</div><div class="line">    b: 2</div><div class="line">  &#125;</div><div class="line">&#125;)</div><div class="line">最终组合成的对象是 &#123;for: 1, bar: 2&#125;</div><div class="line"></div><div class="line">也可以用扩展运算符 ... 来将一个对象展开</div><div class="line"></div><div class="line">&lt;template is=&quot;objectCombine&quot; data=&quot;&#123;&#123;...obj1, ...obj2, e: 5&#125;&#125;&quot;&gt;&lt;/template&gt;</div><div class="line">Page(&#123;</div><div class="line">  data: &#123;</div><div class="line">    obj1: &#123;</div><div class="line">      a: 1,</div><div class="line">      b: 2</div><div class="line">    &#125;,</div><div class="line">    obj2: &#123;</div><div class="line">      c: 3,</div><div class="line">      d: 4</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line">&#125;)</div></pre></td></tr></table></figure></p>
]]></content>
      
        <categories>
            
            <category> Document </category>
            
        </categories>
        
        
    </entry>
    
    <entry>
      <title><![CDATA[Dockerfile]]></title>
      <url>/Document/Dockerfile.html</url>
      <content type="html"><![CDATA[<h1 id="Dockerfile"><a href="#Dockerfile" class="headerlink" title="Dockerfile"></a>Dockerfile</h1><p>使用Dockerfile可以允许用户创建自定义镜像</p>
<h2 id="基本结构"><a href="#基本结构" class="headerlink" title="基本结构"></a>基本结构</h2><p>Dockerfile 由一行行命令语句组成,并支持以#开头的注释行.<br>一般的，Dockerfile 分为四部分：基础镜像信息、维护者信息、镜像操作指令和容<br>器启动时执行指令:</p>
<p>`   </p>
<pre><code># This dockerfile uses the ubuntu image
# VERSION 2 - EDITION 1
# Author: docker_user
# Command format: Instruction [arguments / command] ..
# Base image to use, this must be set as the first line FROM ubuntu
# Maintainer: docker_user &lt;docker_user at email.com&gt; (@docker_user)
MAINTAINER docker_user docker_user@email.com
# Commands to update the image
RUN echo &quot;deb http://archive.ubuntu.com/ubuntu/ raring main universe&quot; &gt;&gt; /etc/apt/sources.list
RUN apt-get update &amp;&amp; apt-get install -y nginx
RUN echo &quot;\ndaemon off;&quot; &gt;&gt; /etc/nginx/nginx.conf
# Commands when creating a new container
CMD /usr/sbin/nginx

</code></pre><p>`</p>
<p>其中，一开始必须指明所基于的镜像名称，接下来推荐说明维护者信息。<br>后面则是镜像操作指令，例如 RUN 指令， RUN 指令将对镜像执行跟随的命令。<br>每运行一条 RUN 指令，镜像添加新的一层，并提交。<br>最后是 CMD 指令，来指定运行容器时的操作命令。<br>下面是一个更复杂的例子</p>
<p>`</p>
<pre><code># Nginx
#
# VERSION 0.0.1
FROM ubuntu
MAINTAINER Victor Vieux &lt;victor@docker.com&gt;
RUN apt-get update &amp;&amp; apt-get install -y inotify-tools nginx apache2 openssh-server
# Firefox over VNC
#
# VERSION 0.3
FROM ubuntu
# Install vnc, xvfb in order to create a &apos;fake&apos; display and firefox
RUN apt-get update &amp;&amp; apt-get install -y x11vnc xvfb firefox
RUN mkdir /.vnc
# Setup a password
RUN x11vnc -storepasswd 1234 ~/.vnc/passwd
# Autostart firefox (might not be the best way, but it does the trick)
RUN bash -c &apos;echo &quot;firefox&quot; &gt;&gt; /.bashrc&apos;
EXPOSE 5900
CMD [&quot;x11vnc&quot;, &quot;-forever&quot;, &quot;-usepw&quot;, &quot;-create&quot;]
# Multiple images example
#
# VERSION 0.1
FROM ubuntu
RUN echo foo &gt; bar
# Will output something like ===&gt; 907ad6c2736f
FROM ubuntu
RUN echo moo &gt; oink
# Will output something like ===&gt; 695d7793cbe4
# You᾿ll now have two images, 907ad6c2736f with /bar, and 695d7793cbe4 with
# /oink.

</code></pre><p>`</p>
]]></content>
      
        <categories>
            
            <category> Document </category>
            
        </categories>
        
        
    </entry>
    
    <entry>
      <title><![CDATA[Docker入门到实践]]></title>
      <url>/Document/Docker%E5%85%A5%E9%97%A8%E5%88%B0%E5%AE%9E%E8%B7%B5.html</url>
      <content type="html"><![CDATA[<h1 id="Docker–入门到实践"><a href="#Docker–入门到实践" class="headerlink" title="Docker–入门到实践"></a>Docker–入门到实践</h1><p>Virtuial Machines<br>each virtualized application includes not nonly the application - which may be only 10s of MB -and the necessary binaries,but also an entire guest operating system - which may weigh 10s of GB</p>
<hr>
<p>Docker<br>The Docker Engine container comprises just the application and its dependencies. it runs aas an isolated process in userspace on the host operating system,sharing the kernel with other containers. Thus,it enjoys the resource and allocation benefits of VMs but is much more portable and efficient.</p>
<hr>
<p>容器除了运行其中的应用外,基本不消耗额外的系统资源,使得应用性能很高,同事系统的开销尽量小,相比传统虚拟机方式运行10个不同的应用要起10个虚拟机,而Docker只需要启动10个隔离应用即可.</p>
<table>
<thead>
<tr>
<th style="text-align:left">特性</th>
<th style="text-align:left">容器</th>
<th style="text-align:left">虚拟机</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">启动</td>
<td style="text-align:left">秒级</td>
<td style="text-align:left">分钟级</td>
</tr>
<tr>
<td style="text-align:left">硬盘使用</td>
<td style="text-align:left">一般为MB</td>
<td style="text-align:left">一般为GB</td>
</tr>
<tr>
<td style="text-align:left">性能</td>
<td style="text-align:left">接近原生</td>
<td style="text-align:left">弱于</td>
</tr>
<tr>
<td style="text-align:left">系统支持量</td>
<td style="text-align:left">单机支持上千个容器</td>
<td style="text-align:left">一般几十个</td>
</tr>
</tbody>
</table>
<hr>
<h2 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h2><ul>
<li>镜像(Image)</li>
</ul>
<p>Docker镜像就是一个只读的模版,如可以包含一个完整的操作系统里面仅安装了Apache<br>镜像可以用来创建Docker容器<br>Docker提供了一个很简单的机制来创建镜像或者更新现有镜像,或者拿一个做好的镜像直接使用</p>
<hr>
<ul>
<li>容器(Container)</li>
</ul>
<p>Docker利用容器俩运行运用<br>容器是从镜像创建的运行实例.可以被启动,开始停止,删除.每个容器容器都是相互隔离的,保证安全的平台<br>可以把容器看作是一个简易版的Linux环境 (包括root用户权限,进程空间,用户空间和网络空间等)和运行在其中的应用程序</p>
<p>// 镜像是只读的,容器在启动的时候创建一层<code>可写层</code>作为最上层</p>
<hr>
<ul>
<li>仓库(Repository)</li>
</ul>
<p>仓库是几种存放镜像文件的场所.仓库注册服务器上往往存放多个仓库,每个仓库中又包含多个镜像,每个镜像有不同的标签(tag)</p>
<p>仓库分为公开仓库(Public)和私有仓库(Private)</p>
<p>最大的公开仓库是Docker Hub,国内公开仓库有Docker Pool</p>
<p>用户可以本地网络内创建一个私有仓库,当创建了自己的镜像后可使用<code>push</code>命令上传到仓库,命令<code>pull</code>从仓库下载</p>
<hr>
<h2 id="CentOS-系列安装Docker"><a href="#CentOS-系列安装Docker" class="headerlink" title="CentOS 系列安装Docker"></a>CentOS 系列安装Docker</h2><p><code>docker pull hub.c.163.com/public/&lt;centos:7 class=&quot;0&quot;&gt;&lt;/centos:7&gt;</code></p>
<p><code>docker run -t -i hub.c.163.com/public/centos:7.0 /bin/bash</code><br>-t  选项让Docker分配一个伪终端（pseudo-tty）并绑定到容器的标准输入上，  -i  则让容器的标准输入保持打开</p>
<p><code>docker run -t -i training/sinatra /bin/bash</code>  //会下载一个新的…镜像</p>
<p>当利用<code>docker run</code> 创建容器时,Docker在后台运行的标准是:</p>
<ul>
<li>检查本地是否存在指定的镜像,不存在就从公有仓库下载</li>
<li>利用镜像创建并启动一个容器</li>
<li>分配一个文件系统,并在只读的镜像层外面挂载一层可读写成</li>
<li>从宿主主机配置的网桥接口中桥接一个虚拟接口道容器中去</li>
<li>从地址池配置一个IP地址给容器</li>
<li>执行用户指定的应用程序</li>
</ul>
<p><code>docker commit -m &quot;Added git&quot; -a &quot;Docker Newbee&quot; 40c9fe0a35ef centos:v2</code></p>
<hr>
<p><code>docker run -t -i hub.c.163.com/public/centos:7.0 /bin/bash</code><br><code>-t 让Docker分配一个伪终端(pseudo-tty)并绑定到容器的标准输入上,-i 让容器的标准输入保持打开</code></p>
<p>当利用docker run 创建容器时,Docker在后台运行的标准操作包括:</p>
<ul>
<li>检查本地是否存在指定的镜像,不存在就从公有仓库下载</li>
<li>利用镜像创建并启动一个容器</li>
<li>分配一个文件系统,并在只读的镜像层外面挂载一层可读写层</li>
<li>从宿主主机配置的网桥接口中桥接一个虚拟接口道容器中去</li>
<li>从地址池配置一个ip地址给容器</li>
<li>执行用户指定的应用程序</li>
<li>执行完毕后容器被终止</li>
</ul>
<h4 id="启动已终止容器"><a href="#启动已终止容器" class="headerlink" title="启动已终止容器"></a>启动已终止容器</h4><p>利用 <code>docker start</code> ,直接将一个已经终止的容器启动运行<br>容器的核心为所执行的应用程序,所需要的资源都是应用程序运行必需的.</p>
<h4 id="后台background运行"><a href="#后台background运行" class="headerlink" title="后台background运行"></a>后台background运行</h4><p>需要让docker在后台运行</p>
<hr>
<p>安装 `.bashrc_docker</p>
<p><figure class="highlight bash"><table><tr><td class="code"><pre><div class="line">wget -P ~ https://github.com/yeasy/docker_practice/raw/master/_local/.bashrc_docker</div><div class="line"><span class="built_in">echo</span> <span class="string">"[ -f ~/.bashrc_docker ] &amp;&amp; . ~/.bashrc_docker"</span> &gt;&gt; ~/.bashrc; <span class="built_in">source</span> ~/.bashrc</div><div class="line"></div><div class="line"><span class="built_in">echo</span> $(docker-pid docker)</div></pre></td></tr></table></figure>
</p>
<hr>
<h2 id="redis-master"><a href="#redis-master" class="headerlink" title="redis-master"></a>redis-master</h2><p><code>docker run --name redis-master -v /home/testuser/docker/master-redis.conf:/usr/local/etc/master-redis.conf -d -p 16397:16379 redis redis-server /usr/local/etc/master-redis.conf</code></p>
<h2 id="redis-slave"><a href="#redis-slave" class="headerlink" title="redis-slave"></a>redis-slave</h2><p><code>docker run --link redis-master:redis-master -v /home/testuser/docker/slave-redis.conf:/usr/local/etc/slave-redis.conf --name redis-slave1 -d redis redis-server /usr/local/etc/slave-redis.conf</code></p>
<h4 id="宿主机上如何获得-docker-container-容器的-ip-地址？"><a href="#宿主机上如何获得-docker-container-容器的-ip-地址？" class="headerlink" title="宿主机上如何获得 docker container 容器的 ip 地址？"></a>宿主机上如何获得 docker container 容器的 ip 地址？</h4><p><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">docker inspect --format=&apos;&#123;&#123;.NetworkSettings.IPAddress&#125;&#125;&apos; $CONTAINER_ID</div></pre></td></tr></table></figure>
</p>
<h3 id="在Docker容器内外互相拷贝数据"><a href="#在Docker容器内外互相拷贝数据" class="headerlink" title="在Docker容器内外互相拷贝数据"></a>在Docker容器内外互相拷贝数据</h3><h4 id="从容器内拷贝文件到主机上"><a href="#从容器内拷贝文件到主机上" class="headerlink" title="从容器内拷贝文件到主机上"></a>从容器内拷贝文件到主机上</h4><p><code>docker cp &lt;containerId&gt;:/file/path/within/container /host/path/target</code></p>
<p>没试验过</p>
<h4 id="从主机上拷贝文件到容器内"><a href="#从主机上拷贝文件到容器内" class="headerlink" title="从主机上拷贝文件到容器内"></a>从主机上拷贝文件到容器内</h4><ol>
<li><p>用-v挂载主机数据卷到容器内<br><code>docker run -v /path/to/hostdir:/mnt $container</code><br>在容器内拷贝<br><code>cp /mnt/sourcefile /path/to/destfile</code></p>
</li>
<li><p>直接在主机上拷贝到容器物理存储系统</p>
<ol>
<li>获取容器名称或者id : <code>docker ps</code></li>
<li><p>获取整个容器的id: </p>
<p><figure class="highlight bash"><table><tr><td class="code"><pre><div class="line">docker inspect -f   <span class="string">'&#123;&#123;.Id&#125;&#125;'</span>  [获取的名称或者id]</div></pre></td></tr></table></figure>
</p>
</li>
<li><p>在主机上拷贝文件: <code>cp path-file-host /var/lib/docker/devicemapper/mnt/[获取的名称或者id]/rootfs/root</code></p>
</li>
</ol>
</li>
</ol>
<p><figure class="highlight bash"><table><tr><td class="code"><pre><div class="line"></div><div class="line">1.</div><div class="line">[testuser@CentOS1 docker]<span class="variable">$docker</span> ps</div><div class="line">CONTAINER ID        IMAGE               COMMAND                CREATED             STATUS              PORTS                                NAMES</div><div class="line">01a349ea86bd        redis               <span class="string">"docker-entrypoint.s   42 minutes ago      Up 42 minutes       6379/tcp                             redis-slave3        </span></div><div class="line">bd45650a61f6        redis               "docker-entrypoint.s   43 minutes ago      Up 43 minutes       6379/tcp                             redis-slave2        </div><div class="line">ce2321f0d4d5        redis               <span class="string">"docker-entrypoint.s   46 minutes ago      Up 46 minutes       6379/tcp                             redis-slave1        </span></div><div class="line">9408dd71f9e9        redis               "docker-entrypoint.s   54 minutes ago      Up 54 minutes       6379/tcp, 0.0.0.0:16397-&gt;16379/tcp   redis-master </div><div class="line"></div><div class="line"></div><div class="line">2. </div><div class="line">[testuser@CentOS1 docker]$ docker inspect -f <span class="string">'&#123;&#123;.Id&#125;&#125;'</span> 9408dd71f9e9</div><div class="line">9408dd71f9e943b6ed08dd146e20700439eef280db7ce914852452340bfbfbbe</div><div class="line"></div><div class="line"></div><div class="line">3.</div><div class="line">[root@CentOS1 ~]<span class="comment">#cp sentinel.conf /var/lib/docker/devicemapper/mnt/9408dd71f9e943b6ed08dd146e20700439eef280db7ce914852452340bfbfbbe/rootfs/root/</span></div></pre></td></tr></table></figure>
</p>
<h4 id="退出容器而不停止容器"><a href="#退出容器而不停止容器" class="headerlink" title="退出容器而不停止容器"></a>退出容器而不停止容器</h4><p>组合键：Ctrl+P+Q</p>
]]></content>
      
        <categories>
            
            <category> Document </category>
            
        </categories>
        
        
    </entry>
    
    <entry>
      <title><![CDATA[mongod error]]></title>
      <url>/Document/mongod%20error.html</url>
      <content type="html"><![CDATA[<p>mongod start<br><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line"></div><div class="line">about to fork child process, waiting until server is ready for connections.</div><div class="line">forked process: 23987</div><div class="line">ERROR: child process failed, exited with error number 100</div></pre></td></tr></table></figure></p>
<p>没有正常关闭mongod服务,导致mongod被锁.删除掉mongod里的mongod.lock 文件.重新启动服务就行了.</p>
<hr>
<p>/sys/kernel/mm/transparent_hugepage/defrag is ‘always’.</p>
<p><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line"></div><div class="line">if test -f /sys/kernel/mm/transparent_hugepage/enabled; then  </div><div class="line">   echo never &gt; /sys/kernel/mm/transparent_hugepage/enabled  </div><div class="line">fi  </div><div class="line">if test -f /sys/kernel/mm/transparent_hugepage/defrag; then  </div><div class="line">   echo never &gt; /sys/kernel/mm/transparent_hugepage/defrag  </div><div class="line">fi</div></pre></td></tr></table></figure>
</p>
<hr>
<p><code>** WARNING: soft rlimits too low. rlimits set to 1024 processes, 100032 files. Number of processes should be at least 50016 : 0.5 times number of files</code></p>
<ol>
<li><p>查看当前mongodb进程信息</p>
<p><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">ps -ef | grep mongod</div><div class="line">testuser 24763  3.5  0.4 394264 35708 ?        Sl   09:56   0:00 mongod --config /etc/mongo/mongodb.conf</div></pre></td></tr></table></figure>
</p>
</li>
<li><p>cat /proc/24763/limits </p>
<p><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">Limit                     Soft Limit           Hard Limit           Units     </div><div class="line">Max cpu time              unlimited            unlimited            seconds   </div><div class="line">Max file size             unlimited            unlimited            bytes     </div><div class="line">Max data size             unlimited            unlimited            bytes     </div><div class="line">Max stack size            10485760             unlimited            bytes     </div><div class="line">Max core file size        0                    unlimited            bytes     </div><div class="line">Max resident set          unlimited            unlimited            bytes     </div><div class="line">Max processes             1024                 30921                processes </div><div class="line">Max open files            100032               100032               files     </div><div class="line">Max locked memory         65536                65536                bytes     </div><div class="line">Max address space         unlimited            unlimited            bytes     </div><div class="line">Max file locks            unlimited            unlimited            locks     </div><div class="line">Max pending signals       30921                30921                signals   </div><div class="line">Max msgqueue size         819200               819200               bytes     </div><div class="line">Max nice priority         0                    0                    </div><div class="line">Max realtime priority     0                    0                    </div><div class="line">Max realtime timeout      unlimited            unlimited            us</div></pre></td></tr></table></figure>
</p>
</li>
</ol>
<p><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line"> $ mongod --oplogSize 40 --port 20000 --noprealloc --smallfiles --replSet testReplSet --dbpath /data/db/testReplSet-0 --setParameter enalbeTestCommands=1</div><div class="line"></div><div class="line">    BadValue: Illegal --setParameter parameter: &quot;enalbeTestCommands&quot;</div><div class="line"> </div><div class="line">    try &apos;mongod --help&apos; for more information</div><div class="line"></div><div class="line">$ mongod --oplogSize 40 --port 20000 --noprealloc --smallfiles --replSet testReplSet --dbpath /data/db/testReplSet-0 --setParameter enableTestCommands=1</div><div class="line"></div><div class="line">rs.help()</div><div class="line"> rs.conf()</div><div class="line"> rs.initiate()</div><div class="line">rs.conf()</div><div class="line">rs.help()</div><div class="line">rs.add(&quot;CentOS2:27017&quot;)</div><div class="line">rs.add(&quot;CentOS3:27017&quot;)</div><div class="line">rs.conf()</div><div class="line">rs.status()</div><div class="line"></div><div class="line"></div><div class="line">port=27017</div><div class="line">bing_ip=</div><div class="line">logpath=</div><div class="line">dbpath=</div><div class="line">logappend=true</div><div class="line">pidfilepath=/usr/local/mongodb/data/</div><div class="line">fork=true</div><div class="line">oplogSize=1024</div><div class="line">replSet=rs1</div></pre></td></tr></table></figure></p>
]]></content>
      
        <categories>
            
            <category> Document </category>
            
        </categories>
        
        
    </entry>
    
    <entry>
      <title><![CDATA[Dockerfile 实例]]></title>
      <url>/Document/Dockerfile%20%E5%AE%9E%E4%BE%8B.html</url>
      <content type="html"><![CDATA[<h1 id="Dockerfile-实例"><a href="#Dockerfile-实例" class="headerlink" title="Dockerfile 实例"></a>Dockerfile 实例</h1><p>apt-get update<br>&lt;!-- apt-get install openssh-server --&gt;<br>apt-get install -y openssh-server apache2 supervisor vim</p>
<hr>
<p>`   </p>
<pre><code>; supervisor config file

[unix_http_server]
file=/var/run/supervisor.sock   ; (the path to the socket file)
chmod=0700                       ; sockef file mode (default 0700)

[supervisord]
logfile=/var/log/supervisor/supervisord.log ; (main log file;default $CWD/supervisord.log)
pidfile=/var/run/supervisord.pid ; (supervisord pidfile;default supervisord.pid)
childlogdir=/var/log/supervisor            ; (&apos;AUTO&apos; child log dir, default $TEMP)

; the below section must remain in the config file for RPC
; (supervisorctl/web interface) to work, additional interfaces may be
; added by defining them in separate rpcinterface: sections
[rpcinterface:supervisor]
supervisor.rpcinterface_factory = supervisor.rpcinterface:make_main_rpcinterface

[supervisorctl]
serverurl=unix:///var/run/supervisor.sock ; use a unix:// URL  for a unix socket

; The [include] section can just contain the &quot;files&quot; setting.  This
; setting can list multiple files (separated by whitespace or
; newlines).  It can also contain wildcards.  The filenames are
; interpreted as relative to this file.  Included files *cannot*
; include files themselves.

[include]
files = /etc/supervisor/conf.d/*.conf


</code></pre><p>`</p>
<hr>
<p>`   </p>
<pre><code>[supervisord]
nodaemon=true
[program:sshd]
command=/usr/sbin/sshd -D
[program:apache2]
command=/bin/bash -c &quot;source /etc/apache2/envvars &amp;&amp; exec /usr/sbin/apache2 -DFOREGROUND&quot;

</code></pre><p>`</p>
]]></content>
      
        <categories>
            
            <category> Document </category>
            
        </categories>
        
        
    </entry>
    
    <entry>
      <title><![CDATA[ubuntu的网络配置]]></title>
      <url>/Document/ubuntu%E7%9A%84%E7%BD%91%E7%BB%9C%E9%85%8D%E7%BD%AE.html</url>
      <content type="html"><![CDATA[<h1 id="ubuntu的网络配置"><a href="#ubuntu的网络配置" class="headerlink" title="ubuntu的网络配置"></a>ubuntu的网络配置</h1><h2 id="有线连接互联网"><a href="#有线连接互联网" class="headerlink" title="有线连接互联网"></a>有线连接互联网</h2><ol>
<li>Ubuntu使用两条配置线路   /etc/network/interfaces这个配置文件主要用于服务器版本的Ubuntu系统使用;<br>当/etc/NetworkManager/NetworkManager.conf  中managed=false ，以interfaces文件中的配置为准， </li>
<li>为了适应移动办公造成ip和网络环境不断变化,上网配置在/networkManager/NetworkManager.conf<br>/etc/NetworkManager/NetworkManager.conf  中managed=true ，以本配置为准。</li>
</ol>
<p><code>sudo ifconfig</code> #示所有网卡的接口信息  如果你看到 eth0  —- 有线网卡 wlan0 — 表示 无线网卡 这样几个模块说明你的网卡已经安装好了。(要用lspci -vnn -d 14e4 :查看网卡信息 ，网上下载合适的驱动))</p>
<p>sudo pppoeconf   # 调出 pppoe有线拨号上网的配置界面，按提示进行配置<br>sudo pon dsl-provider    # 建立连接<br>sudo poff        # 终止连接<br>sudo vim /etc/network/interfaces<br>修改interfaces 文件如下：让系统开机时自己连接上有线网络</p>
<p>`</p>
<pre><code>#interfaces(5) file used by ifup(8) and ifdown(8)
auto lo
iface lo inet loopback

auto dsl-provider
iface dsl-provider inet ppp
pre-up /sbin/ifconfig eth0 up # line maintained by pppoeconf
provider dsl-provider
auto eth0
iface eth0 inet manual

</code></pre><p>`</p>
<p>sudo etc/init.d/networking restart  ##配置完毕，终端命令重启网络配置</p>
<hr>
<p>在调试过程中 常用的网络调试命令是<br>sudo stop network-manager   #禁用和启用网络管理面板<br>sudo start network-manager<br>sudo etc/init.d/networking restart   #重启网络配置<br>sudo ifconfig eth0 down<br>sudo ifconfig eth0 up    #禁用和启用有线网卡</p>
<hr>
<h2 id="无线连接互联网"><a href="#无线连接互联网" class="headerlink" title="无线连接互联网"></a>无线连接互联网</h2><p>iwconfig  wlan0   #显示无线网卡联网情况<br>iwlist wlan0 scan    #扫描无线网络<br>sudo iwconfig wlan0 essid 【account】  key  【password】</p>
<p>dhclient wlan0   #自动获取ip地址<br>ifconfig wlan0 down  #关闭网卡<br>ifconfig wlan0 up    #启动网卡<br>ping  192.168.1.1  # 拼网关，检测是否已能联上网</p>
<h2 id="wifi故障情况分析"><a href="#wifi故障情况分析" class="headerlink" title="wifi故障情况分析"></a>wifi故障情况分析</h2><p>情况是网速非常慢，ping 网关速度慢，ping外网没网络。<br>终端执行如下命令：<br>lspci -vnn -d 14e4  查看网卡型号，和现用驱动<br>比对下表查看自己的驱动是否安装正确<br><a href="http://askubuntu.com/questions/55868/installing-broadcom-wireless-drivers?lq=1" target="_blank" rel="external">http://askubuntu.com/questions/55868/installing-broadcom-wireless-drivers?lq=1</a> 的<br>BROADCOM WIRELESS TABLE (Updated 31 March 2014) 表</p>
<hr>
<p>sudo apt-update<br>sudo apt-get remove –purge bcmwl-kernel-source   #卸载当前驱动 bcmwl-kernel-source<br>sudo modprobe -r b43 ssb wl brcmfmac  # 移除你看到的驱动模块 b43, ssb ….. 是 驱动模块名<br>sudo apt-get reinstall install bcmwl-kernel-source     #安装新驱动，可以到ubuntu的软件中心安装<br>sudo modprobe wl 加载新驱动模块到linux内核</p>
<p>到/etc/modprobe.d/中去删除 冗余的 .conf 配置，并检查文件 blacklist 中是否把新装的驱动在屏蔽范围内，如有则解除屏蔽。<br>sudo rm /etc/modprobe.d/blacklist-bcm43.conf<br>sudo rm /etc/modprobe.d/broadcom-sta-common.conf<br>sudo rm /etc/modprobe.d/broadcom-sta-dkms.conf<br> sudo sed -i “s/blacklist b43/#blacklist b43/g” $(egrep -lo ‘blacklist b43’ /etc/modprobe.d/<em>)<br>sudo sed -i “s/blacklist ssb/#blacklist ssb/g” $(egrep -lo ‘blacklist ssb’ /etc/modprobe.d/</em>)<br>sudo sed -i “s/blacklist bcma/#blacklist bcma/g” $(egrep -lo ‘blacklist bcma’ /etc/modprobe.d/*)</p>
<p>网卡的驱动属于外设驱动，可到 System &gt; Administration &gt; Hardware/Additional Drivers<br>查看网卡状态，ubuntu 14.04，是在  系统设置 &gt; 软件和更新 &gt; 附加驱动 查看</p>
]]></content>
      
        <categories>
            
            <category> Document </category>
            
        </categories>
        
        
    </entry>
    
    <entry>
      <title><![CDATA[Redis]]></title>
      <url>/Redis/Redis.html</url>
      <content type="html"><![CDATA[<h3 id="名字引用规则"><a href="#名字引用规则" class="headerlink" title="名字引用规则"></a>名字引用规则</h3><pre><code>在第一次应用Redis源代码文件file中的名字name时,本书使用file/name格式,比如redis.c/main 表示redis.c文件中的main函数,而redis.h/redisDb则表示redis.h文件中的redisDb结构,

另外,在第一次应用标准库头文件file中的名字name时,本书使用&lt;file&gt;/name格式,比如&lt;unistd.h&gt;/write表示unistd.h头文件的write函数,而&lt;sudio.h&gt;/printf则表示stdio.h头文件的write函数,而&lt;stdio.h&gt;/printf则表示stdio.h头文件的printf函数,

在第一次引用某个名字之后,本书就回去掉名字前缀的文件名,直接使用名字本身.举个例子.当第一次引用redis.h文件的redisDb结构的时候,会使用redis.h/redisDb格式,而之后再次引用redisDb结构时,只使用redisDb.


</code></pre><hr>
<h3 id="结构引用规则"><a href="#结构引用规则" class="headerlink" title="结构引用规则"></a>结构引用规则</h3><pre><code>本书使用struct.property格式来引用struct结构的property属性,比如redisDb.id表示redisDb结构的id属性,而redisDb.expires则表示redisDb结构的expires属性


</code></pre><hr>
<h3 id="Cluster群集一般来讲有四个功能"><a href="#Cluster群集一般来讲有四个功能" class="headerlink" title="Cluster群集一般来讲有四个功能"></a>Cluster群集一般来讲有四个功能</h3><ol>
<li>冗余功能,就是说在这个群集中的任何一台机器出现本机或网络故障时，整个网络仍不中断，对外的服务也不中断，网络有多个路线可以走,服务器也可以相互代替。<br>2.负载均衡，就是当网络流量很大时。少量的服务器可能响应不过来，这时需要多台服务器为其分担处理。打个比方：一台IA服务器的处理能力是每秒几万个，显然无法在一秒钟内处理几十万个请求，但如果我们能够有10台这样的服务器组成一个系统，如果有办法将所有的请求平均分配到所有的服务器，那么这个系统就拥有了每秒处理几十万个请求的能力。</li>
<li>协商处理，这种方案的原理是客户请求会同时被所有的节点所接收，然后所有节点按照一定的规则协商决定由哪个节点处理这个请求。此种方案中比较显著的特点就是整个集群中没有显著的管理节点，所有决定由全体工作节点共同协商作出。<br>4。流量分发，原理是所有的用户请求首先到达集群的管理节点，管理节点可以根据所有服务节点的处理能力和现状来决定将这个请求分发给某个服务节点。当某个服务节点由于硬件或软件原因故障时，管理节点能够自动检测到并停止向这个服务节点分发流量。这样，既通过将流量分担而增加了整个系统的性能和处理能力，又可以很好的提高系统的可用性。</li>
</ol>
]]></content>
      
        <categories>
            
            <category> redis </category>
            
        </categories>
        
        
    </entry>
    
    <entry>
      <title><![CDATA[Linux Performance Observability Tools]]></title>
      <url>/Linux/Linux%20Performance%20Observability%20Tools.html</url>
      <content type="html"><![CDATA[<h2 id="agenda"><a href="#agenda" class="headerlink" title="agenda"></a>agenda</h2><p>a brief discussion of 6 facets of Linux performance:</p>
<ol>
<li>observalility</li>
<li>Methodologies</li>
<li>benchmarking</li>
<li>profiling</li>
<li>tracing</li>
<li>tuning</li>
</ol>
<hr>
<h3 id="observability-tools"><a href="#observability-tools" class="headerlink" title="observability tools"></a>observability tools</h3><ul>
<li>Tools showcase common metrics<ul>
<li>lenrning linux tools is useful even if you never use them:<br>the same metrics are in GUIs</li>
</ul>
</li>
<li>we usually use these metrics via:<ul>
<li>netflix atlas: cloud-wide monitoring</li>
<li>netflix vector: instance analysis</li>
</ul>
</li>
<li>Linux has many tools<ul>
<li>plus many extra kernel sourcse of data that lack tools,are harder to use ,and are practically undocumented</li>
</ul>
</li>
</ul>
<hr>
<h4 id="uptime"><a href="#uptime" class="headerlink" title="uptime"></a>uptime</h4><ul>
<li>one way to print load averages:</li>
<li>a measure of resource demand: CPUs + disk<ul>
<li>other OSes only show CPUs: easier to interpret</li>
</ul>
</li>
<li>exponentially-damped moving averages</li>
<li>time constants of 1,5,and 15 minutes<ul>
<li>historic trend without the line graph</li>
</ul>
</li>
<li>Load &gt; # of CPUs,may mean CPU saturation<ul>
<li>Don’t spent more than 5 seconds studying these </li>
</ul>
</li>
</ul>
<hr>
<h4 id="top-or-htop"><a href="#top-or-htop" class="headerlink" title="top (or htop)"></a>top (or htop)</h4><ul>
<li>system and per-process interval summary:</li>
<li>%CPU is summed across all CPUs</li>
<li>Can miss short-lives processes(atop won’t)</li>
<li>Can consume noticeable CPU to read/proc</li>
</ul>
<hr>
<h4 id="vmstat"><a href="#vmstat" class="headerlink" title="vmstat"></a>vmstat</h4><ul>
<li>Virtual memory statistics and more:</li>
<li>USAGE:vmstat [interval[count]]</li>
<li>First output line has some summary since boot values <ul>
<li>Should be all;partial is confusing</li>
</ul>
</li>
<li>High level CPU summary<ul>
<li>“r” is runnable tasks</li>
</ul>
</li>
</ul>
<hr>
<h4 id="iostat"><a href="#iostat" class="headerlink" title="iostat"></a>iostat</h4><ul>
<li>Block I/O (disk)stats.1st output is since boot.</li>
<li>very useful set of stats</li>
</ul>
<hr>
<h4 id="free"><a href="#free" class="headerlink" title="free"></a>free</h4><ul>
<li>Main memory usage:</li>
<li>buffers: block device I/O cache</li>
<li>cached: virtua; page cache</li>
</ul>
<hr>
<h4 id="strace"><a href="#strace" class="headerlink" title="strace"></a>strace</h4><ul>
<li>System call tracer:</li>
<li>Eg, -ttt:time (us) since epoch;-T:syscall time(s)</li>
<li>Translates syscall arges<ul>
<li>very helpful for solving system usage issues</li>
</ul>
</li>
<li>Currently has massive overhead(ptrace based)<ul>
<li>can slow the target by &gt; 100x.Use extreme caution.</li>
</ul>
</li>
</ul>
<hr>
<h4 id="tcpdump"><a href="#tcpdump" class="headerlink" title="tcpdump"></a>tcpdump</h4><ul>
<li>sniff network packets for post analysis:</li>
<li>study packet sequences with timestamps(us)</li>
<li>CPU overhead optimized (socket ring buffers),but can still be significant.use caution.</li>
</ul>
<hr>
<p>netstat</p>
<ul>
<li>various network protocol statistics using -s:</li>
<li>a multi-tool:<ul>
<li>i:interface stats</li>
<li>r:route table<br>default:list conns</li>
</ul>
</li>
<li>netstat -p:shows process details!</li>
<li>Per-second interval with -c</li>
</ul>
<hr>
<h4 id="slabtop"><a href="#slabtop" class="headerlink" title="slabtop"></a>slabtop</h4><ul>
<li>Kernel slab allocator memory usage</li>
</ul>
<hr>
<h4 id="pcstat"><a href="#pcstat" class="headerlink" title="pcstat"></a>pcstat</h4><ul>
<li>show page cache residency </li>
<li>Uses the mincore(2)syscall.Useful for database performance analysis.</li>
</ul>
<hr>
<h4 id="perf-events"><a href="#perf-events" class="headerlink" title="perf_events"></a>perf_events</h4><ul>
<li>Provides the “perf” command</li>
<li>in Linux source code:tools/perf<ul>
<li>Usually pkg added by linux-tools-common.etc.</li>
</ul>
</li>
<li><code>Multi-tool</code> with many capabilities<ul>
<li>CPU profiling</li>
<li>PMC profiling</li>
<li>Static &amp; dynamic tracing</li>
</ul>
</li>
<li>Covered later in Profiling &amp; Tracing</li>
</ul>
<hr>
<h3 id="Methodologies"><a href="#Methodologies" class="headerlink" title="Methodologies"></a>Methodologies</h3><h4 id="anti-Methodologies"><a href="#anti-Methodologies" class="headerlink" title="anti-Methodologies"></a>anti-Methodologies</h4><ul>
<li>The lack of a deliberate methodology…</li>
<li>Street Light Anti-Method:<ol>
<li>Pick observability tools that are<ul>
<li>Familiar</li>
<li>Found on the Internet</li>
<li>Found at random</li>
</ul>
</li>
<li>Run tools</li>
<li>Look for obvious issues</li>
</ol>
</li>
<li>Drunk Man Anti-Method:<ul>
<li>Tune things at random until the problem goes away</li>
</ul>
</li>
</ul>
<hr>
<p>在Linux世界，进程不能直接访问硬件设备，当进程需要访问硬件设备(比如读取磁盘文件，接收网络数据等等)时，必须由用户态模式切换至内核态模式，通 过系统调用访问硬件设备。strace可以跟踪到一个进程产生的系统调用,包括参数，返回值，执行消耗的时间。</p>
<hr>
<h4 id="ss"><a href="#ss" class="headerlink" title="ss"></a>ss</h4><p>ss -l 显示本地打开的所有端口<br>ss -pl 显示每个进程具体打开的socket<br>ss -t -a 显示所有tcp socket<br>ss -u -a 显示所有的UDP Socekt<br>ss -o state established ‘( dport = :smtp or sport = :smtp )’ 显示所有已建立的SMTP连接<br>ss -o state established ‘( dport = :http or sport = :http )’ 显示所有已建立的HTTP连接<br>ss -x src /tmp/.X11-unix/* 找出所有连接X服务器的进程<br>ss -s 列出当前socket详细信息:</p>
<hr>
<h4 id="strace-1"><a href="#strace-1" class="headerlink" title="strace"></a>strace</h4><p>-c 统计每一系统调用的所执行的时间,次数和出错的次数等.<br>-d 输出strace关于标准错误的调试信息.<br>-f 跟踪由fork调用所产生的子进程.<br>-ff 如果提供-o filename,则所有进程的跟踪结果输出到相应的filename.pid中,pid是各进程的进程号.<br>-F 尝试跟踪vfork调用.在-f时,vfork不被跟踪.<br>-h 输出简要的帮助信息.<br>-i 输出系统调用的入口指针.<br>-q 禁止输出关于脱离的消息.<br>-r 打印出相对时间关于,,每一个系统调用.<br>-t 在输出中的每一行前加上时间信息.<br>-tt 在输出中的每一行前加上时间信息,微秒级.<br>-ttt 微秒级输出,以秒了表示时间.<br>-T 显示每一调用所耗的时间.<br>-v 输出所有的系统调用.一些调用关于环境变量,状态,输入输出等调用由于使用频繁,默认不输出.<br>-V 输出strace的版本信息.<br>-x 以十六进制形式输出非标准字符串<br>-xx 所有字符串以十六进制形式输出.<br>-a column<br>设置返回值的输出位置.默认 为40.<br>-e expr<br>指定一个表达式,用来控制如何跟踪.格式如下:<br>[qualifier=][!]value1[,value2]…<br>qualifier只能是 trace,abbrev,verbose,raw,signal,read,write其中之一.value是用来限定的符号或数字.默认的 qualifier是 trace.感叹号是否定符号.例如:<br>-eopen等价于 -e trace=open,表示只跟踪open调用.而-etrace!=open表示跟踪除了open以外的其他调用.有两个特殊的符号 all 和 none.<br>注意有些shell使用!来执行历史记录里的命令,所以要使用\.<br>-e trace=set<br>只跟踪指定的系统 调用.例如:-e trace=open,close,rean,write表示只跟踪这四个系统调用.默认的为set=all.<br>-e trace=file<br>只跟踪有关文件操作的系统调用.<br>-e trace=process<br>只跟踪有关进程控制的系统调用.<br>-e trace=network<br>跟踪与网络有关的所有系统调用.<br>-e strace=signal<br>跟踪所有与系统信号有关的 系统调用<br>-e trace=ipc<br>跟踪所有与进程通讯有关的系统调用<br>-e abbrev=set<br>设定 strace输出的系统调用的结果集.-v 等与 abbrev=none.默认为abbrev=all.<br>-e raw=set<br>将指 定的系统调用的参数以十六进制显示.<br>-e signal=set<br>指定跟踪的系统信号.默认为all.如 signal=!SIGIO(或者signal=!io),表示不跟踪SIGIO信号.<br>-e read=set<br>输出从指定文件中读出 的数据.例如:<br>-e read=3,5<br>-e write=set<br>输出写入到指定文件中的数据.<br>-o filename<br>将strace的输出写入文件filename<br>-p pid<br>跟踪指定的进程pid.<br>-s strsize<br>指定输出的字符串的最大长度.默认为32.文件名一直全部输出.<br>-u username<br>以username 的UID和GID执行被跟踪的命令</p>
]]></content>
      
        <categories>
            
            <category> Linux </category>
            
        </categories>
        
        
    </entry>
    
    <entry>
      <title><![CDATA[Linux 防火墙]]></title>
      <url>/Linux/Linux%20%E9%98%B2%E7%81%AB%E5%A2%99.html</url>
      <content type="html"><![CDATA[<p><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">[root@CentOS3 redis]# service iptables status</div><div class="line">Table: filter</div><div class="line">Chain INPUT (policy ACCEPT)</div><div class="line">num  target     prot opt source               destination         </div><div class="line">1    ACCEPT     all  --  0.0.0.0/0            0.0.0.0/0           state RELATED,ESTABLISHED </div><div class="line">2    ACCEPT     icmp --  0.0.0.0/0            0.0.0.0/0           </div><div class="line">3    ACCEPT     all  --  0.0.0.0/0            0.0.0.0/0           </div><div class="line">4    ACCEPT     tcp  --  0.0.0.0/0            0.0.0.0/0           state NEW tcp dpt:22 </div><div class="line">5    REJECT     all  --  0.0.0.0/0            0.0.0.0/0           reject-with icmp-host-prohibited </div><div class="line"></div><div class="line">Chain FORWARD (policy ACCEPT)</div><div class="line">num  target     prot opt source               destination         </div><div class="line">1    REJECT     all  --  0.0.0.0/0            0.0.0.0/0           reject-with icmp-host-prohibited </div><div class="line"></div><div class="line">Chain OUTPUT (policy ACCEPT)</div><div class="line">num  target     prot opt source               destination         </div><div class="line"></div><div class="line">[root@CentOS3 redis]# service iptables stop</div><div class="line">iptables: Setting chains to policy ACCEPT: filter          [  OK  ]</div><div class="line">iptables: Flushing firewall rules:                         [  OK  ]</div><div class="line">iptables: Unloading modules:                               [  OK  ]</div><div class="line">[root@CentOS3 redis]# service ip6tables stop</div><div class="line">ip6tables: Setting chains to policy ACCEPT: filter         [  OK  ]</div><div class="line">ip6tables: Flushing firewall rules:                        [  OK  ]</div><div class="line">ip6tables: Unloading modules:                              [  OK  ]</div><div class="line">[root@CentOS3 redis]# chkconfig iptables down</div><div class="line">chkconfig version 1.3.49.3 - Copyright (C) 1997-2000 Red Hat, Inc.</div><div class="line">This may be freely redistributed under the terms of the GNU Public License.</div><div class="line"></div><div class="line">usage:   chkconfig [--list] [--type &lt;type&gt;] [name]</div><div class="line">         chkconfig --add &lt;name&gt;</div><div class="line">         chkconfig --del &lt;name&gt;</div><div class="line">         chkconfig --override &lt;name&gt;</div><div class="line">         chkconfig [--level &lt;levels&gt;] [--type &lt;type&gt;] &lt;name&gt; &lt;on|off|reset|resetpriorities&gt;</div><div class="line">[root@CentOS3 redis]# chkconfig iptables off</div><div class="line">[root@CentOS3 redis]# chkconfig ip6tables off</div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line">`service iptables status`   查看iptables状态</div><div class="line">`service iptables restart`  iptables服务重启</div><div class="line">`service iptables stop`     iptables服务禁用</div></pre></td></tr></table></figure>
</p>
<hr>
<p>ip地址后边加个/8(16,24,32)是什么意思?<br>是掩码的位数，A类IP地址的默认子网掩码为255.0.0.0（由于255相当于二进制的8位1，所以也缩写成“/8”，表示网络号占了8位）;B类的为255.255.0.0（/16）;C类的为255.255.255.0(/24)。/30就是255.255.255.252。32就是255.255.255.255.</p>
<p>本机的DNS配置信息是在：/etc/resolv.conf</p>
<hr>
<p>time cost is(3\d{3}?ms)</p>
]]></content>
      
        <categories>
            
            <category> Linux </category>
            
        </categories>
        
        
    </entry>
    
    <entry>
      <title><![CDATA[Linux系统管理技术手册第二章]]></title>
      <url>/Linux/Linux%E7%B3%BB%E7%BB%9F%E7%AE%A1%E7%90%86%E6%8A%80%E6%9C%AF%E6%89%8B%E5%86%8C%E7%AC%AC%E4%BA%8C%E7%AB%A0.html</url>
      <content type="html"><![CDATA[<h1 id="Linux系统管理技术手册第二章"><a href="#Linux系统管理技术手册第二章" class="headerlink" title="Linux系统管理技术手册第二章"></a>Linux系统管理技术手册第二章</h1><p>开机时,计算机执行存储在ROM中的引导代码,这些代码及诶奥莱尝试确定如何加载并启动内核,内核检测系统的硬件,如何产生系统的<code>init</code>进程,这个进程总是PID1.</p>
<h2 id="引导过程的步骤"><a href="#引导过程的步骤" class="headerlink" title="引导过程的步骤"></a>引导过程的步骤</h2><p>Linux系统典型的引导过程由6个不同阶段组成</p>
<ul>
<li>加载并初始化内核</li>
<li>检查和配置设备</li>
<li>创建内核线程</li>
<li>操作员干预(仅用于手工引导)</li>
<li>执行系统启动脚本</li>
<li>多用户模式运行</li>
</ul>
]]></content>
      
        <categories>
            
            <category> Linux </category>
            
        </categories>
        
        
    </entry>
    
    <entry>
      <title><![CDATA[centos sublime]]></title>
      <url>/Linux/centos%20sublime.html</url>
      <content type="html"><![CDATA[<h1 id="建立软件安装目录（我一般把软件安装在opt目录下）"><a href="#建立软件安装目录（我一般把软件安装在opt目录下）" class="headerlink" title="建立软件安装目录（我一般把软件安装在opt目录下）"></a>建立软件安装目录（我一般把软件安装在opt目录下）</h1><p><code>mkdir /opt</code><br><code>cd /opt</code></p>
<h2 id="下载软件"><a href="#下载软件" class="headerlink" title="下载软件"></a>下载软件</h2><p><a href="http://www.sublimetext.com/3" target="_blank" rel="external">http://www.sublimetext.com/3</a><br>注意一定要下Ubuntu/tarball包。<br>也可以用命令下载</p>
<p>wget <a href="http://c758482.r82.cf2.rackcdn.com/sublime_text_3_build_3083_x64.tar.bz2" target="_blank" rel="external">http://c758482.r82.cf2.rackcdn.com/sublime_text_3_build_3083_x64.tar.bz2</a></p>
<p>如果链接地址失效，请到官网获取最新下载地址。</p>
<h2 id="解压软件包"><a href="#解压软件包" class="headerlink" title="解压软件包"></a>解压软件包</h2><p>tar jxvf sublime_text_3_build_3059_x64.tar.bz2</p>
<h2 id="命令行下直接运行"><a href="#命令行下直接运行" class="headerlink" title="命令行下直接运行"></a>命令行下直接运行</h2><p><code>cd /opt/sublime_text_3</code><br><code>./sublime_text</code></p>
<h2 id="创建桌面快捷方式"><a href="#创建桌面快捷方式" class="headerlink" title="创建桌面快捷方式"></a>创建桌面快捷方式</h2><p>复制文件</p>
<p><code>cp /opt/sublime_text_3/sublime_text.desktop /usr/share/applications</code></p>
<p>更改配置文件</p>
<p><code>vim /usr/share/applications/sublime_text.desktop</code></p>
<p><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">我的配置如下</div><div class="line">[Desktop Entry]</div><div class="line">Version=1.0</div><div class="line">Type=Application</div><div class="line">Name=Sublime Text</div><div class="line">GenericName=Text Editor</div><div class="line">Comment=Sophisticated text editor for code, markup and prose</div><div class="line">Exec=/opt/sublime_text_3/sublime_text %F</div><div class="line">Terminal=false</div><div class="line">MimeType=text/plain;</div><div class="line">Icon=/opt/sublime_text_3/Icon/48x48/sublime-text.png</div><div class="line">Categories=TextEditor;Development;</div><div class="line">StartupNotify=true</div><div class="line">Actions=Window;Document;</div><div class="line"></div><div class="line">[Desktop Action Window]</div><div class="line">Name=New Window</div><div class="line">Exec=/opt/sublime_text_3/sublime_text -n</div><div class="line">OnlyShowIn=Unity;</div><div class="line"></div><div class="line">[Desktop Action Document]</div><div class="line">Name=New File</div><div class="line">Exec=/opt/sublime_text/sublime_text_3 --command new_file</div><div class="line">OnlyShowIn=Unity;</div></pre></td></tr></table></figure>
</p>
<h2 id="打开软件"><a href="#打开软件" class="headerlink" title="打开软件"></a>打开软件</h2><p>应用程序 &gt;编程 &gt; Sublime Text”右键”将此启动器添加到桌面”</p>
]]></content>
      
        <categories>
            
            <category> Linux </category>
            
        </categories>
        
        
    </entry>
    
    <entry>
      <title><![CDATA[YAML]]></title>
      <url>/Document/YAML.html</url>
      <content type="html"><![CDATA[<h1 id="YAML"><a href="#YAML" class="headerlink" title="YAML"></a>YAML</h1><p><a href="http://nodeca.github.io/js-yaml/" target="_blank" rel="external">demo实例</a></p>
<ul>
<li>大小写敏感</li>
<li>使用缩进表示层级关系</li>
<li>缩进时不允许使用Tab键,只运行使用空格</li>
<li>缩进的空格数目不重要,只要相同层级的元素左侧对齐即可</li>
<li><h1 id="表示注释-从这个字符一直到行尾-都会被解析器忽略"><a href="#表示注释-从这个字符一直到行尾-都会被解析器忽略" class="headerlink" title="表示注释,从这个字符一直到行尾,都会被解析器忽略"></a>表示注释,从这个<em>字符</em>一直到行尾,都会被解析器忽略</h1></li>
</ul>
<h3 id="YAML支持的数据结构有三种"><a href="#YAML支持的数据结构有三种" class="headerlink" title="YAML支持的数据结构有三种"></a>YAML支持的数据结构有三种</h3><ul>
<li>对象:键值对的集合又称为映射（mapping）/ 哈希（hashes） / 字典（dictionary）</li>
<li>数组:一组按次序排列的值，又称为序列（sequence） / 列表（list）</li>
<li>纯量（scalars）：单个的、不可再分的值</li>
</ul>
<h2 id="对象"><a href="#对象" class="headerlink" title="对象"></a>对象</h2><p>对象的一组键值对，使用冒号结构表示<br>animal: pets –&gt; JS { animal: ‘pets’ }</p>
<p>Yaml 也允许另一种写法，将所有键值对写成一个行内对象。<br>hash: { name: Steve, foo: bar }   –&gt; JS { hash: { name: ‘Steve’, foo: ‘bar’ } }</p>
<h2 id="数组"><a href="#数组" class="headerlink" title="数组"></a>数组</h2><p>一组连词线开头的行，构成一个数组。<br>`</p>
<pre><code>- Cat
- Dog
- Goldfish
`


</code></pre><p>–&gt; JS<br>[ ‘Cat’, ‘Dog’, ‘Goldfish’ ]</p>
<p>数据结构的子成员是一个数组，则可以在该项下面缩进一个空格。<br>`</p>
<pre><code>-
 - Cat
 - Dog
 - Goldfish    

</code></pre><p>`<br>–&gt;JS<br>[ [ ‘Cat’, ‘Dog’, ‘Goldfish’ ] ]<br>数组也可以采用行内表示法。</p>
]]></content>
      
        <categories>
            
            <category> Document </category>
            
        </categories>
        
        
    </entry>
    
    <entry>
      <title><![CDATA[redis_version_3.2.4-sc]]></title>
      <url>/Redis/redis-conf-%E7%AC%94%E8%AE%B0/redis_version_3.2.4-sc.html</url>
      <content type="html"><![CDATA[<p>127.0.0.1:6379&gt; info</p>
<h1 id="Server"><a href="#Server" class="headerlink" title="Server"></a>Server</h1><p>redis_version:3.2.3<br>redis_git_sha1:00000000<br>redis_git_dirty:0<br>redis_build_id:276113d15453bda8<br>redis_mode:cluster<br>os:Linux 3.0.76-0.11-default x86_64<br>arch_bits:64<br>multiplexing_api:epoll<br>gcc_version:4.3.4<br>process_id:21542<br>run_id:9bb6d2f200729eb837c86510afcc7025dec26586<br>tcp_port:6379<br>uptime_in_seconds:6130854<br>uptime_in_days:70<br>hz:10<br>lru_clock:16723061<br>executable:/usr/local/bin/redis-server<br>config_file:/etc/redis/redis.conf</p>
<h1 id="Clients"><a href="#Clients" class="headerlink" title="Clients"></a>Clients</h1><p>connected_clients:358<br>client_longest_output_list:0<br>client_biggest_input_buf:0<br>blocked_clients:0</p>
<h1 id="Memory"><a href="#Memory" class="headerlink" title="Memory"></a>Memory</h1><p>used_memory:151730448<br>used_memory_human:144.70M<br>used_memory_rss:159973376<br>used_memory_rss_human:152.56M<br>used_memory_peak:677126784<br>used_memory_peak_human:645.76M<br>total_system_memory:16686497792<br>total_system_memory_human:15.54G<br>used_memory_lua:37888<br>used_memory_lua_human:37.00K<br>maxmemory:8589934592<br>maxmemory_human:8.00G<br>maxmemory_policy:noeviction<br>mem_fragmentation_ratio:1.05<br>mem_allocator:jemalloc-4.0.3</p>
<h1 id="Persistence"><a href="#Persistence" class="headerlink" title="Persistence"></a>Persistence</h1><p>loading:0<br>rdb_changes_since_last_save:11820<br>rdb_bgsave_in_progress:0<br>rdb_last_save_time:1476340843<br>rdb_last_bgsave_status:ok<br>rdb_last_bgsave_time_sec:1<br>rdb_current_bgsave_time_sec:-1<br>aof_enabled:0<br>aof_rewrite_in_progress:0<br>aof_rewrite_scheduled:0<br>aof_last_rewrite_time_sec:-1<br>aof_current_rewrite_time_sec:-1<br>aof_last_bgrewrite_status:ok<br>aof_last_write_status:ok</p>
<h1 id="Stats"><a href="#Stats" class="headerlink" title="Stats"></a>Stats</h1><p>total_connections_received:136623<br>total_commands_processed:3090632890<br>instantaneous_ops_per_sec:1288<br>total_net_input_bytes:274676332779<br>total_net_output_bytes:20528196260<br>instantaneous_input_kbps:108.62<br>instantaneous_output_kbps:8.95<br>rejected_connections:0<br>sync_full:0<br>sync_partial_ok:0<br>sync_partial_err:0<br>expired_keys:43<br>evicted_keys:0<br>keyspace_hits:512379822<br>keyspace_misses:419500843<br>pubsub_channels:0<br>pubsub_patterns:0<br>latest_fork_usec:3517<br>migrate_cached_sockets:0</p>
<h1 id="Replication"><a href="#Replication" class="headerlink" title="Replication"></a>Replication</h1><p>role:master<br>connected_slaves:0<br>master_repl_offset:0<br>repl_backlog_active:0<br>repl_backlog_size:1048576<br>repl_backlog_first_byte_offset:0<br>repl_backlog_histlen:0</p>
<h1 id="CPU"><a href="#CPU" class="headerlink" title="CPU"></a>CPU</h1><p>used_cpu_sys:51186.74<br>used_cpu_user:45645.72<br>used_cpu_sys_children:2273.35<br>used_cpu_user_children:58689.30</p>
<h1 id="Cluster"><a href="#Cluster" class="headerlink" title="Cluster"></a>Cluster</h1><p>cluster_enabled:1</p>
<h1 id="Keyspace"><a href="#Keyspace" class="headerlink" title="Keyspace"></a>Keyspace</h1><p>db0:keys=3,expires=2,avg_ttl=86397596</p>
]]></content>
      
        <categories>
            
            <category> redis </category>
            
            <category> redis-conf-笔记 </category>
            
        </categories>
        
        
    </entry>
    
    <entry>
      <title><![CDATA[redis_version_3.2.4]]></title>
      <url>/Redis/redis-conf-%E7%AC%94%E8%AE%B0/redis_version_3.2.4.html</url>
      <content type="html"><![CDATA[<p>redis_version:3.2.4                                                                                     //Redis 服务器版本<br>redis_git_sha1:00000000                                                                                     //Git SHA1<br>redis_git_dirty:0                                                                                   //Git dirty flag<br>redis_build_id:96326b2f9b8cf2d2<br>redis_mode:standalone<br>os:Linux 3.10.0-327.el7.x86_64 x86_64                                                                                   //Redis 服务器的宿主操作系统<br>arch_bits:64                                                                                    // 架构（32 或 64 位）<br>multiplexing_api:epoll                                                                                      // Redis 所使用的事件处理机制<br>gcc_version:4.8.5                                                                                       //编译 Redis 时所使用的 GCC 版本<br>process_id:23278                                                                                    //服务器进程的 PID<br>run_id:f5284394b52327c8697f7bff3995643dcbc2af7b                                                                                     //Redis 服务器的随机标识符（用于 Sentinel 和集群）<br>tcp_port:7380                                                                                   //TCP/IP 监听端口<br>uptime_in_seconds:617                                                                                       //自 Redis 服务器启动以来，经过的秒数<br>uptime_in_days:0                                                                                        // 自 Redis 服务器启动以来，经过的天数<br>hz:10<br>lru_clock:16720581                                                                                      //以分钟为单位进行自增的时钟，用于 LRU 管理<br>executable:/root/redis-server<br>config_file:/etc/redis/7380.conf</p>
<h1 id="Clients"><a href="#Clients" class="headerlink" title="Clients"></a>Clients</h1><p>connected_clients:7                                                  : 已连接客户端的数量（不包括通过从属服务器连接的客户端）<br>client_longest_output_list:0                                                 : 当前连接的客户端当中，最长的输出列表<br>client_biggest_input_buf:0                                                   : 当前连接的客户端当中，最大输入缓存<br>blocked_clients:0                                                : 正在等待阻塞命令（BLPOP、BRPOP、BRPOPLPUSH）的客户端的数量</p>
<h1 id="Memory"><a href="#Memory" class="headerlink" title="Memory"></a>Memory</h1><p>used_memory:6210872                                                  : 由 Redis 分配器分配的内存总量，以字节（byte）为单位<br>used_memory_human:5.92M                                                  : 以人类可读的格式返回 Redis 分配的内存总量<br>used_memory_rss:2621440                                                  : 从操作系统的角度，返回 Redis 已分配的内存总量（俗称常驻集大小）。这个值和 top 、 ps 等命令的输出一致。<br>used_memory_rss_human:2.50M<br>used_memory_peak:6332792                                                 : Redis 的内存消耗峰值（以字节为单位）<br>used_memory_peak_human:6.04M<br>total_system_memory:8087961600<br>total_system_memory_human:7.53G<br>used_memory_lua:37888    Lua 引擎所使用的内存大小（以字节为单位）<br>used_memory_lua_human:37.00K<br>maxmemory:2147483648<br>maxmemory_human:2.00G<br>maxmemory_policy:noeviction<br>mem_fragmentation_ratio:0.42                                                 : used_memory_rss 和 used_memory 之间的比率<br>mem_allocator:jemalloc-4.0.3                                                 : 在编译时指定的， Redis 所使用的内存分配器。可以是 libc 、 jemalloc 或者 tcmalloc </p>
<pre><code>在理想情况下， used_memory_rss 的值应该只比 used_memory 稍微高一点儿。

</code></pre><p>当 rss &gt; used ，且两者的值相差较大时，表示存在（内部或外部的）内存碎片。<br>内存碎片的比率可以通过 mem_fragmentation_ratio 的值看出。<br>当 used &gt; rss 时，表示 Redis 的部分内存被操作系统换出到交换空间了，在这种情况下，操作可能会产生明显的延迟。<br>        Because Redis does not have control over how its allocations are mapped to memory pages, high used_memory_rss is often the result of a spike in memory usage.<br>当 Redis 释放内存时，分配器可能会，也可能不会，将内存返还给操作系统。<br>如果 Redis 释放了内存，却没有将内存返还给操作系统，那么 used_memory 的值可能和操作系统显示的 Redis 内存占用并不一致。<br>查看 used_memory_peak 的值可以验证这种情况是否发生。</p>
<h1 id="Persistence-RDB-和-AOF-的相关信息"><a href="#Persistence-RDB-和-AOF-的相关信息" class="headerlink" title="Persistence                                                    : RDB 和 AOF 的相关信息"></a>Persistence                                                    : RDB 和 AOF 的相关信息</h1><p>loading:0<br>rdb_changes_since_last_save:0<br>rdb_bgsave_in_progress:0<br>rdb_last_save_time:1476337757<br>rdb_last_bgsave_status:ok<br>rdb_last_bgsave_time_sec:0<br>rdb_current_bgsave_time_sec:-1<br>aof_enabled:1<br>aof_rewrite_in_progress:0<br>aof_rewrite_scheduled:0<br>aof_last_rewrite_time_sec:-1<br>aof_current_rewrite_time_sec:-1<br>aof_last_bgrewrite_status:ok<br>aof_last_write_status:ok<br>aof_current_size:0<br>aof_base_size:0<br>aof_pending_rewrite:0<br>aof_buffer_length:0<br>aof_rewrite_buffer_length:0<br>aof_pending_bio_fsync:0<br>aof_delayed_fsync:0</p>
<h1 id="Stats-一般统计信息"><a href="#Stats-一般统计信息" class="headerlink" title="Stats                                                  : 一般统计信息"></a>Stats                                                  : 一般统计信息</h1><p>total_connections_received:9<br>total_commands_processed:3497<br>instantaneous_ops_per_sec:6<br>total_net_input_bytes:180031<br>total_net_output_bytes:12844181<br>instantaneous_input_kbps:0.35<br>instantaneous_output_kbps:1.08<br>rejected_connections:0<br>sync_full:1<br>sync_partial_ok:0<br>sync_partial_err:0<br>expired_keys:0<br>evicted_keys:0<br>keyspace_hits:0<br>keyspace_misses:0<br>pubsub_channels:1<br>pubsub_patterns:0<br>latest_fork_usec:83<br>migrate_cached_sockets:0</p>
<h1 id="Replication-主-从复制信息"><a href="#Replication-主-从复制信息" class="headerlink" title="Replication                                                : 主/从复制信息"></a>Replication                                                : 主/从复制信息</h1><p>role:master<br>connected_slaves:1<br>slave0:ip=192.168.29.136,port=7381,state=online,offset=128863,lag=0<br>master_repl_offset:128863<br>repl_backlog_active:1<br>repl_backlog_size:5242880<br>repl_backlog_first_byte_offset:2<br>repl_backlog_histlen:128862</p>
<h1 id="CPU-CPU-计算量统计信息"><a href="#CPU-CPU-计算量统计信息" class="headerlink" title="CPU                                                : CPU 计算量统计信息"></a>CPU                                                : CPU 计算量统计信息</h1><p>used_cpu_sys:0.33<br>used_cpu_user:0.20<br>used_cpu_sys_children:0.00<br>used_cpu_user_children:0.00</p>
<h1 id="Cluster-Redis-集群信息"><a href="#Cluster-Redis-集群信息" class="headerlink" title="Cluster                                                : Redis 集群信息"></a>Cluster                                                : Redis 集群信息</h1><p>cluster_enabled:0</p>
<h1 id="Keyspace"><a href="#Keyspace" class="headerlink" title="Keyspace"></a>Keyspace</h1>]]></content>
      
        <categories>
            
            <category> redis </category>
            
            <category> redis-conf-笔记 </category>
            
        </categories>
        
        
    </entry>
    
    <entry>
      <title><![CDATA[Redis Sentinel]]></title>
      <url>/Redis/redis%E6%9C%8D%E5%8A%A1%E6%90%AD%E5%BB%BA/Redis%20Sentinel.html</url>
      <content type="html"><![CDATA[<h1 id="Redis-Sentinel"><a href="#Redis-Sentinel" class="headerlink" title="Redis Sentinel"></a>Redis Sentinel</h1><h2 id="Sentinel介绍"><a href="#Sentinel介绍" class="headerlink" title="Sentinel介绍"></a>Sentinel介绍</h2><p>Sentinel是Redis官方为集群提供的高可用解决方案。 在实际项目中可以使用sentinel去做redis自动故障转移，减少人工介入的工作量。另外sentinel也给客户端提供了监控消息的通知，这样客户端就可根据消息类型去判断服务器的状态，去做对应的适配操作。</p>
<p>Sentinel主要功能列表:</p>
<ul>
<li>Monitoring：Sentinel持续检查集群中的master、slave状态，判断是否存活。</li>
<li>Notification：在发现某个redis实例死的情况下，Sentinel能通过API通知系统管理员或其他程序脚本。</li>
<li>Automatic failover：如果一个master挂掉后，sentinel立马启动故障转移，把某个slave提升为master。其他的slave重新配置指向新master。</li>
<li>Configuration provider：对于客户端来说sentinel通知是有效可信赖的。客户端会连接sentinel去请求当前master的地址，一旦发生故障sentinel会提供新地址给客户端。</li>
</ul>
<h2 id="Sentinel配置"><a href="#Sentinel配置" class="headerlink" title="Sentinel配置"></a>Sentinel配置</h2><p>Sentinel本质上只是一个运行在特殊模式下的redis服务器，通过不同配置来区分提供服务。 sentinel.conf配置：</p>
<table>
<thead>
<tr>
<th style="text-align:left"></th>
<th style="text-align:left">[监控名称]</th>
<th style="text-align:left">[ip]</th>
<th style="text-align:left">[port]</th>
<th style="text-align:left">[多少sentinel同意才发生故障转移]</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">sentinel</td>
<td style="text-align:left">monitor mymaster</td>
<td style="text-align:left">127.0.0.1</td>
<td style="text-align:left">6379</td>
<td style="text-align:left">2</td>
</tr>
</tbody>
</table>
<table>
<thead>
<tr>
<th style="text-align:left"></th>
<th style="text-align:left">[监控名称]</th>
<th style="text-align:left">[Master多少毫秒后不回应ping命令，就认为master是主观下线状态]</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">sentinel</td>
<td style="text-align:left">down-after-milliseconds mymaster</td>
<td style="text-align:left">60000</td>
</tr>
</tbody>
</table>
<table>
<thead>
<tr>
<th style="text-align:left"></th>
<th style="text-align:left">[故障转移超时时间]</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">sentinel</td>
<td style="text-align:left">failover-timeout mymaster</td>
<td style="text-align:left">180000</td>
</tr>
</tbody>
</table>
<table>
<thead>
<tr>
<th style="text-align:left"></th>
<th style="text-align:left">[在执行故障转移时,最多可以有多少个从服务器同时对新的主服务器进行同步]</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">sentinel</td>
<td style="text-align:left">parallel-syncs mymaster</td>
<td style="text-align:left">1</td>
</tr>
</tbody>
</table>
<p>sentinel需要使用redis2.8版本以上，启动如下：<br><code>redis-sentinel sentinel.conf</code><br>启动后Sentinel会：</p>
<ul>
<li>以10秒一次的频率，向被监视的master发送info命令，根据回复获取master当前信息。</li>
<li>以1秒一次的频率，向所有redis服务器、包含sentinel在内发送PING命令，通过回复判断服务器是否在线。</li>
<li>以2秒一次的频率，通过向所有被监视的master，slave服务器发送包含当前sentinel，master信息的消息。</li>
</ul>
<p>另外建议sentinel至少起3个实例以上，并配置2个实例同意即可发生转移。 5个实例，配置3个实例同意以此类推。</p>
<h2 id="故障转移消息接收的3种方式"><a href="#故障转移消息接收的3种方式" class="headerlink" title="故障转移消息接收的3种方式"></a>故障转移消息接收的3种方式</h2><p>Redis服务器一旦发送故障后，sentinel通过raft算法投票选举新master。 故障转移过程可以通过sentinel的API获取/订阅接收事件消息。</p>
<h3 id="脚本接收"><a href="#脚本接收" class="headerlink" title="脚本接收"></a>脚本接收</h3><p>//当故障转移期间，可以指定一个“通知”脚本用来告知系统管理员，当前集群的情况。<br>//脚本被允许执行的最大时间为60秒，如果超时，脚本将会被终止(KILL)<br><code>sentinel notification-script mymaster /var/redis/notify.sh</code><br>//故障转移期之后，配置通知客户端的脚本.<br><code>sentinel client-reconfig-script mymaster /var/redis/notifyReconfig.sh</code></p>
<h3 id="客户端直接接收"><a href="#客户端直接接收" class="headerlink" title="客户端直接接收"></a>客户端直接接收</h3><p>Sentinel的故障转移消息通知使用的是redis发布订阅(详解Redis发布订阅及客户端编程)。就是说在故障转移期间所有产生的事件信息，都通过频道(channel)发布出去。比如我们加台slave服务器，sentinel监听到后会发布加slave的消息到”+slave”频道上，客户端只需要订阅”+slave”频道即可接收到对应消息。</p>
<p>其消息格式如下：<br><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">[实例类型] [事件服务器名称] [服务器ip] [服务器端口] @[master名称] [ip] [端口]</div><div class="line"></div><div class="line">&lt;instance-type&gt; &lt;name&gt; &lt;ip&gt; &lt;port&gt; @ &lt;master-name&gt; &lt;master-ip&gt; &lt;master-port&gt;</div></pre></td></tr></table></figure></p>
<p>通知消息格式示例：<br><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">*          //订阅类型， *即订阅所有事件消息。</div><div class="line">-sdown     //消息类型</div><div class="line">slave 127.0.0.1:6379 127.0.0.1 6379 @ mymaster 127.0.0.1 6381</div></pre></td></tr></table></figure></p>
<p>订阅消息示例：<br><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">using (RedisSentinel rs = new RedisSentinel(CurrentNode.Host, CurrentNode.Port))</div><div class="line">            &#123;</div><div class="line">                var redisPubSub = new RedisPubSub(node.Host, node.Port);</div><div class="line">                redisPubSub.OnMessage += OnMessage;</div><div class="line">                redisPubSub.OnSuccess += (msg) =&gt;&#123;&#125;;</div><div class="line">                redisPubSub.OnUnSubscribe += (obj) =&gt;&#123;&#125;;</div><div class="line">                redisPubSub.OnError = (exception) =&gt;&#123; &#125;;</div><div class="line">                redisPubSub.PSubscribe(&quot;*&quot;);</div><div class="line">            &#125;</div></pre></td></tr></table></figure></p>
<h3 id="服务间接接收"><a href="#服务间接接收" class="headerlink" title="服务间接接收"></a>服务间接接收</h3><p>这种方式在第二种基础上扩展了一层，即应用端不直接订阅sentinel。 单独做服务去干这件事情，然后应用端提供API供这个服务回调通知。 这样做的好处在于：</p>
<ul>
<li>减少应用端监听失败出错的可能性。</li>
<li>应用端由主动方变成被动方，降低耦合。</li>
<li>性能提高，轮询变回调。</li>
<li>独立成服务可扩展性更高。</li>
</ul>
<p>比如:</p>
<ol>
<li><p>以后换掉sentinel，我们只需要动服务即可，应用端无需更改。</p>
</li>
<li><p>可以在服务内多增加一层守护线程去主动拉取redis状态，这样可确保即使sentinel不生效，也能及时察觉redis状态，并通知到应用端。 当然这种情况很极端，因为sentinel配的也是多节点，同时挂的几率非常小。 </p>
</li>
</ol>
<p>示例：<br>应用端提供回调API，在这个API逻辑下去刷新内存中的Redis连接。<br><code>http://127.0.0.1/redis/notify.api</code><br>独立服务监控到状况后，调用API通知应用端：<br><code>httprequest.post(&quot;http://127.0.0/redis/notify.api&quot;);</code></p>
<h2 id="整体设计"><a href="#整体设计" class="headerlink" title="整体设计"></a>整体设计</h2>]]></content>
      
        <categories>
            
            <category> redis </category>
            
            <category> redis服务搭建 </category>
            
        </categories>
        
        
    </entry>
    
    <entry>
      <title><![CDATA[redis-sentinel docker集群]]></title>
      <url>/Redis/redis%E6%9C%8D%E5%8A%A1%E6%90%AD%E5%BB%BA/redis-sentinel%20docker%E9%9B%86%E7%BE%A4.html</url>
      <content type="html"><![CDATA[<h2 id="redis-master"><a href="#redis-master" class="headerlink" title="redis-master"></a>redis-master</h2><p><code>docker run --name redis-master -v /home/testuser/docker/master-redis.conf:/usr/local/etc/master-redis.conf -d -p 16397:16379 redis redis-server /usr/local/etc/master-redis.conf</code></p>
<h2 id="redis-slave"><a href="#redis-slave" class="headerlink" title="redis-slave"></a>redis-slave</h2><p><code>docker run --link redis-master:redis-master -v /home/testuser/docker/slave-redis.conf:/usr/local/etc/slave-redis.conf --name redis-slave1 -d redis redis-server /usr/local/etc/slave-redis.conf</code></p>
<p><code>docker run --link redis-master:redis-master -v /home/testuser/docker/slave-redis.conf:/usr/local/etc/slave-redis.conf --name redis-slave2 -d redis redis-server /usr/local/etc/slave-redis.conf</code></p>
<h4 id="宿主机上如何获得-docker-container-容器的-ip-地址？"><a href="#宿主机上如何获得-docker-container-容器的-ip-地址？" class="headerlink" title="宿主机上如何获得 docker container 容器的 ip 地址？"></a>宿主机上如何获得 docker container 容器的 ip 地址？</h4><p><figure class="highlight bash"><table><tr><td class="code"><pre><div class="line">docker inspect --format=<span class="string">'&#123;&#123;.NetworkSettings.IPAddress&#125;&#125;'</span> [<span class="variable">$CONTAINER_ID</span>]</div></pre></td></tr></table></figure>
</p>
<p>172.17.0.51 mster<br>172.17.0.55 slave1<br>56  2<br>57  3</p>
<p><figure class="highlight bash"><table><tr><td class="code"><pre><div class="line">docker inspect -f <span class="string">'&#123;&#123;.Id&#125;&#125;'</span> [获取的名称或者id]</div></pre></td></tr></table></figure>
</p>
<p><figure class="highlight bash"><table><tr><td class="code"><pre><div class="line">[testuser@CentOS1 docker]$ redis-sentinel 3sentinel.conf &amp;</div><div class="line">[3] 28932</div><div class="line">[testuser@CentOS1 docker]$                 _._                                                  </div><div class="line">           _.-``__ <span class="string">''</span>-._                                             </div><div class="line">      _.-``    `.  `_.  <span class="string">''</span>-._           Redis 3.0.7 (00000000/0) 64 bit</div><div class="line">  .-`` .-```.  ```\/    _.,_ <span class="string">''</span>-._                                   </div><div class="line"> (    <span class="string">'      ,       .-`  | `,    )     Running in sentinel mode</span></div><div class="line"> |`-._`-...-` __...-.``-._|'` _.-<span class="string">'|     Port: 26383</span></div><div class="line"> |    `-._   `._    /     _.-'    |     PID: 28932</div><div class="line">  `-._    `-._  `-./  _.-<span class="string">'    _.-'</span>                                   </div><div class="line"> |`-._`-._    `-.__.-<span class="string">'    _.-'</span>_.-<span class="string">'|                                  </span></div><div class="line"> |    `-._`-._        _.-'_.-<span class="string">'    |           http://redis.io        </span></div><div class="line">  `-._    `-._`-.__.-'_.-<span class="string">'    _.-'</span>                                   </div><div class="line"> |`-._`-._    `-.__.-<span class="string">'    _.-'</span>_.-<span class="string">'|                                  </span></div><div class="line"> |    `-._`-._        _.-'_.-<span class="string">'    |                                  </span></div><div class="line">  `-._    `-._`-.__.-'_.-<span class="string">'    _.-'</span>                                   </div><div class="line">      `-._    `-.__.-<span class="string">'    _.-'</span>                                       </div><div class="line">          `-._        _.-<span class="string">'                                           </span></div><div class="line">              `-.__.-'                                               </div><div class="line"></div><div class="line">28932:X 05 Sep 16:10:12.875 <span class="comment"># Sentinel runid is d1d5d361d1d4c9576d64ab274571361ac963351f</span></div><div class="line">28932:X 05 Sep 16:10:12.875 <span class="comment"># +monitor master redis-master 172.17.0.62 16379 quorum 2</span></div><div class="line">28932:X 05 Sep 16:10:12.876 * +slave slave 172.17.0.63:16379 172.17.0.63 16379 @ redis-master 172.17.0.62 16379</div><div class="line">28932:X 05 Sep 16:10:12.908 * +slave slave 172.17.0.64:16379 172.17.0.64 16379 @ redis-master 172.17.0.62 16379</div><div class="line">28932:X 05 Sep 16:10:12.933 * +sentinel sentinel 172.17.42.1:26381 172.17.42.1 26381 @ redis-master 172.17.0.62 16379</div><div class="line">28932:X 05 Sep 16:10:13.009 * +sentinel sentinel 172.17.42.1:26382 172.17.42.1 26382 @ redis-master 172.17.0.62 16379</div></pre></td></tr></table></figure>
</p>
]]></content>
      
        <categories>
            
            <category> redis </category>
            
            <category> redis服务搭建 </category>
            
        </categories>
        
        
    </entry>
    
    <entry>
      <title><![CDATA[redis服务搭建]]></title>
      <url>/Redis/redis%E6%9C%8D%E5%8A%A1%E6%90%AD%E5%BB%BA/redis%E6%9C%8D%E5%8A%A1%E6%90%AD%E5%BB%BA.html</url>
      <content type="html"><![CDATA[<h1 id="redis服务搭建"><a href="#redis服务搭建" class="headerlink" title="redis服务搭建"></a>redis服务搭建</h1><h2 id="主机命名"><a href="#主机命名" class="headerlink" title="主机命名"></a>主机命名</h2><ol>
<li>确定操作系统类型<br><code>cat  /etc/issue</code></li>
<li><p>查看当前系统的主机名<code>hostname</code></p>
</li>
<li><p>改/etc/sysconfig下的network文件<code>vi /etc/sysconfig/network</code></p>
<p>把HOSTNAME后面的值改为想要设置的主机名<br><code>NETWORKING=yes
HOSTNAME=CentOS2</code></p>
</li>
</ol>
<p>system network reset</p>
<p>exit 验证</p>
<p>不得重启</p>
<ol>
<li><p>更改/etc下的hosts文件</p>
<p><code>vi /etc/hosts</code><br>然后将localhost.localdomain改为想要设置的主机名。</p>
<pre><code>127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4
::1         localhost localhost.localdomain localhost6 localhost6.localdomain6
192.168.29.136  CentOS1
192.168.29.135  CentOS2
192.168.29.134  CentOS3




</code></pre></li>
</ol>
<ol>
<li><code>reboot</code></li>
<li>检查<code>hostname</code></li>
</ol>
<h2 id="时钟同步"><a href="#时钟同步" class="headerlink" title="时钟同步"></a>时钟同步</h2><h3 id="CentOS6"><a href="#CentOS6" class="headerlink" title="CentOS6"></a>CentOS6</h3><ol>
<li><p>检查是否启动时钟同步服务<br>service检查:  <code>service ntpd status</code><br>job 检查 :    <code>crontab -l</code><br>修改定时任务: <code>crontab -e</code> </p>
</li>
<li><p>设置时钟同步:</p>
<ul>
<li>修改 /etc/ntp.conf  加入 ntp server ip<br><code>vi /etc/ntp.conf</code> <code>server 10.200.92.18</code></li>
<li>启动ntp服务:<br><code>service ntpd start</code></li>
<li>设置服务自动启动: <ol>
<li>设置 <code>chkconfig ntpd on</code></li>
<li>检查 <code>chkconfig -list ntpd</code></li>
</ol>
</li>
</ul>
</li>
</ol>
<h3 id="CentOS7"><a href="#CentOS7" class="headerlink" title="CentOS7"></a>CentOS7</h3><ul>
<li>使用rpm检查ntp包是否安装<br>[root@CentOS3 home]# rpm -q ntp<br>ntp-4.2.6p5-22.el7.centos.x86_64</li>
<li>如果已经安装则略过此步，否则使用yum进行安装，并设置系统开机自动启动并启动服务<p><figure class="highlight plain"><table><tr><td class="code"><pre><div class="line">yum -y install ntp</div><div class="line">systemctl enable ntpd</div><div class="line">systemctl start ntpd</div></pre></td></tr></table></figure>
</p>
</li>
</ul>
<p>修改完成后重启ntpd服务systemctl restart ntpd</p>
<p>使用ntpq -p 查看网络中的NTP服务器，同时显示客户端和每个服务器的关系</p>
<p>使用ntpstat 命令查看时间同步状态，这个一般需要5-10分钟后才能成功连接和同步。所以，服务器启动后需要稍等下：<br>刚启动的时候，一般是：</p>
<pre><code>unsynchronised
  time server re-starting
   polling server every 8 s





</code></pre><h2 id="操作系统"><a href="#操作系统" class="headerlink" title="操作系统"></a>操作系统</h2><p>CentOS release 6.8 (Final)</p>
<p>CentOS 7</p>
<h2 id="网络"><a href="#网络" class="headerlink" title="网络"></a>网络</h2><h3 id="CentOS-6-8"><a href="#CentOS-6-8" class="headerlink" title="CentOS 6.8:"></a>CentOS 6.8:</h3><pre><code>- 防火墙:
    - 即使生效,重启后失效
        开启: service iptables start
        关闭: service iptables stop
    - 重启后生效
        开启: chkconfig iptables on
        关闭: chkconfig iptables off
- 开启了防火墙时,做如下设置可以仅开启相关端口:
    修改/etc/sysconfig/iptables 文件,添加以下内容:
        -A RH-Firewall-1-INPUT -m state --state NEW -m tcp -p tcp --dport 80 -j ACCEPT 
        -A RH-Firewall-1-INPUT -m state --state NEW -m tcp -p tcp --dport 22 -j ACCEPT


</code></pre><h3 id="CentOS-7-0"><a href="#CentOS-7-0" class="headerlink" title="CentOS 7.0:"></a>CentOS 7.0:</h3><p>CentOS 7.0默认使用的是firewall作为防火墙，这里改为iptables防火墙。<br>firewall：</p>
<pre><code>systemctl start firewalld.service#启动firewall

systemctl stop firewalld.service#停止firewall

systemctl disable firewalld.service#禁止firewall开机启动



</code></pre><h2 id="源码安装redis"><a href="#源码安装redis" class="headerlink" title="源码安装redis"></a>源码安装redis</h2><pre><code>wget http://download.redis.io/releases/redis-3.2.4.tar.gz
tar xzf redis-3.2.4.tar.gz
cd redis-3.2.4
make
make test   //只用单核运行 make test：taskset -c 1 sudo make test
make install
mv redis-3.2.4 redis
mv redis /usr/local/



</code></pre><h2 id="修复3个redis-server-启动警告"><a href="#修复3个redis-server-启动警告" class="headerlink" title="修复3个redis-server 启动警告"></a>修复3个redis-server 启动警告</h2><pre><code>echo 1 &gt; /proc/sys/vm/overcommit_memory
echo never &gt; /sys/kernel/mm/transparent_hugepage/enabled
echo 511 &gt; /proc/sys/net/core/somaxconn


</code></pre><h2 id="开机启动redis"><a href="#开机启动redis" class="headerlink" title="开机启动redis"></a>开机启动redis</h2><p><code>/usr/local/redis/utils/install_server.sh</code><br>第一个参数分别选择7380 7381 26379</p>
<h3 id="26379"><a href="#26379" class="headerlink" title="26379"></a>26379</h3><pre><code>Please select the redis executable path [/usr/local/bin/redis-server] 
+  /usr/local/bin/redis-sentinel



</code></pre><h3 id="关闭实例"><a href="#关闭实例" class="headerlink" title="关闭实例"></a>关闭实例</h3><pre><code>[root@CentOS3 redis]# ps aux|grep redis
root     19874  0.0  0.0 136912  7528 ?        Ssl  15:43   0:00 /usr/local/bin/redis-server 127.0.0.1:7380
root     20178  0.0  0.0 136912  7528 ?        Ssl  15:43   0:00 /usr/local/bin/redis-server 127.0.0.1:7381
root     20541  0.0  0.0 136912  7528 ?        Ssl  15:47   0:00 /usr/local/bin/redis-server 127.0.0.1:26379
root     20705  0.0  0.0 112644   992 pts/2    S+   15:48   0:00 grep --color=auto redis
[root@CentOS3 redis]# redis-cli -p 7380 shutdown
[root@CentOS3 redis]# redis-cli -p 7381 shutdown
[root@CentOS3 redis]# redis-cli -p 26379 shutdown
[root@CentOS3 redis]# ps aux|grep redis
root     20721  0.0  0.0 112644   996 pts/2    S+   15:49   0:00 grep --color=auto redis


</code></pre><p><code>redis-cli -p 7380 shutdown &amp;&amp; redis-cli -p 7381 shutdown &amp;&amp; redis-cli -p 26379 shutdown</code></p>
<h3 id="修改sentinel启动脚本"><a href="#修改sentinel启动脚本" class="headerlink" title="修改sentinel启动脚本"></a>修改sentinel启动脚本</h3><p><code>vim /etc/init.d/redis_26379</code></p>
<pre><code>+  REDISHOSTNAME=&quot;192.168.29.134&quot;

-   $CLIEXEC -p $REDISPORT shutdown
+   $CLIEXEC -h $REDISHOSTNAME -p $REDISPORT shutdown


</code></pre><h2 id="resid-service-config-135-7380"><a href="#resid-service-config-135-7380" class="headerlink" title="resid-service config (135:7380)"></a>resid-service config (135:7380)</h2><pre><code>-  bind 127.0.0.1
+  bind 192.168.29.135   

-  protected-mode yes
+  protected-mode no

-  # unixsocket /tmp/redis.sock
-  # unixsocketperm 700
+  unixsocket /tmp/redis.sock
+  unixsocketperm 700

-  timeout 0 
+  timeout 300

-  tcp-keepalive 300
+  tcp-keepalive 0

-  save 900 1
-  save 300 10
-  save 60 10000
+  #save 900 1
+  #save 300 10
+  #save 60 10000

-  stop-writes-on-bgsave-error yes
+  stop-writes-on-bgsave-error no

-  #repl-ping-slave-period 10
+  repl-ping-slave-period 10

-  # repl-backlog-size 1mb
+  repl-backlog-size 5mb
-  # repl-backlog-ttl 3600
+  repl-backlog-ttl 600

-  #maxclients 10000
+  maxclients 10000

-  # maxmemory &lt;bytes&gt;
+  maxmemory 2gb

-  appendonly no
+  appendonly yes 


</code></pre><h3 id="resid-service-config-135-7381"><a href="#resid-service-config-135-7381" class="headerlink" title="resid-service config (135:7381)"></a>resid-service config (135:7381)</h3><pre><code>cp /etc/redis/7380.conf /etc/redis/7381.conf
vim  /etc/redis/7381.conf

    `:%s/7380/7381/g`   //替换所有的7380为7381


</code></pre><h2 id="redis-sentinal-config-135-26379"><a href="#redis-sentinal-config-135-26379" class="headerlink" title="redis-sentinal config (135:26379)"></a>redis-sentinal config (135:26379)</h2><pre><code>rm -f /etc/redis/26379.conf
cp /usr/local/redis/sentinel.conf /etc/redis/26379.conf
vim /usr/local/redis/26379.conf

    +  daemonize yes
    -  #protected-mode no
    +  protected-mode no
    -  dir /tmp
    +  dir /var/lib/redis/26379
    +  logfile /var/log/redis_26379.log

    -  sentinel monitor mymaster 127.0.0.1 6379 2
    +  sentinel monitor redis136 192.168.29.136 7380 2
    -  sentinel down-after-milliseconds mymaster 30000
    +  sentinel down-after-milliseconds redis136 5000        
    -  sentinel parallel-syncs mymaster 1
    +  sentinel parallel-syncs redis136 1 
    -  sentinel failover-timeout mymaster 180000
    +  sentinel failover-timeout redis136 60000

    +  sentinel monitor redis135 192.168.29.135 7380 2
    +  sentinel down-after-milliseconds redis135 5000
    +  sentinel failover-timeout redis135 60000
    +  sentinel parallel-syncs redis135 1

    +  sentinel monitor redis134 192.168.29.134 7380 2
    +  sentinel down-after-milliseconds redis134 5000
    +  sentinel failover-timeout redis134 60000
    +  sentinel parallel-syncs redis134 1



</code></pre><h2 id="redis部署"><a href="#redis部署" class="headerlink" title="redis部署"></a>redis部署</h2><h3 id="预定分配方案"><a href="#预定分配方案" class="headerlink" title="预定分配方案:"></a>预定分配方案:</h3><pre><code>            192.168.29.134
             +---------+  
             | M1 7380 |
             | R3 7381 |
             | S1      |
             +---------+
192.168.29.136    |    192.168.29.135
  +---------+     |     +---------+
  | M3 7380 |     |     | M2 7380 |  
  | R2 7381 |-----------| R1 7381 |
  | S3      |           | S2      |
  +---------+           +---------+

  Configuration: quorum = 2




</code></pre><h3 id="redis-server实例启动"><a href="#redis-server实例启动" class="headerlink" title="redis-server实例启动:"></a>redis-server实例启动:</h3><pre><code>redis-server /etc/redis/7380.conf
redis-server /etc/redis/7381.conf

netstat -lntpa |grep redis 

</code></pre><h3 id="主从分配"><a href="#主从分配" class="headerlink" title="主从分配"></a>主从分配</h3><p>三台主机实例均启动之后,进行主从配置:</p>
<pre><code>redis-cli -h 192.168.29.134 -p 7381 slaveof 192.168.29.136 7380
redis-cli -h 192.168.29.135 -p 7381 slaveof 192.168.29.134 7380
redis-cli -h 192.168.29.136 -p 7381 slaveof 192.168.29.135 7380


</code></pre><h3 id="redis-sentinel实例启动"><a href="#redis-sentinel实例启动" class="headerlink" title="redis-sentinel实例启动:"></a>redis-sentinel实例启动:</h3><p>最后在三台主机上启动sentinel</p>
<p><code>redis-sentinel /etc/redis/26379.conf</code></p>
<h2 id="检查failover"><a href="#检查failover" class="headerlink" title="检查failover"></a>检查failover</h2><pre><code>redis-cli -h 192.168.29.135 -p 7380  debug sleep 300s
redis-cli -p 26379 sentinel master redis135



</code></pre>]]></content>
      
        <categories>
            
            <category> redis </category>
            
            <category> redis服务搭建 </category>
            
        </categories>
        
        
    </entry>
    
  
  
</search>
